nohup: 忽略输入
OrderedDict([('conv1.weight', 
(0 ,0 ,.,.) = 
  1.3335e-02  1.4664e-02 -1.5351e-02  ...  -4.0896e-02 -4.3034e-02 -7.0755e-02
  4.1205e-03  5.8477e-03  1.4948e-02  ...   2.2060e-03 -2.0912e-02 -3.8517e-02
  2.2331e-02  2.3595e-02  1.6120e-02  ...   1.0281e-01  6.2641e-02  5.1977e-02
                 ...                   ⋱                   ...                
 -9.0349e-04  2.7767e-02 -1.0105e-02  ...  -1.2722e-01 -7.6604e-02  7.8453e-03
  3.5894e-03  4.8006e-02  6.2051e-02  ...   2.4267e-02 -3.3662e-02 -1.5709e-02
 -8.0029e-02 -3.2238e-02 -1.7808e-02  ...   3.5359e-02  2.2439e-02  1.7077e-03

(0 ,1 ,.,.) = 
 -1.8452e-02  1.1415e-02  2.3850e-02  ...   5.3736e-02  4.4022e-02 -9.4675e-03
 -7.7273e-03  1.8890e-02  6.7981e-02  ...   1.5956e-01  1.4606e-01  1.1999e-01
 -4.6013e-02 -7.6075e-02 -8.9648e-02  ...   1.2108e-01  1.6705e-01  1.7619e-01
                 ...                   ⋱                   ...                
  2.8818e-02  1.3665e-02 -8.3825e-02  ...  -3.8081e-01 -3.0414e-01 -1.3966e-01
  8.2868e-02  1.3864e-01  1.5241e-01  ...  -5.1232e-03 -1.2435e-01 -1.2967e-01
 -7.2789e-03  7.7021e-02  1.3999e-01  ...   1.8427e-01  1.1144e-01  2.3438e-02

(0 ,2 ,.,.) = 
 -1.8311e-02 -5.6424e-03  8.7224e-03  ...   2.5775e-02  2.6431e-02 -3.9914e-03
 -1.0098e-02  4.1615e-03  4.9730e-02  ...   1.2447e-01  1.1950e-01  1.1198e-01
 -6.3478e-02 -1.0146e-01 -9.8343e-02  ...   1.0630e-01  1.3982e-01  1.4942e-01
                 ...                   ⋱                   ...                
  2.5810e-02  1.0501e-02 -7.4578e-02  ...  -3.1385e-01 -2.5495e-01 -1.2276e-01
  7.3049e-02  1.1170e-01  1.3093e-01  ...  -6.4584e-03 -1.2548e-01 -1.2446e-01
 -6.4210e-03  6.6393e-02  1.2177e-01  ...   1.9075e-01  1.1415e-01  2.3337e-02
     ⋮ 

(1 ,0 ,.,.) = 
  6.8609e-02  3.7955e-02  5.3564e-02  ...   2.6891e-02  4.8369e-02  6.3264e-02
  6.1844e-02  1.8407e-02  2.2672e-02  ...  -4.8800e-02 -2.2130e-02 -5.7287e-03
  5.6553e-02  1.4883e-02 -6.9185e-03  ...  -1.2919e-01 -9.5042e-02 -5.8671e-02
                 ...                   ⋱                   ...                
  2.3802e-02 -5.2273e-02 -1.1277e-01  ...  -2.5591e-01 -2.4049e-01 -2.0315e-01
  5.6221e-02 -2.1824e-02 -5.9207e-02  ...  -2.3806e-01 -1.9836e-01 -1.6578e-01
  5.9635e-02  3.7172e-03 -4.8716e-02  ...  -1.6098e-01 -1.4336e-01 -1.0251e-01

(1 ,1 ,.,.) = 
 -9.9041e-02 -7.2129e-02 -7.2748e-02  ...  -3.6232e-02 -8.1876e-02 -8.8119e-02
 -7.0621e-02 -3.9200e-02 -1.0514e-02  ...   5.9519e-02  2.5891e-02 -1.3287e-02
 -9.3963e-02 -2.5318e-02  3.0725e-02  ...   2.0738e-01  1.6162e-01  8.6865e-02
                 ...                   ⋱                   ...                
 -3.9978e-02  6.4148e-02  1.6941e-01  ...   4.5814e-01  3.7839e-01  2.5870e-01
 -6.4985e-02  1.3637e-02  1.3008e-01  ...   3.6661e-01  3.2009e-01  2.0442e-01
 -1.0433e-01 -2.7216e-02  4.1199e-02  ...   2.2797e-01  1.8622e-01  1.1854e-01

(1 ,2 ,.,.) = 
  4.2717e-02  4.7269e-02  1.7798e-02  ...   3.4693e-02  2.8595e-02  4.3173e-02
  3.6308e-02  3.3459e-02 -1.2449e-03  ...   4.2910e-03  4.6513e-03  2.4962e-02
  2.3541e-02  1.6826e-02 -1.5325e-02  ...  -7.2865e-02 -7.2030e-02 -2.3747e-02
                 ...                   ⋱                   ...                
  2.9623e-02  1.8176e-03 -7.9559e-02  ...  -1.8138e-01 -1.6950e-01 -6.1192e-02
  3.5233e-02 -8.5295e-03 -5.4253e-02  ...  -1.5068e-01 -1.2747e-01 -4.5340e-02
  5.4786e-02  3.9805e-02 -1.0473e-02  ...  -4.2332e-02 -5.2220e-02  5.1277e-04
     ⋮ 

(2 ,0 ,.,.) = 
  7.3052e-03  1.5686e-02 -2.4423e-04  ...  -3.2539e-02 -3.1297e-02 -3.0249e-02
  8.6377e-03  2.4622e-02  1.1249e-02  ...  -1.0645e-02 -1.5026e-02 -3.4091e-02
  7.7960e-03  3.1337e-02  2.5109e-02  ...   1.2514e-02 -5.7764e-04 -5.3505e-03
                 ...                   ⋱                   ...                
 -1.0597e-03  1.3395e-02  1.1477e-03  ...   3.6588e-02  4.6791e-02  7.5218e-02
 -1.5273e-02  8.1007e-03 -7.3382e-04  ...   3.7322e-03  1.9803e-02  7.4966e-02
 -1.9642e-02 -1.8543e-03 -6.9084e-03  ...  -3.2996e-03  2.8972e-02  8.7559e-02

(2 ,1 ,.,.) = 
  1.2775e-02  8.8500e-03  2.3261e-03  ...   1.3541e-02  3.6049e-02  4.8399e-02
  1.4153e-02  1.1684e-02 -2.9641e-03  ...   2.7169e-02  5.1851e-02  3.8690e-02
  3.1984e-03  5.0850e-03 -9.3271e-03  ...   4.1137e-02  5.4548e-02  4.5094e-02
                 ...                   ⋱                   ...                
 -1.7035e-02 -4.2867e-02 -8.4639e-02  ...  -4.9248e-02 -2.3901e-02  2.6232e-03
 -2.8155e-03 -8.8370e-03 -4.8081e-02  ...  -8.6432e-02 -7.1647e-02 -2.3398e-02
  2.8788e-02  2.3000e-02 -4.6879e-03  ...  -5.0775e-02 -3.9457e-02 -7.1134e-03

(2 ,2 ,.,.) = 
  9.3511e-03 -3.6039e-02 -3.1732e-02  ...   3.4844e-03  4.3197e-02  4.5083e-02
 -1.1875e-02 -5.8264e-02 -6.3004e-02  ...  -1.7674e-02  1.5884e-02 -7.6554e-03
  1.4215e-02 -3.3805e-02 -5.0096e-02  ...  -2.1366e-03  6.0525e-03 -8.0126e-03
                 ...                   ⋱                   ...                
  8.0828e-03 -4.9916e-02 -9.0253e-02  ...  -8.7565e-02 -9.9468e-02 -1.0924e-01
  1.8781e-02 -1.4367e-02 -4.6708e-02  ...  -1.1187e-01 -1.4514e-01 -1.4444e-01
  5.7803e-02  1.7252e-02 -5.7767e-03  ...  -7.4414e-02 -1.1313e-01 -1.2434e-01
...   
     ⋮ 

(61,0 ,.,.) = 
  1.5639e-02  2.0411e-02  3.5717e-02  ...   7.1576e-03  4.5493e-02 -7.6534e-03
 -2.1845e-02  5.6129e-02  6.0663e-02  ...  -5.6907e-02  8.5169e-02 -2.6663e-02
 -4.7579e-02  1.0689e-01  7.5667e-02  ...  -8.8005e-02  1.5159e-01 -3.2074e-02
                 ...                   ⋱                   ...                
  1.4774e-03  1.2554e-01 -4.9284e-02  ...   5.9099e-02  1.5756e-01 -4.1167e-02
  5.0183e-03  6.2567e-02 -5.8753e-02  ...   4.7302e-02  8.2366e-02 -4.6016e-02
  1.6201e-02  5.4047e-03 -6.5849e-02  ...   5.1001e-02  4.6469e-02 -1.6759e-02

(61,1 ,.,.) = 
  1.0200e-02  4.4592e-02  1.5321e-03  ...  -6.6545e-02  4.5731e-02  4.8529e-02
  4.5088e-03  1.2873e-01  3.0149e-02  ...  -1.4642e-01  1.4743e-01  8.3739e-02
  5.6123e-03  2.1745e-01  3.0934e-02  ...  -1.8033e-01  2.6112e-01  9.4205e-02
                 ...                   ⋱                   ...                
  4.6450e-02  1.9461e-01 -1.3893e-01  ...   4.3828e-02  2.8577e-01  5.1682e-02
  2.5445e-02  9.4402e-02 -1.3597e-01  ...   5.5534e-02  1.5707e-01 -1.3938e-02
  1.6235e-02  2.9242e-02 -1.0514e-01  ...   7.2561e-02  8.2445e-02 -1.0449e-02

(61,2 ,.,.) = 
  7.6312e-04  2.2041e-02  1.2029e-02  ...  -3.9314e-02  7.8765e-03  1.1339e-02
 -2.6744e-02  7.6046e-02  6.5150e-02  ...  -5.3509e-02  7.7586e-02  1.5460e-02
 -4.3151e-02  1.2443e-01  7.8582e-02  ...  -7.2363e-02  1.3668e-01 -3.7664e-03
                 ...                   ⋱                   ...                
 -4.8974e-03  1.2171e-01 -4.9993e-02  ...   4.0816e-02  1.2784e-01 -2.7264e-02
 -5.7832e-03  6.6111e-02 -4.9786e-02  ...   3.8487e-02  6.9511e-02 -4.3904e-02
  3.3817e-03  3.2986e-02 -4.2698e-02  ...   4.3439e-02  3.3467e-02 -2.9978e-02
     ⋮ 

(62,0 ,.,.) = 
  4.5842e-02  5.2311e-02  4.4927e-02  ...  -2.9410e-02  4.5506e-03  1.4374e-02
  5.2478e-02  5.1215e-02  4.7960e-02  ...  -1.1274e-01 -8.2460e-02 -2.5520e-02
  9.0280e-02  7.7345e-02  6.7258e-02  ...  -2.1452e-01 -1.1146e-01 -1.7177e-02
                 ...                   ⋱                   ...                
  3.2732e-02  7.1492e-05 -1.5341e-01  ...  -2.5537e-01 -1.1447e-01  4.2084e-02
  1.9229e-02 -2.7870e-02 -1.4789e-01  ...  -2.4871e-01 -3.0199e-02  8.2352e-02
 -6.6101e-03 -5.7227e-02 -1.5145e-01  ...  -1.8681e-01  9.7780e-03  9.8369e-02

(62,1 ,.,.) = 
 -3.1084e-03  1.2002e-02  1.6918e-02  ...   4.7799e-03  4.7867e-03 -8.5661e-04
  1.9549e-02  1.5186e-02  3.4710e-02  ...  -4.2644e-03 -1.3299e-02 -5.1674e-03
  2.0221e-02  1.1181e-02  5.0552e-02  ...  -6.5437e-02 -6.4139e-03  1.8563e-02
                 ...                   ⋱                   ...                
 -1.1129e-02  1.0629e-02 -4.2425e-02  ...  -4.8182e-02 -2.8246e-02  4.6424e-02
  7.5653e-03  1.1309e-02 -1.3535e-02  ...  -6.6639e-02  1.0465e-02  3.6264e-02
  9.8492e-03  6.3770e-03 -1.5771e-04  ...  -3.0868e-02  2.6524e-02  3.0090e-02

(62,2 ,.,.) = 
 -4.2418e-02 -2.3199e-02 -2.2478e-02  ...   1.4661e-02  1.2153e-02 -2.1124e-02
 -2.9424e-02 -2.5660e-02  1.4858e-03  ...   4.2752e-02  2.1531e-02 -5.6081e-03
 -3.9465e-02 -4.5056e-02  1.4975e-02  ...   3.0639e-02  4.1439e-02 -7.7498e-03
                 ...                   ⋱                   ...                
 -3.1252e-02  7.7716e-03  1.5953e-02  ...   6.3187e-02  4.8559e-03 -2.9812e-02
 -1.2930e-02  1.6789e-02  4.8435e-02  ...   4.6301e-02  2.6589e-02 -4.1627e-02
 -1.7138e-02  7.2573e-03  4.8229e-02  ...   3.5378e-02  1.4400e-02 -4.7261e-02
     ⋮ 

(63,0 ,.,.) = 
  1.9836e-02 -2.8316e-02  6.1643e-02  ...  -5.2705e-02  9.8331e-03  4.9678e-03
 -4.6186e-02  5.3722e-02 -2.6572e-02  ...   1.9153e-02 -2.2031e-02 -1.3209e-02
  5.2939e-02 -4.4223e-02 -3.7457e-02  ...  -2.1368e-01 -2.1620e-03  3.4433e-02
                 ...                   ⋱                   ...                
 -3.5689e-02  9.3666e-02  1.1909e-01  ...   5.2430e-01 -4.5373e-02 -5.9195e-02
 -1.1405e-02  1.0079e-01 -1.9849e-01  ...   2.3351e-01 -2.6296e-01  1.2434e-01
 -1.4071e-02  5.2204e-03 -7.5740e-02  ...  -5.6693e-02  3.3713e-02 -3.4239e-02

(63,1 ,.,.) = 
  1.8626e-02 -3.9080e-02  4.1489e-02  ...  -4.2309e-02  2.7354e-02 -5.3296e-03
 -4.5311e-02  6.2526e-02 -1.8939e-02  ...   2.9117e-02  3.0331e-03 -1.8276e-03
  3.6988e-02 -6.5929e-02 -2.1645e-02  ...  -2.6028e-01  5.6034e-02  3.7797e-02
                 ...                   ⋱                   ...                
 -3.4665e-02  1.2999e-01  1.2347e-01  ...   6.5182e-01 -1.6673e-02 -1.2347e-01
 -1.9596e-02  1.2236e-01 -2.4884e-01  ...   3.3043e-01 -3.2030e-01  9.4385e-02
  2.0938e-03  2.5218e-02 -8.2988e-02  ...  -6.9019e-02  2.0379e-02 -1.5437e-02

(63,2 ,.,.) = 
  1.3068e-02 -3.7082e-02  1.9962e-02  ...  -4.6411e-02  2.6141e-02  2.5953e-03
 -3.3303e-02  7.5309e-02 -2.8833e-02  ...   4.7499e-02 -7.2355e-03 -1.5955e-02
  6.1195e-02 -4.4915e-02 -1.3650e-01  ...  -1.3014e-01  6.3804e-03  1.3756e-02
                 ...                   ⋱                   ...                
 -1.5692e-02  8.1412e-03  1.4574e-01  ...   4.6457e-01 -1.6762e-01 -2.8489e-02
 -4.5328e-02  3.7344e-02 -1.2535e-01  ...   1.0561e-01 -2.7718e-01  1.6692e-01
  1.0456e-02  1.9020e-02 -1.5351e-02  ...  -1.1350e-01  6.7615e-02 -6.7650e-03
[torch.FloatTensor of size 64x3x7x7]
), ('bn1.weight', 
 2.3888e-01
 2.9136e-01
 3.1615e-01
 2.7122e-01
 2.1731e-01
 3.0903e-01
 2.2937e-01
 2.3086e-01
 2.1129e-01
 2.8054e-01
 1.9923e-01
 3.1894e-01
 1.7955e-01
 1.1246e-08
 1.9704e-01
 2.0996e-01
 2.4317e-01
 2.1697e-01
 1.9415e-01
 3.1569e-01
 1.9648e-01
 2.3214e-01
 2.1962e-01
 2.1633e-01
 2.4357e-01
 2.9683e-01
 2.3852e-01
 2.1162e-01
 1.4492e-01
 2.9388e-01
 2.2911e-01
 9.2716e-02
 4.3334e-01
 2.0782e-01
 2.7990e-01
 3.5804e-01
 2.9315e-01
 2.5306e-01
 2.4210e-01
 2.1755e-01
 3.8645e-01
 2.1003e-01
 3.6805e-01
 3.3724e-01
 5.0826e-01
 1.9341e-01
 2.3914e-01
 2.6652e-01
 3.9020e-01
 1.9840e-01
 2.1694e-01
 2.6666e-01
 4.9806e-01
 2.3553e-01
 2.1349e-01
 2.5951e-01
 2.3547e-01
 1.7579e-01
 4.5354e-01
 1.7102e-01
 2.4903e-01
 2.5148e-01
 3.8020e-01
 1.9665e-01
[torch.FloatTensor of size 64]
), ('bn1.bias', 
 2.2484e-01
 6.0617e-01
 1.2483e-02
 1.3270e-01
 1.8030e-01
 1.4739e-01
 1.7430e-01
 1.9023e-01
 2.3226e-01
 2.0082e-01
 1.2834e-01
-2.1285e-01
 1.5065e-01
-3.9217e-08
 2.4985e-01
 2.0454e-01
 5.4934e-01
 2.1021e-01
 2.2505e-01
 4.6484e-01
 2.3888e-01
 2.0442e-01
 2.1546e-01
 6.6194e-01
 2.2755e-01
 6.6069e-01
 2.0587e-01
 1.9292e-01
 1.1195e-01
 3.3785e-01
 1.2393e-01
 4.1079e-02
 7.7150e-01
 2.6964e-01
 3.3347e-01
 5.7908e-01
 1.5026e-01
 1.7534e-01
 1.9429e-01
 1.7248e-01
 8.0577e-01
 2.3693e-01
-4.3369e-01
 8.4813e-01
-3.7857e-01
 2.4787e-01
 1.8101e-01
 3.2949e-01
-2.8598e-01
 2.2717e-01
 2.6168e-01
 5.7609e-02
-5.0320e-01
 1.5704e-01
 1.7890e-01
 2.8114e-01
 4.2167e-01
-9.7650e-02
-3.1231e-01
-2.5637e-02
 8.8566e-02
 1.8052e-01
 8.3045e-01
 2.5015e-01
[torch.FloatTensor of size 64]
), ('bn1.running_mean', 
 2.4589e-04
-6.6723e-02
 8.1560e-03
-1.1569e-02
 9.1918e-04
-3.9417e-03
 6.8244e-05
-1.0278e-04
 6.2920e-05
 1.0156e-02
-1.3827e-03
 8.5514e-03
-9.6786e-04
-5.0542e-09
-1.3922e-03
 9.7684e-04
-4.2287e-03
-4.3058e-04
 8.8580e-04
-5.7929e-02
 3.4196e-03
-6.9578e-04
-2.2842e-03
 4.3616e-02
 2.6490e-03
 4.7517e-03
-1.1206e-03
-3.1145e-04
 2.6560e-03
 5.6712e-02
-6.8490e-04
-5.3309e-03
-1.4519e-02
-5.2398e-04
-3.5607e-02
 8.8422e-02
 1.7903e-03
 4.4882e-03
 1.6055e-03
-3.0522e-04
 2.9796e-03
-1.2307e-03
 7.8966e-03
 5.9528e-03
 9.8027e-03
-1.4366e-03
 3.5911e-04
-9.7358e-04
-9.1738e-03
 7.6298e-05
-3.3611e-04
-4.8319e-05
 3.6836e-03
-2.1531e-03
-2.1549e-04
-8.7561e-07
 3.4184e-02
-3.5004e-03
 1.1910e-02
-1.0551e-03
 5.2965e-04
 6.0597e-04
-8.4048e-03
-1.6792e-03
[torch.FloatTensor of size 64]
), ('bn1.running_var', 
 1.0956e+00
 3.8935e+00
 4.8601e+00
 5.0922e+00
 5.9482e-01
 6.5676e+00
 6.8618e-01
 6.4897e-01
 4.3926e-01
 4.5669e+00
 4.0052e-01
 2.8487e+00
 2.9018e-01
 2.9161e-12
 2.0027e-01
 5.0347e-01
 1.5794e+00
 5.2107e-01
 1.8563e-01
 8.1094e+00
 2.4821e-01
 6.3539e-01
 3.5313e-01
 2.2571e+00
 1.0236e+00
 3.9252e+00
 7.3166e-01
 6.3357e-01
 7.6921e-02
 5.1747e+00
 1.3232e+00
 5.5709e-02
 1.4234e+01
 1.0767e-01
 8.3864e+00
 1.3844e+01
 5.6570e+00
 2.8593e+00
 9.8436e-01
 7.5888e-01
 1.0733e+01
 2.2787e-01
 2.9440e+00
 5.1670e+00
 9.2119e+00
 4.5519e-01
 6.2876e-01
 2.5094e+00
 1.5883e+00
 2.9337e-01
 4.9738e-01
 1.9022e+00
 4.4529e+00
 6.9012e-01
 6.8237e-01
 1.9428e+00
 1.2602e+00
 3.6957e-01
 4.8405e+00
 4.0578e-01
 1.2564e+00
 1.3468e+00
 9.8125e+00
 2.0060e-01
[torch.FloatTensor of size 64]
), ('layer1.0.conv1.weight', 
(0 ,0 ,.,.) = 
  3.5144e-03

(0 ,1 ,.,.) = 
  3.9855e-02

(0 ,2 ,.,.) = 
 -2.4795e-02
   ...

(0 ,61,.,.) = 
  1.0283e-01

(0 ,62,.,.) = 
 -2.8931e-01

(0 ,63,.,.) = 
 -1.5175e-01
     ⋮ 

(1 ,0 ,.,.) = 
  1.1493e-01

(1 ,1 ,.,.) = 
  5.6982e-03

(1 ,2 ,.,.) = 
  2.8378e-02
   ...

(1 ,61,.,.) = 
  8.0681e-02

(1 ,62,.,.) = 
 -8.4798e-02

(1 ,63,.,.) = 
 -1.3576e-01
     ⋮ 

(2 ,0 ,.,.) = 
 -1.9008e-02

(2 ,1 ,.,.) = 
  1.1191e-02

(2 ,2 ,.,.) = 
 -1.6003e-02
   ...

(2 ,61,.,.) = 
  7.1472e-02

(2 ,62,.,.) = 
  7.6694e-02

(2 ,63,.,.) = 
 -4.6049e-02
...   
     ⋮ 

(61,0 ,.,.) = 
 -1.9799e-03

(61,1 ,.,.) = 
 -1.9214e-03

(61,2 ,.,.) = 
  2.3714e-02
   ...

(61,61,.,.) = 
  1.6071e-02

(61,62,.,.) = 
 -6.2259e-02

(61,63,.,.) = 
  9.3809e-03
     ⋮ 

(62,0 ,.,.) = 
 -5.2812e-02

(62,1 ,.,.) = 
  1.0688e-02

(62,2 ,.,.) = 
  3.2017e-02
   ...

(62,61,.,.) = 
 -2.1943e-02

(62,62,.,.) = 
 -6.1035e-03

(62,63,.,.) = 
  1.2559e-02
     ⋮ 

(63,0 ,.,.) = 
 -5.6478e-02

(63,1 ,.,.) = 
 -1.1904e-01

(63,2 ,.,.) = 
  4.7069e-02
   ...

(63,61,.,.) = 
  4.1034e-02

(63,62,.,.) = 
  2.4489e-01

(63,63,.,.) = 
 -7.9294e-03
[torch.FloatTensor of size 64x64x1x1]
), ('layer1.0.bn1.weight', 
 2.1341e-01
 1.8848e-01
 1.4136e-01
 1.5273e-01
 1.3220e-01
 1.8735e-01
 1.4475e-01
 4.5110e-08
 1.5993e-01
 1.4946e-01
 2.3499e-01
 1.8315e-01
 1.8516e-01
 1.4933e-01
 1.3090e-01
 1.0634e-01
 3.7487e-01
 1.2644e-01
 3.1895e-01
 2.7160e-01
 2.5810e-01
 2.9458e-01
 1.8395e-01
 2.1088e-08
 3.3313e-01
 2.0461e-01
 3.0399e-01
 1.1805e-08
 1.4977e-01
 1.5719e-01
 1.4011e-01
 1.4900e-01
 1.2438e-01
 1.8786e-01
 1.4257e-01
 3.4828e-01
 1.5038e-01
 3.0034e-01
 2.5925e-01
 1.0711e-01
 2.6875e-01
 1.3552e-01
 1.1822e-01
 1.1189e-01
 2.8736e-01
 3.2637e-01
 1.4781e-01
 2.3105e-01
 3.3638e-01
 2.8808e-01
 1.2319e-01
 3.0763e-01
 1.1846e-01
 1.3137e-01
 2.0671e-01
 1.5787e-01
 2.6574e-08
 2.0467e-01
 2.8797e-08
 1.8284e-01
 3.0180e-01
 1.7401e-01
 2.8438e-01
 2.3715e-01
[torch.FloatTensor of size 64]
), ('layer1.0.bn1.bias', 
 4.3266e-01
 4.6854e-02
-8.0134e-02
 7.3302e-02
 2.7970e-01
-7.8047e-03
 9.4087e-02
-1.0086e-07
-1.4034e-01
-5.1599e-02
 4.4470e-02
 2.1814e-01
 4.0718e-02
 1.1979e-01
 1.4432e-01
 1.3672e-01
-1.1168e-01
 1.4774e-01
-1.2879e-01
-5.3147e-02
-3.3920e-02
-2.0600e-02
 6.2783e-02
-6.5736e-08
-7.1213e-02
 6.9510e-02
-1.3264e-01
-6.4411e-08
-2.8908e-02
 9.4164e-02
 2.4790e-01
-8.2850e-02
-2.8872e-02
-1.7086e-01
 9.9522e-02
-1.1357e-01
 1.9770e-01
 1.4800e-02
-7.0896e-02
 1.0722e-01
 1.2536e-02
-3.6633e-02
 1.4959e-01
 1.0533e-01
 2.0933e-02
-1.0502e-01
-4.8848e-02
 4.9007e-01
-1.4755e-01
-1.0900e-01
 1.9815e-02
-7.0964e-02
-4.6543e-02
 1.0874e-01
-2.7878e-01
 4.4500e-03
-7.7156e-08
 7.5060e-02
-8.4474e-08
 2.2533e-01
-7.1593e-02
-1.5823e-01
-3.4459e-02
 5.2894e-01
[torch.FloatTensor of size 64]
), ('layer1.0.bn1.running_mean', 
-6.5291e-01
-4.0608e-01
 2.6649e-01
-2.8128e-01
-8.8687e-02
-3.4664e-01
-1.1847e-01
-1.5021e-07
 2.1632e-01
-2.9337e-01
-3.5630e-01
 1.2380e+00
-1.3713e-01
-2.1850e-01
-3.4513e-01
-2.0850e-02
-4.1887e-01
 7.9472e-01
-3.7321e-01
-3.1457e-01
-2.5704e-01
-1.5258e-01
-1.8025e-01
-1.1998e-08
-4.3559e-01
-1.9106e-01
-3.0302e-01
 2.8477e-09
-2.5849e-01
-3.3013e-01
-8.8281e-03
 2.2963e-01
 1.9096e-02
-1.3918e-01
-3.1064e-01
-5.0453e-01
-2.3467e-01
-2.4235e-01
-3.0498e-01
 3.7054e-01
-1.4106e-01
-1.2815e-01
-3.3314e-01
-4.9696e-01
-2.4964e-01
-3.5101e-01
-4.1712e-01
-3.3097e-01
-2.3772e-01
-4.8956e-01
-2.4006e-02
-3.3748e-01
 9.8532e-02
-3.6601e-01
-3.8796e-02
-6.9354e-01
-9.1306e-09
-2.6680e-01
-6.6826e-08
 1.1290e+00
-1.5822e-01
-1.1575e-02
-4.5714e-01
-1.1958e-01
[torch.FloatTensor of size 64]
), ('layer1.0.bn1.running_var', 
 1.7619e-01
 5.1697e-02
 9.0866e-03
 1.7317e-02
 4.9763e-02
 1.9986e-02
 2.0359e-02
 3.9287e-15
 1.0058e-02
 1.5385e-02
 7.0793e-02
 1.8359e-01
 3.0475e-02
 1.8266e-02
 1.5837e-02
 8.7857e-03
 1.0126e-01
 7.5200e-02
 8.1127e-02
 7.9580e-02
 5.5055e-02
 1.0592e-01
 3.3010e-02
 7.2025e-16
 9.6578e-02
 3.3688e-02
 8.4036e-02
 5.3741e-16
 1.9374e-02
 2.9053e-02
 6.1685e-02
 1.0359e-02
 8.9361e-03
 1.6872e-02
 1.3151e-02
 1.2134e-01
 6.1661e-02
 9.6685e-02
 8.7319e-02
 2.3794e-02
 5.2243e-02
 1.3271e-02
 2.9673e-02
 9.1149e-03
 8.1366e-02
 9.5220e-02
 1.7207e-02
 1.8998e-01
 1.0934e-01
 8.9131e-02
 1.1704e-02
 7.8835e-02
 9.3558e-03
 1.8481e-02
 1.5793e-02
 3.4199e-02
 1.1201e-15
 3.8694e-02
 1.3307e-15
 1.3738e-01
 1.0056e-01
 1.6990e-02
 8.9689e-02
 2.2839e-01
[torch.FloatTensor of size 64]
), ('layer1.0.conv2.weight', 
(0 ,0 ,.,.) = 
  2.0919e-09 -1.4152e-09  6.5850e-09
  4.7922e-09  3.2819e-09  9.4284e-10
  6.4763e-09 -3.0156e-09 -2.3958e-09

(0 ,1 ,.,.) = 
  1.1471e-08  1.2815e-08  1.3568e-08
  1.0737e-08  4.9712e-09  3.8516e-09
  1.0572e-08  1.2153e-08  6.9175e-09

(0 ,2 ,.,.) = 
 -6.8447e-09 -2.8470e-09 -1.5396e-09
  4.8804e-09 -3.0935e-09  2.3919e-09
  5.2998e-09 -1.4133e-11 -7.5312e-09
   ...

(0 ,61,.,.) = 
 -6.4828e-09 -7.6425e-09 -9.5572e-09
 -7.0080e-09 -4.0208e-09 -4.5819e-09
 -4.9349e-09 -5.4604e-10 -1.3119e-08

(0 ,62,.,.) = 
 -1.0458e-09 -6.9635e-09 -5.1090e-09
 -3.4306e-10 -4.2656e-09 -2.2703e-09
 -1.8570e-09 -4.8467e-09 -3.5571e-09

(0 ,63,.,.) = 
  1.8746e-09  8.6741e-10 -5.5741e-10
  2.6233e-10 -5.0061e-09  5.8350e-09
  2.4322e-10  1.6342e-10  3.3284e-09
     ⋮ 

(1 ,0 ,.,.) = 
 -2.3484e-02  1.8706e-03  2.5798e-02
  3.5355e-03 -5.2646e-03  1.9523e-02
  2.4600e-02  2.1933e-02  1.6219e-04

(1 ,1 ,.,.) = 
 -3.3788e-03  3.2976e-03  1.2707e-02
  1.7570e-02  1.5054e-02  1.7517e-02
  2.2276e-02  2.2339e-02 -5.5213e-03

(1 ,2 ,.,.) = 
 -3.8421e-03 -5.9462e-03  1.5058e-02
 -3.1875e-03 -2.5218e-02 -3.3736e-03
  4.9239e-04 -1.9673e-02 -1.8729e-02
   ...

(1 ,61,.,.) = 
  5.2840e-03  6.6714e-03  1.1301e-02
  1.3283e-02  9.0130e-03  6.2237e-03
  2.1055e-02  1.5775e-02  8.4497e-03

(1 ,62,.,.) = 
  5.9612e-04  2.7906e-03  6.5433e-03
  8.4950e-03  9.2682e-02  5.1201e-03
 -9.0937e-03 -6.4196e-03 -2.7363e-04

(1 ,63,.,.) = 
 -2.0518e-02 -4.6551e-03 -2.9206e-02
 -1.1651e-02  1.6589e-02 -1.8225e-03
 -3.0077e-02  4.2854e-03  3.0994e-03
     ⋮ 

(2 ,0 ,.,.) = 
  3.8186e-02  4.5909e-02  2.6377e-02
  4.5241e-02  6.5042e-02  5.4767e-02
  4.5518e-02  5.9040e-02  3.6224e-02

(2 ,1 ,.,.) = 
  6.4452e-03 -9.0585e-04  7.1830e-03
  6.7971e-03  6.3635e-03  2.1455e-02
  1.1262e-02  1.6089e-02  1.7535e-02

(2 ,2 ,.,.) = 
  4.0125e-03 -1.5436e-02  6.1919e-03
 -1.6548e-02 -3.7222e-03  2.9918e-02
 -4.1074e-03 -4.8143e-03  1.5329e-02
   ...

(2 ,61,.,.) = 
  2.4917e-02  1.9070e-02  3.3504e-02
  3.1252e-02 -7.5642e-03  3.5888e-02
  2.9346e-02  3.3745e-02  4.2878e-02

(2 ,62,.,.) = 
  6.5392e-03 -1.6478e-02  2.1843e-02
 -1.3546e-02  2.4295e-03  6.9680e-03
  3.1423e-03 -3.7818e-03  3.2182e-03

(2 ,63,.,.) = 
  7.3159e-03 -2.3604e-02 -1.1507e-02
 -3.3326e-02 -4.4065e-02 -4.6069e-02
 -3.1847e-02 -3.7215e-02 -2.4266e-02
...   
     ⋮ 

(61,0 ,.,.) = 
  1.8211e-09  2.9120e-09 -5.6310e-09
  4.3212e-09 -1.8635e-09 -3.3057e-09
  2.2608e-09 -1.2302e-09  1.6536e-10

(61,1 ,.,.) = 
  1.9862e-09 -2.7085e-09 -1.5189e-08
  9.0284e-09  2.6774e-09 -2.3783e-09
  5.4744e-09  4.7255e-09 -5.1706e-09

(61,2 ,.,.) = 
  4.2245e-10 -6.2215e-09 -7.1381e-09
 -4.5743e-09 -8.9245e-09 -4.1074e-09
 -1.7179e-09 -1.4299e-10 -4.7585e-09
   ...

(61,61,.,.) = 
 -1.6792e-09  3.7950e-09  5.4831e-10
 -4.9152e-09 -9.4884e-09 -6.3708e-09
  8.2731e-09  3.9463e-09  3.8983e-09

(61,62,.,.) = 
  4.0006e-09 -1.3585e-09 -2.7413e-12
 -1.2756e-09  2.5309e-09 -4.6894e-09
  4.9880e-10  3.3068e-09 -8.5953e-09

(61,63,.,.) = 
  2.7820e-09  7.1263e-10  3.2218e-09
  3.4826e-09  5.6161e-09 -3.8766e-09
  2.3460e-09  6.0292e-09  4.8710e-10
     ⋮ 

(62,0 ,.,.) = 
  8.1531e-02  5.8091e-02  3.6932e-02
  3.4071e-02 -1.3114e-02 -2.9585e-02
  1.0524e-02 -3.7806e-02 -5.0393e-02

(62,1 ,.,.) = 
 -7.4381e-03 -1.9210e-03 -3.3986e-03
  1.3362e-02  1.5364e-02 -1.0274e-03
 -7.1327e-03 -2.5406e-02 -3.1194e-02

(62,2 ,.,.) = 
  6.0968e-03 -1.9646e-02 -8.9450e-03
  1.4864e-03 -1.2455e-02  1.0394e-02
 -2.6015e-03 -1.3369e-02  1.5983e-02
   ...

(62,61,.,.) = 
  3.0871e-03  6.2054e-04 -6.2765e-03
 -4.0756e-03 -1.3461e-02 -1.4281e-02
 -1.0118e-02 -2.1184e-02 -1.2876e-02

(62,62,.,.) = 
  2.3335e-03  1.8122e-02  1.0758e-03
  1.1701e-02  2.8367e-02  6.5168e-03
 -1.4372e-02  1.6207e-02  1.3691e-02

(62,63,.,.) = 
 -6.2637e-02 -2.5556e-02  4.6860e-03
 -3.8629e-02  3.0089e-02  4.4090e-02
  1.2748e-03  7.1193e-02  6.3498e-02
     ⋮ 

(63,0 ,.,.) = 
 -4.4771e-02 -3.8409e-02  9.7917e-02
 -1.7643e-01  3.2383e-03  2.8121e-01
 -1.0757e-01 -4.9282e-02  1.4446e-01

(63,1 ,.,.) = 
  5.1254e-03  1.9011e-03  4.1052e-02
 -3.4516e-02 -1.4285e-02  2.9000e-02
 -3.2176e-02 -3.1341e-03  5.7282e-02

(63,2 ,.,.) = 
 -3.8417e-02 -5.1169e-02 -2.7389e-02
 -3.8442e-02 -1.0360e-01 -8.7211e-02
 -4.4993e-02 -5.8818e-02 -3.8287e-02
   ...

(63,61,.,.) = 
 -2.1003e-02 -1.7952e-02  6.6377e-03
 -5.5188e-02 -1.3361e-03  6.0603e-02
 -1.4080e-02  1.6523e-03  2.5246e-02

(63,62,.,.) = 
  2.2449e-03  1.1214e-02  1.3945e-02
 -1.1470e-02 -6.9804e-03  4.3370e-02
 -1.2780e-02  2.6532e-03  6.3033e-02

(63,63,.,.) = 
  1.9265e-01 -4.7934e-02 -1.4074e-01
  3.7943e-01 -1.4166e-02 -3.0701e-01
  2.4804e-01 -5.6059e-02 -1.9182e-01
[torch.FloatTensor of size 64x64x3x3]
), ('layer1.0.bn2.weight', 
 2.3224e-08
 1.3805e-01
 2.4155e-01
 1.3627e-01
 1.1577e-01
 1.8789e-01
 1.2005e-01
 1.3625e-01
 1.7837e-01
 1.3934e-01
 1.0924e-01
 1.7452e-01
 1.3525e-01
 1.5064e-01
 1.3912e-01
 1.2596e-01
 1.5728e-01
 1.3527e-01
 2.2781e-01
 4.6913e-07
 2.7175e-01
 1.6913e-01
 2.0877e-01
 2.0823e-01
 1.5117e-01
 1.7594e-01
 1.7099e-01
 1.8060e-01
 1.8968e-01
 1.8072e-01
 1.5526e-01
 1.3217e-01
 1.5328e-01
 2.2601e-01
 1.4130e-01
 1.4350e-01
 1.5354e-01
 1.6157e-01
 1.7243e-01
 2.2057e-01
 2.0561e-01
 2.2331e-01
 1.7635e-01
 1.1320e-01
 1.7612e-01
 2.0285e-01
 1.0247e-01
 2.6118e-01
 1.6209e-01
 1.4597e-01
 1.6170e-01
 1.7936e-01
 1.8842e-01
 1.2481e-01
 1.9059e-01
 1.7148e-01
 2.0484e-01
 1.3645e-01
 1.7423e-01
 1.5198e-03
 1.7775e-01
 9.5639e-09
 1.7993e-01
 2.3351e-01
[torch.FloatTensor of size 64]
), ('layer1.0.bn2.bias', 
-1.1646e-07
 3.8030e-02
 3.5188e-01
 1.2425e-01
 2.7166e-01
 3.4886e-01
 2.8938e-01
 1.5056e-01
-1.4002e-01
-8.9164e-02
-2.8543e-02
 3.1789e-01
 2.8283e-01
-3.4367e-02
-1.3814e-01
 2.8288e-01
 3.7119e-01
-1.0956e-01
 2.2031e-01
-1.4170e-06
 8.4392e-02
 1.4088e-01
-4.2237e-02
 4.2857e-01
 1.0983e-01
 1.7426e-01
 1.6968e-01
-2.6071e-02
-5.6061e-02
-2.9518e-02
 7.3970e-02
 3.1024e-01
 3.4015e-01
 5.2495e-01
-2.8603e-03
-9.6186e-02
 3.9447e-01
 1.6469e-01
 1.0085e-01
 1.0875e-01
 3.6777e-01
 7.9735e-02
 6.4225e-02
 2.2529e-01
-5.5575e-02
-5.9820e-02
 2.7217e-01
-2.2807e-01
 3.8712e-01
-2.7749e-02
 3.7759e-01
 1.4439e-01
 8.5736e-02
 1.6832e-01
-4.1062e-03
 2.0312e-02
-2.0508e-01
-4.9455e-02
-1.2807e-01
-6.6944e-03
-5.0324e-02
-4.7086e-08
-4.9077e-02
 9.1173e-02
[torch.FloatTensor of size 64]
), ('layer1.0.bn2.running_mean', 
 5.0454e-08
 8.0653e-04
 9.6387e-02
-7.1218e-02
-1.2185e-02
-3.2577e-02
-1.2039e-02
 8.2556e-02
 1.2451e-01
 7.5336e-02
-5.6711e-02
 6.1111e-02
-3.2641e-02
 1.5730e-01
 5.1263e-02
 1.1029e-01
-1.3518e-02
 6.8737e-02
-2.0974e-01
 1.1555e-06
 1.9133e-01
 2.0825e-02
 1.2151e-01
 2.9969e-01
 1.5213e-01
 1.5418e-01
 4.2578e-01
 1.6260e-01
-1.9899e-01
 2.0943e-01
-3.0306e-02
 7.4573e-02
 7.2496e-02
-6.0348e-02
 2.1292e-02
-9.3666e-02
 1.4028e-01
 3.9296e-02
 1.1735e-01
 2.5877e-01
 1.2679e-01
 1.7135e-01
 2.0394e-01
-7.9133e-02
 2.3914e-01
 1.4239e-01
 6.1945e-02
-4.2711e-01
 4.2302e-02
-1.2335e-01
-3.5943e-02
 2.5419e-02
 2.9738e-01
-1.1664e-01
-2.5950e-01
-1.2490e-01
-6.6262e-01
-1.7787e-03
-3.4706e-02
-5.9543e-03
-2.3221e-01
-2.5439e-08
 2.9606e-01
 2.1416e-01
[torch.FloatTensor of size 64]
), ('layer1.0.bn2.running_var', 
 5.6544e-16
 1.6101e-02
 1.3729e-01
 1.8011e-02
 1.1142e-02
 5.5350e-02
 2.1735e-02
 1.1938e-02
 1.2029e-02
 6.1643e-03
 3.0570e-03
 6.1368e-02
 2.8438e-02
 1.6098e-02
 5.6585e-03
 2.5052e-02
 2.3762e-02
 7.1350e-03
 9.9083e-02
 7.9400e-13
 1.0512e-01
 2.1955e-02
 1.9802e-02
 1.0576e-01
 1.4998e-02
 2.7837e-02
 1.3126e-02
 2.5028e-02
 2.1796e-02
 2.3617e-02
 2.3042e-02
 5.2243e-02
 5.5837e-02
 1.0006e-01
 9.6541e-03
 4.6445e-03
 7.7565e-02
 1.8963e-02
 2.4567e-02
 6.8292e-02
 1.0630e-01
 8.1392e-02
 3.6109e-02
 2.6201e-02
 2.1351e-02
 1.2851e-02
 1.5734e-02
 1.1305e-02
 5.5519e-02
 1.4701e-02
 5.3849e-02
 3.3676e-02
 2.2656e-02
 4.4932e-03
 1.9725e-02
 1.5899e-02
 1.2054e-02
 4.4893e-03
 8.7057e-03
 1.1669e-04
 1.8264e-02
 3.8467e-16
 1.7363e-02
 6.9920e-02
[torch.FloatTensor of size 64]
), ('layer1.0.conv3.weight', 
( 0 , 0 ,.,.) = 
 -4.0119e-09

( 0 , 1 ,.,.) = 
  1.7726e-02

( 0 , 2 ,.,.) = 
  4.9594e-02
    ... 

( 0 ,61 ,.,.) = 
  1.0813e-08

( 0 ,62 ,.,.) = 
 -1.7592e-02

( 0 ,63 ,.,.) = 
 -4.8830e-02
      ⋮  

( 1 , 0 ,.,.) = 
  5.7415e-09

( 1 , 1 ,.,.) = 
  2.7212e-03

( 1 , 2 ,.,.) = 
  2.0600e-03
    ... 

( 1 ,61 ,.,.) = 
  2.3281e-09

( 1 ,62 ,.,.) = 
  3.7770e-03

( 1 ,63 ,.,.) = 
 -3.4820e-04
      ⋮  

( 2 , 0 ,.,.) = 
 -2.2345e-09

( 2 , 1 ,.,.) = 
 -1.2277e-05

( 2 , 2 ,.,.) = 
 -8.5743e-03
    ... 

( 2 ,61 ,.,.) = 
 -3.2814e-09

( 2 ,62 ,.,.) = 
  7.8754e-04

( 2 ,63 ,.,.) = 
 -1.1076e-02
...     
      ⋮  

(253, 0 ,.,.) = 
  6.1314e-09

(253, 1 ,.,.) = 
 -7.2828e-04

(253, 2 ,.,.) = 
  3.0347e-03
    ... 

(253,61 ,.,.) = 
 -1.2589e-08

(253,62 ,.,.) = 
  4.5773e-03

(253,63 ,.,.) = 
  6.7829e-05
      ⋮  

(254, 0 ,.,.) = 
 -7.4988e-09

(254, 1 ,.,.) = 
 -6.2381e-03

(254, 2 ,.,.) = 
  9.3970e-02
    ... 

(254,61 ,.,.) = 
  3.6372e-09

(254,62 ,.,.) = 
 -4.9588e-02

(254,63 ,.,.) = 
  1.0847e-03
      ⋮  

(255, 0 ,.,.) = 
  1.4321e-09

(255, 1 ,.,.) = 
 -4.5942e-03

(255, 2 ,.,.) = 
 -1.7466e-02
    ... 

(255,61 ,.,.) = 
  6.1837e-09

(255,62 ,.,.) = 
  2.9926e-02

(255,63 ,.,.) = 
 -1.0834e-01
[torch.FloatTensor of size 256x64x1x1]
), ('layer1.0.bn3.weight', 
 6.1257e-02
 3.2458e-04
 3.6922e-02
 3.1272e-01
 5.7752e-02
 5.3033e-02
 1.1429e-01
 3.8885e-01
 9.7341e-02
 1.8650e-01
 2.7582e-01
 1.8488e-01
 2.1568e-01
 2.1534e-01
 2.3184e-01
 1.6155e-01
 2.2608e-01
 3.4452e-01
 7.7296e-02
 1.5048e-01
 1.3025e-01
 2.1159e-01
 1.1078e-06
 9.5138e-09
 1.7439e-01
 4.9364e-04
 7.4613e-03
 2.7059e-01
 2.1378e-01
 2.0946e-01
 1.7827e-01
 3.3235e-01
 2.4776e-01
 1.8123e-01
 1.2957e-01
 2.0325e-01
 1.4420e-01
 2.1640e-01
 7.3485e-04
 2.2436e-02
-1.1337e-02
 2.1912e-01
 1.7915e-01
 2.3578e-01
 1.2075e-01
 1.5954e-05
 8.8865e-02
 1.4537e-01
 8.8577e-02
-1.0227e-02
 2.1639e-01
 3.4835e-02
 1.6429e-07
 1.3006e-01
 7.1514e-02
 1.6472e-01
 7.2320e-03
 2.2837e-01
 1.8653e-01
 2.0731e-01
 1.6906e-01
 1.5374e-01
 1.6835e-01
 9.4061e-02
 7.8185e-08
 3.9745e-03
 8.6304e-02
 2.6490e-01
 1.6982e-01
 5.5553e-03
 2.8270e-02
-2.6206e-07
 2.6815e-01
 5.8706e-02
 3.1914e-01
 5.1964e-02
 9.1686e-04
 5.5850e-02
 2.8677e-01
 1.7436e-01
 2.1439e-01
 2.4483e-01
 1.6849e-01
-4.5969e-06
 2.4238e-01
 1.7576e-01
 1.4963e-01
 7.5519e-07
 1.7493e-01
 9.7360e-07
-7.9266e-04
 5.9956e-02
 3.1109e-03
 3.9409e-04
 2.0501e-02
 8.4718e-02
 2.0573e-01
 2.8278e-01
 9.7521e-02
 1.1674e-01
 2.6729e-01
 3.7739e-07
 2.7196e-01
 1.0523e-01
 2.5316e-01
 1.0540e-01
 2.1283e-01
 7.0997e-07
 1.5051e-02
 2.5951e-01
 8.4987e-02
 2.1008e-01
-4.4472e-03
-9.5576e-02
 2.4416e-01
 2.6090e-01
 2.8566e-01
 1.6807e-08
 2.2492e-01
 2.8301e-01
 1.8392e-01
 2.0118e-01
 2.8446e-01
 1.7130e-05
 7.6160e-02
 2.1699e-01
 1.9139e-01
 1.6825e-05
 6.9757e-02
 2.0047e-01
 1.9512e-01
 1.4369e-01
 1.6181e-01
 1.0848e-06
 1.0534e-06
 1.7272e-02
 1.3205e-01
 5.6653e-02
 2.2185e-01
 9.6189e-02
 6.2859e-02
 3.1226e-01
 7.7507e-02
 1.2070e-01
 5.8322e-02
 2.3725e-01
 2.5693e-01
 1.1205e-01
 6.6947e-02
 1.5241e-01
 8.6475e-02
 6.2732e-02
 8.2831e-02
-8.1404e-02
 1.3252e-01
 1.3642e-01
 1.3644e-01
 1.4410e-01
 2.1566e-01
-1.1276e-01
 2.8843e-01
 2.2682e-01
 2.1664e-01
-1.8349e-02
 3.6098e-02
 1.3628e-02
 3.6618e-02
 2.6597e-01
 1.5605e-01
 1.1028e-01
-1.6914e-02
 2.4349e-01
 1.2086e-01
 1.3346e-01
 1.1930e-05
 6.3407e-02
 2.4949e-01
 2.0691e-01
 1.6867e-01
 1.5140e-01
 1.6620e-01
 6.7047e-02
 1.4889e-01
 2.4088e-01
-6.1604e-03
-1.7191e-06
 1.6970e-01
 2.7898e-01
 2.5087e-01
 7.2350e-06
 1.1479e-01
 3.0211e-03
 8.4946e-02
 1.8868e-01
 5.3232e-03
 9.9272e-02
 1.4196e-01
 2.1632e-01
 1.9907e-01
 1.5927e-05
 2.6186e-01
 3.2927e-01
 9.0553e-07
 2.8694e-01
-4.3297e-08
 3.2081e-03
 1.4184e-01
-9.3964e-02
 1.3650e-01
 2.2932e-01
 1.5370e-01
-1.0009e-01
 9.0725e-02
 1.5219e-01
 8.0811e-07
 2.5773e-01
 1.8997e-01
 1.8119e-01
 2.0239e-01
 2.5482e-01
-2.8486e-04
 1.5002e-01
 7.8557e-02
 5.6243e-02
 6.7529e-02
 1.7906e-01
-8.9718e-04
 1.5554e-01
 4.1687e-03
 1.4187e-02
 1.5717e-01
 2.7333e-01
 3.5062e-02
 2.7199e-01
 2.0747e-01
 6.5914e-02
 1.5763e-01
-1.5537e-03
 6.2626e-02
 4.1613e-02
 1.0547e-01
-2.1628e-02
-4.1070e-03
 2.8397e-01
 1.7258e-01
 6.1170e-02
-1.5988e-05
 7.2228e-02
 1.1727e-01
 1.6374e-01
 2.1004e-01
 5.8066e-02
 2.6768e-01
 1.4760e-03
 2.9346e-01
 1.4359e-01
[torch.FloatTensor of size 256]
), ('layer1.0.bn3.bias', 
-4.7521e-03
 7.3382e-02
 2.9881e-02
 5.4250e-02
-4.2102e-02
 4.5748e-02
 7.0444e-02
 1.3612e-01
 9.5970e-02
 8.6802e-02
 4.6588e-02
 8.7879e-03
-5.5630e-03
 3.5592e-02
 6.0245e-02
 2.9497e-02
 1.1487e-01
 1.0022e-01
-7.0565e-02
 5.7853e-02
 5.9134e-02
 3.1986e-02
-2.0646e-06
-1.1174e-07
 9.5474e-02
-1.2143e-03
-1.0645e-01
-7.6647e-03
 4.3269e-02
 1.0157e-02
 3.3298e-02
 1.0682e-02
 5.0505e-02
-2.5173e-02
 5.8002e-02
-3.9510e-02
 2.4334e-02
 2.4601e-02
 1.9136e-02
 8.7431e-02
-1.0810e-02
 4.6122e-02
-2.8169e-03
 6.6885e-02
 3.9518e-02
-4.4509e-05
 4.4938e-02
 1.7748e-02
-1.1751e-01
 3.7823e-02
 1.1494e-01
 1.1707e-01
-3.6071e-07
 3.7226e-02
-4.0908e-02
 8.1562e-02
 1.4487e-01
 5.9809e-03
 2.4173e-02
-1.3589e-02
-5.1669e-02
 5.4798e-02
 4.6337e-02
 2.2271e-02
-1.4790e-07
 4.1644e-02
 3.8625e-03
 2.1603e-02
-8.6729e-03
 9.4292e-02
-4.4369e-02
-6.0294e-07
 2.1652e-01
 8.9551e-02
 1.8707e-02
 2.0667e-02
-2.9589e-03
-2.1687e-01
 5.0260e-02
 3.8473e-02
 5.9135e-02
 3.6103e-02
 9.9497e-02
-2.0677e-05
 1.4253e-01
 1.9072e-01
 3.8042e-02
-1.2605e-06
-5.3548e-03
-2.0082e-06
 9.8597e-02
-9.4223e-02
-4.1343e-02
-8.2671e-03
-2.8833e-02
 1.0674e-01
 1.5557e-01
 2.7903e-02
 5.3139e-02
-3.1206e-02
 3.9483e-02
-7.7537e-07
 3.0037e-03
 3.5518e-02
 7.9860e-02
 2.3793e-02
-1.2981e-01
-1.3511e-06
-6.7246e-02
 1.7338e-01
 9.5783e-02
 9.1839e-02
-4.3202e-02
 7.8402e-02
-6.3475e-05
 5.4036e-02
 4.6969e-02
-2.4410e-07
 1.5709e-01
 2.9272e-02
 3.0758e-02
-1.5072e-01
 9.3047e-02
-7.6240e-05
 1.1170e-01
 1.5533e-02
 6.0914e-02
-5.9870e-04
 2.6366e-02
 5.2601e-02
 1.6601e-01
-3.9785e-02
 5.7633e-02
-2.7045e-06
-3.3708e-06
 1.1546e-01
 4.9188e-02
-1.4883e-02
 4.4428e-02
-3.1150e-02
-2.3653e-02
 6.5779e-02
 6.7056e-02
 9.8390e-02
-2.3785e-01
 3.3035e-02
 1.9017e-01
 1.6324e-02
-3.0738e-01
 3.1187e-02
-2.1642e-01
 1.1853e-02
-4.2425e-03
 5.6296e-03
-7.5315e-02
 1.3189e-02
 2.1798e-02
 6.3251e-04
 4.8708e-03
-5.7906e-02
 5.1894e-02
 1.3344e-01
-1.5952e-02
 1.6532e-01
-2.8305e-02
 4.0512e-03
 2.7601e-02
 1.2813e-02
-1.5498e-03
 2.9502e-02
 8.6315e-02
-4.0821e-02
-3.2629e-02
-1.5396e-02
-1.8829e-05
 1.0559e-02
 3.5416e-02
 1.2415e-01
 7.7687e-02
-1.1105e-02
 1.8961e-02
-5.8442e-02
-1.7247e-02
-4.9154e-02
 1.1553e-02
-2.0810e-06
 1.3872e-02
 1.0846e-01
 6.8531e-02
 1.7672e-01
 3.5634e-02
-4.0294e-02
 4.6163e-02
 3.3498e-02
 2.2283e-02
 4.6689e-02
 2.9249e-02
 1.0380e-01
 2.7987e-02
-4.7313e-05
 8.3607e-02
 7.9628e-02
-2.0540e-06
 1.2765e-01
-2.4337e-07
 1.1281e-01
-1.0358e-02
-4.9932e-03
 9.1279e-02
 5.0166e-02
-1.9497e-02
-3.3888e-04
-3.0161e-02
 7.7116e-02
-2.9716e-06
 7.0705e-02
 2.3647e-02
 3.5755e-02
-6.3678e-02
 1.0491e-01
 1.1383e-01
 1.3069e-03
-4.9135e-02
 1.1538e-01
-6.0227e-02
 9.9392e-04
 6.4781e-02
 7.6379e-02
 5.9853e-02
 1.1961e-01
-5.1671e-02
 5.2803e-02
 2.7144e-02
 5.1999e-02
 1.9074e-01
 1.7143e-02
 5.5178e-02
 1.2572e-01
-2.6754e-03
-3.1276e-02
 4.7069e-02
 5.1690e-02
-5.9475e-03
 2.9633e-02
 1.9122e-01
 1.4001e-02
-4.0885e-04
 2.3924e-02
-3.6612e-02
 3.5178e-02
 8.2615e-02
 6.9214e-02
 1.1781e-01
 3.4332e-02
 1.9473e-01
 3.2748e-02
[torch.FloatTensor of size 256]
), ('layer1.0.bn3.running_mean', 
 2.0248e-02
 4.8730e-04
-8.9333e-03
-7.5473e-02
-6.2560e-03
 1.3965e-02
-6.0137e-02
 4.8575e-02
 2.4598e-02
-5.3748e-02
-4.6156e-02
 2.2983e-02
 2.8183e-02
 7.9738e-02
 2.6930e-02
 9.1457e-03
-1.1706e-01
 3.8947e-02
 4.7379e-03
-1.3667e-02
-7.2562e-02
 1.1075e-01
 1.3731e-06
-1.1530e-07
-1.2960e-02
-9.9785e-05
 1.5518e-02
 3.3496e-02
-1.9226e-01
 7.8629e-02
-9.6039e-02
-4.4410e-03
-4.5078e-02
-5.0616e-02
-1.0124e-03
-1.1315e-02
-1.8294e-02
-3.2853e-03
-1.4958e-02
 3.2166e-03
 3.7707e-03
-2.2008e-02
-1.1220e-01
 1.6612e-02
-2.6063e-02
 1.7339e-05
 5.5588e-03
-4.2753e-04
-1.3877e-02
 8.0042e-04
-1.5434e-03
-1.7712e-02
-2.7835e-07
 2.8073e-02
 6.5603e-02
 5.7230e-02
-7.5369e-03
-9.3610e-02
 5.3512e-02
 6.6237e-02
 2.6994e-02
-5.7194e-02
-7.8499e-02
-2.1602e-02
-5.7685e-08
-1.4537e-03
 3.3200e-02
-6.0889e-02
 9.2091e-02
-6.7206e-03
-2.6983e-02
-1.4232e-07
-6.1400e-02
 8.1593e-02
 7.2027e-02
 3.6891e-02
 2.1151e-03
 6.6098e-03
 9.1676e-02
 6.3930e-02
 4.0399e-02
 1.8484e-01
-1.0856e-01
-1.0847e-05
 3.1961e-02
-5.6652e-02
-1.6060e-02
 7.5603e-07
-3.5063e-02
 8.7876e-07
 9.5141e-03
 2.6868e-02
-7.0109e-03
-1.2987e-04
-1.2118e-03
-1.7315e-02
-1.2206e-01
 8.2267e-02
 1.2823e-02
-3.2915e-04
 5.8198e-02
-7.1830e-07
-1.6214e-02
-1.0910e-02
-4.4865e-02
 2.4227e-02
-1.4179e-02
 3.5787e-07
 1.1837e-02
 2.1739e-02
 5.0791e-02
 2.8691e-02
-1.4611e-02
 1.3998e-02
 2.5274e-02
-2.5655e-02
 5.1763e-02
 2.6272e-08
-6.8142e-04
-1.0947e-02
-7.9420e-02
-1.6390e-02
-4.8153e-02
 5.0740e-05
 2.4842e-02
 2.7480e-02
 5.3922e-02
-9.1222e-05
-1.0937e-01
-3.6547e-03
-5.2424e-02
 7.5125e-02
-3.4904e-02
 5.1530e-08
 8.2597e-07
 5.3126e-03
-4.1296e-03
-3.9993e-02
 6.1077e-02
 3.3040e-02
-3.1933e-02
 3.8163e-02
-4.7102e-02
-4.5577e-02
-7.9224e-02
-3.8073e-02
-6.3041e-02
-2.3157e-02
-2.4089e-02
 1.3346e-02
 8.0434e-02
 3.0560e-02
 3.3887e-02
 5.6183e-03
 3.9446e-02
 7.3682e-03
 7.2999e-02
-5.3931e-02
-9.5135e-02
-5.9862e-02
 2.3830e-01
-3.4603e-02
-2.1281e-02
 6.1854e-05
 6.2426e-03
-2.8713e-02
-1.1831e-02
 2.0316e-01
-3.6296e-02
 3.0640e-02
-3.3334e-03
-6.8383e-02
 1.2789e-02
 5.5615e-02
-1.1860e-06
-2.6463e-02
-3.4606e-02
 1.0164e-01
 4.5595e-02
 2.9584e-03
 6.8592e-03
-2.6123e-02
 1.7266e-02
-1.6676e-02
-1.1939e-02
 7.0249e-07
 2.9303e-02
 2.3476e-02
-1.7391e-02
 8.4576e-03
-7.3945e-03
-6.8570e-03
 1.8293e-03
-7.1194e-02
-2.3595e-03
-2.7526e-02
 2.6283e-03
-1.8376e-01
-1.3474e-01
 1.8556e-05
 7.6428e-03
 5.2103e-02
 2.1550e-07
 1.8797e-01
-3.8884e-09
-6.2746e-03
 5.8296e-02
 1.9225e-02
-2.2333e-01
 3.0174e-02
 6.2938e-02
 3.4384e-02
 2.3603e-02
-7.5242e-02
-2.3886e-06
 1.1514e-01
-5.1569e-02
 7.0233e-02
 3.5531e-03
-6.4757e-03
-6.9624e-04
-4.5591e-02
 4.1128e-03
 2.9049e-02
 7.8785e-02
 3.7205e-02
-8.0091e-03
-1.2150e-02
-7.5258e-03
-9.3316e-03
-1.6075e-02
 2.2821e-01
 4.7105e-02
 2.1185e-02
 2.0996e-01
-1.1493e-02
 3.2331e-03
 1.0133e-02
 2.3218e-02
 3.2509e-02
-7.0988e-02
 2.5248e-02
-1.6438e-03
 3.5380e-02
-2.6366e-02
-2.6784e-02
-4.4134e-05
-8.6551e-03
 3.9663e-02
 7.9651e-03
-5.3890e-02
 4.0238e-02
-1.2606e-01
 1.4087e-02
-2.6403e-02
-3.5119e-02
[torch.FloatTensor of size 256]
), ('layer1.0.bn3.running_var', 
 6.1156e-03
 2.9289e-04
 2.2270e-04
 2.5206e-02
 2.3668e-04
 2.1101e-03
 3.9804e-03
 3.5540e-02
 1.1318e-02
 7.6980e-03
 1.0177e-02
 7.1479e-03
 9.3717e-03
 2.4703e-02
 1.3916e-02
 5.9240e-03
 9.4648e-03
 2.2293e-02
 5.3223e-04
 1.9786e-02
 3.3521e-03
 4.6003e-03
 3.9740e-13
 1.5798e-14
 1.2626e-02
 2.5580e-07
 2.1913e-04
 6.6939e-03
 7.2351e-03
 5.3529e-03
 4.7508e-03
 1.8389e-02
 1.7030e-02
 2.7199e-03
 4.6269e-03
 1.1320e-02
 8.2884e-03
 8.1185e-03
 5.6675e-04
 2.2868e-03
 1.7520e-04
 1.6450e-02
 1.0558e-02
 1.5857e-02
 6.3096e-03
 7.2426e-11
 3.2445e-03
 3.1909e-03
 5.6776e-04
 1.6716e-04
 1.0301e-02
 1.0369e-03
 3.9647e-14
 3.1722e-03
 1.8326e-03
 1.4443e-02
 2.2395e-04
 9.5232e-03
 7.9495e-03
 7.8007e-03
 3.6890e-03
 3.6499e-03
 7.1178e-03
 3.0668e-03
 2.2848e-14
 3.1353e-04
 2.4843e-03
 6.0634e-03
 3.1698e-03
 2.0646e-04
 2.7121e-04
 9.6232e-14
 1.3606e-02
 1.1785e-02
 2.2210e-02
 1.2949e-03
 2.3439e-06
 9.0565e-04
 9.8010e-03
 5.0451e-03
 1.5651e-02
 1.4202e-02
 1.1446e-02
 3.5631e-11
 1.0058e-02
 1.5123e-02
 5.8673e-03
 1.9368e-13
 1.5531e-02
 1.4734e-13
 5.9995e-04
 9.6620e-04
 1.8868e-04
 3.5389e-06
 1.4912e-04
 2.0906e-03
 1.3577e-02
 8.8133e-03
 3.0929e-03
 3.2200e-03
 1.5147e-02
 1.1499e-13
 1.5323e-02
 4.8300e-03
 1.8153e-02
 6.0416e-03
 2.5575e-03
 5.8345e-13
 2.0123e-04
 1.8207e-02
 1.2850e-02
 6.5016e-03
 1.5941e-04
 5.1617e-03
 8.0239e-03
 8.9726e-03
 1.5062e-02
 1.2622e-14
 1.5973e-02
 2.0372e-02
 5.0241e-03
 2.9704e-03
 1.7240e-02
 1.0178e-08
 5.3284e-03
 7.2460e-03
 5.8293e-03
 6.2344e-08
 8.5009e-04
 6.5999e-03
 1.6327e-02
 1.9107e-03
 7.0041e-03
 1.4475e-12
 6.1205e-12
 3.6569e-04
 2.5622e-03
 1.1118e-03
 1.3251e-02
 4.9472e-04
 9.4857e-04
 1.0643e-02
 3.7959e-03
 4.3091e-03
 1.6960e-03
 1.4202e-02
 1.7883e-02
 5.4667e-03
 9.7222e-04
 3.3517e-03
 2.6460e-03
 1.0071e-03
 1.2959e-03
 1.2572e-03
 2.4918e-03
 3.3046e-03
 3.5806e-03
 3.4750e-03
 6.4689e-03
 2.1421e-03
 9.0937e-03
 2.2441e-02
 6.5876e-03
 9.1385e-04
 2.4762e-04
 1.4895e-04
 1.0191e-03
 1.5732e-02
 6.5308e-03
 4.9739e-03
 5.6203e-04
 4.9834e-03
 1.3844e-03
 2.3522e-03
 1.3823e-10
 1.1323e-03
 9.8281e-03
 2.1158e-02
 4.5923e-03
 6.1467e-03
 5.4092e-03
 1.4950e-03
 8.0531e-03
 5.3923e-03
 2.5260e-04
 1.3728e-12
 7.1859e-03
 2.5397e-02
 1.0099e-02
 4.3619e-04
 1.1964e-02
 1.7284e-04
 1.6628e-02
 6.9418e-03
 1.9856e-04
 7.3933e-03
 9.1563e-03
 8.4241e-03
 4.5760e-03
 7.7662e-10
 2.0884e-02
 1.6493e-02
 4.3986e-13
 2.0789e-02
 1.3381e-14
 1.9390e-04
 3.0632e-03
 1.1826e-03
 4.1969e-03
 9.1141e-03
 7.2921e-03
 2.7093e-03
 1.3520e-03
 4.8755e-03
 3.2251e-12
 2.0720e-02
 3.7866e-03
 6.1420e-03
 7.4174e-03
 1.4055e-02
 1.0189e-03
 2.4269e-03
 5.9940e-04
 1.2306e-02
 7.2489e-04
 7.4105e-03
 3.0979e-04
 5.3680e-03
 2.7332e-04
 6.0603e-04
 8.8523e-04
 1.4653e-02
 6.5373e-04
 2.0604e-02
 5.6702e-03
 1.0128e-03
 5.1281e-03
 1.1092e-03
 7.2114e-04
 5.7841e-04
 1.4148e-03
 4.2170e-04
 4.3786e-04
 1.6336e-02
 1.7828e-02
 2.6956e-03
 1.2839e-08
 7.1133e-04
 1.9528e-03
 3.8536e-03
 1.4366e-02
 1.1385e-02
 1.8928e-02
 2.0566e-04
 3.1573e-02
 2.7765e-03
[torch.FloatTensor of size 256]
), ('layer1.0.downsample.0.weight', 
( 0 , 0 ,.,.) = 
  7.5811e-03

( 0 , 1 ,.,.) = 
 -1.6594e-01

( 0 , 2 ,.,.) = 
  9.1546e-03
    ... 

( 0 ,61 ,.,.) = 
 -2.2368e-02

( 0 ,62 ,.,.) = 
  4.2574e-02

( 0 ,63 ,.,.) = 
  1.4474e-03
      ⋮  

( 1 , 0 ,.,.) = 
  1.2800e-02

( 1 , 1 ,.,.) = 
 -1.1457e-02

( 1 , 2 ,.,.) = 
 -2.5123e-02
    ... 

( 1 ,61 ,.,.) = 
 -4.1692e-04

( 1 ,62 ,.,.) = 
  2.2987e-02

( 1 ,63 ,.,.) = 
  2.1549e-03
      ⋮  

( 2 , 0 ,.,.) = 
 -3.5923e-02

( 2 , 1 ,.,.) = 
  1.3393e-02

( 2 , 2 ,.,.) = 
 -2.6259e-02
    ... 

( 2 ,61 ,.,.) = 
  5.9477e-02

( 2 ,62 ,.,.) = 
 -2.7209e-04

( 2 ,63 ,.,.) = 
 -6.3618e-03
...     
      ⋮  

(253, 0 ,.,.) = 
 -3.9645e-02

(253, 1 ,.,.) = 
 -3.9428e-01

(253, 2 ,.,.) = 
  7.2763e-02
    ... 

(253,61 ,.,.) = 
 -1.0293e-02

(253,62 ,.,.) = 
  1.1058e-01

(253,63 ,.,.) = 
  1.6492e-02
      ⋮  

(254, 0 ,.,.) = 
  1.5314e-03

(254, 1 ,.,.) = 
  1.1643e-02

(254, 2 ,.,.) = 
  6.7886e-02
    ... 

(254,61 ,.,.) = 
 -7.0245e-02

(254,62 ,.,.) = 
  1.6036e-01

(254,63 ,.,.) = 
  3.6155e-02
      ⋮  

(255, 0 ,.,.) = 
  2.0393e-02

(255, 1 ,.,.) = 
  3.6532e-02

(255, 2 ,.,.) = 
 -3.5239e-02
    ... 

(255,61 ,.,.) = 
 -1.1597e-01

(255,62 ,.,.) = 
  2.4156e-02

(255,63 ,.,.) = 
 -5.0600e-02
[torch.FloatTensor of size 256x64x1x1]
), ('layer1.0.downsample.1.weight', 
 2.4457e-01
 2.2984e-01
 1.4612e-01
 3.4260e-01
 2.5927e-02
 1.1437e-01
 2.1102e-01
 4.5606e-01
 1.8027e-01
 1.5431e-01
 3.5582e-01
 2.1846e-01
 2.5678e-01
 1.5123e-02
 4.0578e-01
 2.0874e-01
 9.8095e-02
 3.9442e-01
 1.9774e-01
 3.0000e-01
 2.4856e-01
 2.3178e-01
-1.0182e-08
 1.3827e-08
 3.1164e-01
 4.8934e-04
 2.6274e-01
 5.9192e-02
 1.9006e-01
 2.0053e-01
 1.6345e-01
 3.0634e-01
 3.5806e-01
 1.8923e-02
 3.0784e-01
 4.3668e-01
 2.7498e-01
 1.6151e-01
 2.9890e-01
 1.2460e-01
 7.0893e-02
 3.3482e-01
 3.4934e-01
 2.8829e-01
 2.7963e-01
 8.1415e-07
 3.6592e-01
 1.6732e-01
 2.2241e-01
 1.9294e-01
 2.9332e-01
 1.4812e-01
 2.0370e-07
 1.6793e-01
 2.2183e-01
 2.7442e-01
-1.1456e-02
 3.9355e-01
 2.5354e-01
 3.2749e-01
 3.3727e-01
 2.0058e-01
 2.4418e-01
 1.6629e-01
 2.0949e-08
 1.8675e-01
 1.8726e-01
 2.4386e-01
 1.7288e-01
 2.2883e-01
 1.9265e-01
 1.8148e-07
 1.8998e-01
 1.9738e-01
 3.0906e-01
 1.1432e-01
 2.7538e-03
 2.8674e-01
 2.9543e-01
 2.0380e-01
 2.9774e-01
 3.0047e-01
 3.0071e-01
 3.0339e-06
 2.5298e-01
 2.8255e-01
 2.3041e-01
-2.2464e-07
 9.0851e-02
 9.4888e-08
 2.3342e-01
 2.8756e-01
 2.5694e-01
 1.9065e-03
 1.9803e-02
 1.2581e-01
 2.5297e-01
 1.0138e-01
 1.9470e-01
 2.4283e-01
 2.2812e-01
 3.9112e-07
 4.3694e-01
 2.0982e-01
 3.3158e-01
 3.6135e-01
 1.6712e-01
 2.8431e-07
 2.4639e-01
 3.1374e-01
 2.0042e-01
 1.7867e-01
 2.4927e-01
 2.6893e-01
 2.4301e-02
 3.3343e-01
 2.8018e-01
 1.0910e-07
 3.1720e-01
 3.2437e-01
 1.7580e-01
 2.1966e-01
 2.2206e-01
 3.8821e-05
 2.1403e-01
 3.1898e-01
 2.2020e-01
 2.1191e-04
 2.4318e-01
 3.2712e-01
 3.5392e-01
 1.9045e-01
 2.4259e-01
 5.9542e-07
 2.1926e-06
 2.0136e-01
 2.0292e-01
 1.6009e-01
 3.1163e-01
 3.1269e-02
 1.5136e-01
 3.4167e-01
 2.2320e-01
 1.7928e-01
 3.0206e-01
 3.3896e-01
 3.2244e-01
 3.3214e-01
 3.1712e-01
 2.6545e-01
 3.4695e-01
 1.9813e-01
 1.5368e-01
 2.3095e-01
 1.2709e-01
 2.2131e-01
 7.9861e-02
 1.0763e-01
 3.4360e-01
 2.8470e-01
 3.1999e-01
 3.4235e-01
 2.8543e-01
 2.5500e-01
 8.0065e-02
 2.1822e-01
 1.2625e-01
 3.3813e-01
 3.6787e-01
 2.0902e-01
 7.5418e-02
 3.0735e-01
 4.3765e-02
 9.5508e-02
-3.7830e-06
 3.1341e-01
 2.3747e-01
 2.9429e-01
 1.7653e-01
 3.3874e-01
 2.2616e-01
 1.9847e-01
 3.5470e-01
 8.1458e-03
 2.0669e-01
 1.0625e-06
 3.6871e-01
 3.6358e-01
 1.9882e-01
 1.6871e-01
 3.9717e-01
 2.4319e-01
 2.2733e-01
 2.5196e-01
 4.8692e-02
 3.4219e-01
 3.5018e-01
-4.2149e-02
 1.8716e-01
-7.9278e-06
 2.8354e-01
 3.7017e-01
 1.4341e-07
 3.1774e-01
 7.5645e-08
 1.6309e-01
 1.6707e-01
 2.2129e-01
 2.7987e-01
 2.5573e-01
 2.8884e-01
 2.4309e-01
 8.3486e-02
 2.5541e-01
 6.3526e-07
 3.6438e-01
 2.3440e-01
 2.2243e-01
 3.1752e-01
 3.1876e-01
 2.5398e-01
 1.6543e-01
 6.8547e-02
 1.9213e-01
 1.8097e-01
 2.8546e-01
 2.0063e-01
 1.9390e-01
 2.0524e-01
 1.8420e-01
 2.0247e-02
 2.7564e-01
 1.4836e-01
 3.3919e-01
 1.8534e-01
 2.9466e-01
 2.7174e-01
 2.9968e-01
 1.3451e-01
 1.9264e-01
 1.2815e-01
 1.7942e-01
 3.1054e-01
 3.0394e-01
 3.0231e-01
 2.6601e-01
 7.0521e-05
 1.6745e-01
 2.4501e-01
 1.2502e-01
 3.6441e-01
 2.4699e-01
 3.7460e-01
 1.6707e-01
 3.6041e-01
 1.8811e-01
[torch.FloatTensor of size 256]
), ('layer1.0.downsample.1.bias', 
-4.7521e-03
 7.3382e-02
 2.9881e-02
 5.4250e-02
-4.2102e-02
 4.5748e-02
 7.0444e-02
 1.3612e-01
 9.5970e-02
 8.6802e-02
 4.6588e-02
 8.7879e-03
-5.5630e-03
 3.5592e-02
 6.0245e-02
 2.9497e-02
 1.1487e-01
 1.0022e-01
-7.0565e-02
 5.7853e-02
 5.9134e-02
 3.1986e-02
-2.0646e-06
-1.1174e-07
 9.5474e-02
-1.2143e-03
-1.0645e-01
-7.6647e-03
 4.3269e-02
 1.0157e-02
 3.3298e-02
 1.0682e-02
 5.0505e-02
-2.5173e-02
 5.8002e-02
-3.9510e-02
 2.4334e-02
 2.4601e-02
 1.9136e-02
 8.7431e-02
-1.0810e-02
 4.6122e-02
-2.8169e-03
 6.6885e-02
 3.9518e-02
-4.4509e-05
 4.4938e-02
 1.7748e-02
-1.1751e-01
 3.7823e-02
 1.1494e-01
 1.1707e-01
-3.6071e-07
 3.7226e-02
-4.0908e-02
 8.1562e-02
 1.4487e-01
 5.9809e-03
 2.4173e-02
-1.3589e-02
-5.1669e-02
 5.4798e-02
 4.6337e-02
 2.2271e-02
-1.4790e-07
 4.1644e-02
 3.8625e-03
 2.1603e-02
-8.6729e-03
 9.4292e-02
-4.4369e-02
-6.0294e-07
 2.1652e-01
 8.9551e-02
 1.8707e-02
 2.0667e-02
-2.9589e-03
-2.1687e-01
 5.0260e-02
 3.8473e-02
 5.9135e-02
 3.6103e-02
 9.9497e-02
-2.0677e-05
 1.4253e-01
 1.9072e-01
 3.8042e-02
-1.2605e-06
-5.3548e-03
-2.0082e-06
 9.8597e-02
-9.4223e-02
-4.1343e-02
-8.2671e-03
-2.8833e-02
 1.0674e-01
 1.5557e-01
 2.7903e-02
 5.3139e-02
-3.1206e-02
 3.9483e-02
-7.7537e-07
 3.0037e-03
 3.5518e-02
 7.9860e-02
 2.3793e-02
-1.2981e-01
-1.3511e-06
-6.7246e-02
 1.7338e-01
 9.5783e-02
 9.1839e-02
-4.3202e-02
 7.8402e-02
-6.3475e-05
 5.4036e-02
 4.6969e-02
-2.4410e-07
 1.5709e-01
 2.9272e-02
 3.0758e-02
-1.5072e-01
 9.3047e-02
-7.6240e-05
 1.1170e-01
 1.5533e-02
 6.0914e-02
-5.9870e-04
 2.6366e-02
 5.2601e-02
 1.6601e-01
-3.9785e-02
 5.7633e-02
-2.7045e-06
-3.3708e-06
 1.1546e-01
 4.9188e-02
-1.4883e-02
 4.4428e-02
-3.1150e-02
-2.3653e-02
 6.5779e-02
 6.7056e-02
 9.8390e-02
-2.3785e-01
 3.3035e-02
 1.9017e-01
 1.6324e-02
-3.0738e-01
 3.1187e-02
-2.1642e-01
 1.1853e-02
-4.2425e-03
 5.6296e-03
-7.5315e-02
 1.3189e-02
 2.1798e-02
 6.3251e-04
 4.8708e-03
-5.7906e-02
 5.1894e-02
 1.3344e-01
-1.5952e-02
 1.6532e-01
-2.8305e-02
 4.0512e-03
 2.7601e-02
 1.2813e-02
-1.5498e-03
 2.9502e-02
 8.6315e-02
-4.0821e-02
-3.2629e-02
-1.5396e-02
-1.8829e-05
 1.0559e-02
 3.5416e-02
 1.2415e-01
 7.7687e-02
-1.1105e-02
 1.8961e-02
-5.8442e-02
-1.7247e-02
-4.9154e-02
 1.1553e-02
-2.0810e-06
 1.3872e-02
 1.0846e-01
 6.8531e-02
 1.7672e-01
 3.5634e-02
-4.0294e-02
 4.6163e-02
 3.3498e-02
 2.2283e-02
 4.6689e-02
 2.9249e-02
 1.0380e-01
 2.7987e-02
-4.7313e-05
 8.3607e-02
 7.9628e-02
-2.0540e-06
 1.2765e-01
-2.4337e-07
 1.1281e-01
-1.0358e-02
-4.9932e-03
 9.1279e-02
 5.0166e-02
-1.9497e-02
-3.3888e-04
-3.0161e-02
 7.7116e-02
-2.9716e-06
 7.0705e-02
 2.3647e-02
 3.5755e-02
-6.3678e-02
 1.0491e-01
 1.1383e-01
 1.3069e-03
-4.9135e-02
 1.1538e-01
-6.0227e-02
 9.9392e-04
 6.4781e-02
 7.6379e-02
 5.9853e-02
 1.1961e-01
-5.1671e-02
 5.2803e-02
 2.7144e-02
 5.1999e-02
 1.9074e-01
 1.7143e-02
 5.5178e-02
 1.2572e-01
-2.6754e-03
-3.1276e-02
 4.7069e-02
 5.1690e-02
-5.9475e-03
 2.9633e-02
 1.9122e-01
 1.4001e-02
-4.0885e-04
 2.3924e-02
-3.6612e-02
 3.5178e-02
 8.2615e-02
 6.9214e-02
 1.1781e-01
 3.4332e-02
 1.9473e-01
 3.2748e-02
[torch.FloatTensor of size 256]
), ('layer1.0.downsample.1.running_mean', 
-3.3726e-01
-3.7005e-02
 8.4843e-02
-2.3233e-01
-1.3259e-02
-2.5136e-01
-2.8319e-01
 1.5388e-01
-7.3795e-01
-4.0045e-01
-4.9723e-01
-1.5364e-02
 1.1043e-02
 1.7025e-01
-1.9047e-01
-1.8722e-01
-2.8338e-01
 1.6583e-01
 3.3551e-01
-7.9627e-01
-3.5260e-01
 5.5593e-02
 1.3857e-06
 6.2537e-07
-8.7529e-02
-9.9917e-04
 2.7928e-01
-1.9474e-01
 5.0298e-02
 1.1937e-01
 1.7855e-02
-6.2864e-02
-9.3953e-02
-1.4843e-02
-4.3980e-01
-5.7708e-01
-6.3612e-01
 1.4103e-01
-5.5314e-01
-1.7979e-01
 4.3170e-02
-2.6326e-01
-2.2073e-01
-8.7107e-02
-2.1167e-01
 3.3194e-05
-3.8259e-01
-2.1429e-01
-3.5122e-02
-5.8087e-01
-3.9025e-01
 3.6322e-02
-1.4431e-06
 1.6630e-02
-2.6390e-01
-8.6894e-01
-1.1056e-01
-3.4739e-02
 1.0765e-01
-2.5030e-01
-6.3856e-01
-8.8252e-02
-1.9975e-01
-2.9636e-01
-8.0308e-07
-7.4356e-02
-2.3403e-01
 8.4561e-03
 1.5742e-02
-3.6031e-01
-2.3964e-01
-2.0620e-07
-7.8557e-01
-3.8937e-01
-1.8827e-01
 3.8027e-01
-8.0350e-03
 2.1824e-01
-3.9279e-01
 1.9478e-01
-4.8282e-01
 1.6595e-01
-4.0018e-01
 6.2102e-05
 7.9813e-01
 1.8114e-01
-3.1322e-01
-6.0070e-07
 1.5892e-01
-7.3137e-07
-4.2683e-01
-2.0465e-01
 7.3856e-03
 1.6836e-02
 4.0549e-03
-4.6866e-01
 2.6099e-01
 3.6795e-01
-6.6750e-02
-3.7969e-01
-2.3474e-01
-1.3522e-06
-2.9263e-01
-5.5633e-01
-1.8536e-01
-4.0307e-01
 8.3629e-03
-2.0529e-06
-4.4396e-02
-8.2865e-02
-3.6988e-01
 3.8752e-01
 4.1428e-02
-1.6859e-01
-1.3558e-01
-4.6316e-02
-3.2712e-01
-3.9872e-07
-5.1091e-02
-9.8691e-02
-3.7356e-01
-2.8653e-02
-3.1354e-01
-2.0610e-04
-4.0701e-01
-1.1951e-01
-3.1954e-01
 1.9050e-03
 5.7707e-02
 8.5263e-02
-2.3637e-02
-1.6454e-01
 1.3402e-01
-2.0994e-06
-1.5371e-05
-2.9732e-01
 1.4661e-01
-4.6323e-02
-2.6843e-01
-6.8299e-02
-1.2500e-01
-1.1708e-01
-6.5646e-02
-4.6190e-01
-3.4429e-03
-3.7613e-01
 3.8901e-02
-1.4678e-01
 2.7054e-02
-3.3150e-01
-1.2276e-01
-3.8530e-02
 1.2213e-01
 6.8455e-02
 2.1314e-01
-2.0980e-01
-1.1683e-01
-2.1416e-03
-2.4109e-01
-5.0769e-01
-2.3973e-01
 2.3310e-01
-1.8435e-01
 1.6355e+00
 4.6324e-02
-3.7285e-01
 2.5217e-02
-7.0982e-01
-2.6779e-01
-2.0338e-01
-5.5598e-02
-3.9569e-01
 7.0004e-02
-8.3563e-02
-3.6107e-05
 5.7685e-03
 2.5696e-01
 9.6119e-01
 4.6588e-01
-3.5012e-01
 1.6271e-01
 2.1767e-01
-3.2732e-01
-1.0062e-01
 4.3192e-02
-2.1291e-06
-7.4488e-01
-2.4256e-01
-1.2209e-01
 1.7136e-01
-4.4009e-01
 2.1959e-01
-2.0899e-01
-2.0576e-01
-8.4679e-02
-1.0325e+00
-2.7388e-01
 2.9876e-01
-9.0823e-02
 6.6930e-05
-3.4086e-01
 1.9395e-01
 7.5280e-07
-8.1316e-03
 6.5531e-08
-3.4516e-01
-1.0249e-01
 1.0455e-03
-2.8234e-01
-1.4810e-01
-2.5492e-01
 7.6042e-02
-3.1430e-02
-3.6049e-01
 2.1677e-06
-1.9596e-01
 7.6099e-02
 1.1745e-01
-2.3464e-01
 3.6845e-01
-1.8433e-01
 1.5708e-01
-8.7092e-02
-4.2463e-01
 1.6762e-01
-5.2849e-01
 1.0744e-01
-4.3159e-01
 1.0036e-01
 2.3984e-01
 5.1295e-02
-5.9733e-02
-2.4893e-01
 1.0825e-01
 4.1912e-01
-8.0573e-01
 3.9302e-01
 1.4361e-01
-3.0970e-01
-1.1542e-01
-1.9073e-01
-6.1461e-01
-4.1960e-01
-8.7137e-02
-9.1075e-02
-4.3178e-01
-1.9362e-04
 2.1060e-01
-3.3042e-01
 2.2124e-01
-1.3658e-01
-3.2380e-01
-9.4365e-01
 1.4835e-01
 2.2765e-01
 1.5125e-01
[torch.FloatTensor of size 256]
), ('layer1.0.downsample.1.running_var', 
 3.2973e-02
 7.7851e-02
 8.0960e-03
 1.1753e-01
 1.0632e-03
 4.0931e-02
 4.7659e-02
 5.0404e-02
 3.4896e-02
 2.6328e-02
 4.4557e-02
 2.6798e-02
 2.4821e-02
 1.0460e-02
 8.5482e-02
 2.0668e-02
 2.7309e-02
 3.7718e-02
 2.1616e-02
 3.5230e-02
 4.0282e-02
 1.8303e-02
 5.2866e-13
 4.7380e-14
 7.1019e-02
 1.5586e-06
 3.2351e-02
 5.4131e-03
 3.4796e-02
 3.5064e-02
 3.0291e-02
 4.9701e-02
 7.9961e-02
 1.1866e-03
 4.9570e-02
 7.2288e-02
 6.0680e-02
 2.9531e-02
 1.1346e-01
 3.2332e-02
 2.9788e-03
 7.2821e-02
 6.7578e-02
 7.3261e-02
 4.0222e-02
 2.1857e-10
 6.9538e-02
 7.6937e-03
 5.2000e-03
 1.8508e-02
 4.6554e-02
 3.2325e-02
 3.5178e-13
 1.7116e-02
 4.0672e-02
 2.8469e-02
 1.7236e-03
 2.5492e-02
 5.7913e-02
 5.6362e-02
 1.6645e-02
 3.2368e-02
 2.7743e-02
 2.4939e-02
 2.0094e-13
 9.3778e-02
 2.4521e-02
 3.2284e-02
 1.2765e-02
 5.8812e-02
 5.6251e-03
 1.5070e-12
 8.5107e-02
 7.7260e-02
 9.9955e-02
 9.8532e-03
 2.8484e-05
 2.2215e-02
 3.6865e-02
 2.8912e-02
 7.7955e-02
 6.7750e-02
 7.2378e-02
 6.5515e-10
 1.1521e-01
 5.2291e-02
 2.8817e-02
 6.9497e-14
 2.6075e-02
 1.4005e-13
 1.1622e-01
 7.9967e-03
 5.7543e-02
 1.4157e-04
 1.1975e-03
 8.7734e-03
 1.4683e-02
 1.8501e-02
 2.7886e-02
 4.4476e-02
 5.4143e-02
 6.4594e-13
 8.3844e-02
 1.3997e-02
 6.1447e-02
 9.7849e-02
 5.1662e-03
 9.2346e-13
 3.6878e-02
 6.7598e-02
 6.4634e-02
 4.8288e-02
 5.2493e-02
 4.2822e-02
 3.9595e-03
 4.2119e-02
 6.2689e-02
 5.4074e-14
 5.2766e-02
 8.7823e-02
 4.3266e-02
 6.0939e-03
 5.3711e-02
 2.0356e-08
 5.4199e-02
 3.7959e-02
 1.8632e-02
 9.7186e-07
 1.2309e-02
 5.2750e-02
 1.0344e-01
 1.5864e-02
 2.8111e-02
 1.3550e-12
 2.2743e-11
 4.2847e-02
 2.1293e-02
 5.6778e-03
 7.5470e-02
 1.9899e-03
 5.3805e-03
 6.0363e-02
 2.5499e-02
 2.7327e-02
 1.2806e-02
 7.1331e-02
 6.6494e-02
 3.9179e-02
 1.2624e-02
 1.4871e-02
 2.3719e-02
 5.2458e-03
 1.3263e-02
 8.8299e-03
 1.0102e-02
 2.6860e-02
 1.2963e-02
 5.0216e-03
 4.8764e-02
 1.9397e-02
 4.3183e-02
 6.7695e-02
 1.4487e-02
 3.5617e-01
 2.9178e-03
 3.0127e-02
 1.0206e-02
 1.8741e-02
 9.8605e-02
 2.1420e-02
 1.3926e-02
 4.1672e-02
 2.6701e-03
 7.6158e-03
 2.3303e-10
 1.3227e-02
 4.5078e-02
 3.1400e-02
 2.9709e-02
 7.6825e-02
 1.8895e-02
 2.0242e-02
 6.0930e-02
 1.8279e-03
 6.2018e-02
 1.0279e-11
 5.5496e-02
 4.6934e-02
 4.2641e-02
 1.1596e-01
 1.5104e-01
 4.9538e-02
 9.4459e-02
 4.0333e-02
 5.1562e-03
 1.7764e-02
 7.9920e-02
 1.2540e-02
 2.1796e-02
 1.3336e-09
 7.9812e-02
 3.3104e-02
 1.4615e-13
 5.5552e-02
 5.8959e-14
 2.4136e-02
 1.5471e-02
 8.6243e-03
 3.6080e-02
 4.1118e-02
 4.6464e-02
 1.5429e-02
 3.1309e-03
 2.9523e-02
 4.0670e-12
 6.2049e-02
 2.1598e-02
 2.4605e-02
 3.9939e-02
 3.2184e-02
 2.3920e-01
 5.5553e-03
 1.5522e-03
 8.9853e-02
 7.3504e-03
 7.4434e-02
 8.6580e-02
 2.1713e-02
 4.5860e-02
 4.2940e-02
 8.0211e-04
 5.2936e-02
 1.9316e-02
 5.2249e-02
 5.0376e-02
 6.7300e-02
 3.7252e-02
 3.0792e-01
 1.1896e-02
 4.2198e-03
 1.2408e-02
 1.9634e-02
 1.1114e-01
 7.8982e-02
 9.4032e-02
 6.9527e-02
 6.9146e-08
 1.2847e-02
 3.9120e-02
 1.5237e-02
 4.1092e-02
 9.5366e-02
 3.3733e-02
 4.7297e-02
 8.6898e-02
 2.2537e-02
[torch.FloatTensor of size 256]
), ('layer1.1.conv1.weight', 
( 0 , 0 ,.,.) = 
  2.5038e-02

( 0 , 1 ,.,.) = 
  2.5615e-03

( 0 , 2 ,.,.) = 
  1.0989e-02
    ... 

( 0 ,253,.,.) = 
  3.5722e-04

( 0 ,254,.,.) = 
  2.0726e-02

( 0 ,255,.,.) = 
 -3.4464e-02
      ⋮  

( 1 , 0 ,.,.) = 
  2.2667e-03

( 1 , 1 ,.,.) = 
 -1.1549e-03

( 1 , 2 ,.,.) = 
 -1.1125e-02
    ... 

( 1 ,253,.,.) = 
 -1.6633e-02

( 1 ,254,.,.) = 
  2.2977e-02

( 1 ,255,.,.) = 
 -7.3551e-02
      ⋮  

( 2 , 0 ,.,.) = 
 -6.0413e-03

( 2 , 1 ,.,.) = 
 -1.8692e-03

( 2 , 2 ,.,.) = 
  8.4820e-03
    ... 

( 2 ,253,.,.) = 
 -6.0168e-03

( 2 ,254,.,.) = 
 -3.8597e-02

( 2 ,255,.,.) = 
  2.8177e-02
...     
      ⋮  

(61 , 0 ,.,.) = 
  5.9032e-03

(61 , 1 ,.,.) = 
  1.8610e-02

(61 , 2 ,.,.) = 
  1.3279e-03
    ... 

(61 ,253,.,.) = 
  1.0600e-02

(61 ,254,.,.) = 
 -1.5613e-02

(61 ,255,.,.) = 
 -4.9641e-02
      ⋮  

(62 , 0 ,.,.) = 
  4.5348e-02

(62 , 1 ,.,.) = 
 -5.2813e-03

(62 , 2 ,.,.) = 
 -1.3080e-02
    ... 

(62 ,253,.,.) = 
  2.1859e-02

(62 ,254,.,.) = 
  3.6713e-02

(62 ,255,.,.) = 
 -2.1189e-02
      ⋮  

(63 , 0 ,.,.) = 
  6.5799e-03

(63 , 1 ,.,.) = 
  1.9079e-02

(63 , 2 ,.,.) = 
  1.4258e-02
    ... 

(63 ,253,.,.) = 
 -3.7024e-03

(63 ,254,.,.) = 
 -2.5108e-02

(63 ,255,.,.) = 
  7.2490e-02
[torch.FloatTensor of size 64x256x1x1]
), ('layer1.1.bn1.weight', 
 1.5418e-01
 3.2359e-01
 2.4792e-01
 2.1051e-01
 1.7820e-01
 1.8067e-01
 1.7301e-01
 2.4763e-01
 1.7234e-01
 3.1702e-01
 1.4758e-01
 1.9952e-01
 1.2419e-01
 1.2985e-09
 1.6273e-01
 1.9145e-01
 1.6043e-01
 1.9408e-01
 2.5509e-01
 1.7340e-01
 1.4120e-01
 1.7380e-01
 1.7927e-01
 1.2663e-01
 1.6290e-01
 1.0250e-01
 1.7276e-01
 1.5056e-01
 1.7809e-01
 2.2617e-01
 2.8132e-01
 2.1156e-01
 2.3651e-01
 1.8941e-01
 1.6280e-01
 1.5587e-01
 1.6888e-01
 2.4972e-01
 3.4105e-01
 1.3929e-01
 3.0862e-01
 3.0111e-01
 1.7055e-01
 2.9651e-01
 1.1900e-01
 4.6577e-07
 1.7862e-01
 1.4380e-08
 1.4639e-01
 1.9021e-01
 1.9271e-01
 2.1601e-01
 1.5010e-01
 2.4548e-01
 1.3231e-01
 2.9485e-01
 8.5831e-09
 1.7127e-01
 1.3440e-01
 1.4374e-01
 2.1540e-01
 1.3796e-01
 1.9064e-01
 1.8207e-01
[torch.FloatTensor of size 64]
), ('layer1.1.bn1.bias', 
 5.7919e-02
-2.0846e-01
-1.0967e-01
-1.0636e-02
-5.2057e-02
-1.7326e-02
-5.5401e-03
-2.0876e-01
 8.0133e-03
-4.0513e-01
 2.5865e-02
-9.4683e-02
 9.5730e-02
-9.1670e-08
 9.9355e-03
-4.4990e-02
 5.5640e-02
 2.2149e-02
-1.4557e-01
-2.4153e-01
-6.7984e-03
 8.1301e-02
 9.7942e-03
 2.3542e-02
 1.0494e-01
 7.5696e-02
-1.4552e-03
 2.3584e-01
-4.5816e-02
-4.4147e-02
-1.7976e-01
 1.5807e-03
-1.0492e-01
-9.8615e-02
 1.9448e-02
-1.0210e-01
 2.7732e-01
-1.4269e-01
-2.2826e-01
 1.4129e-02
-1.9629e-01
-2.3039e-01
-1.0216e-01
-1.9040e-01
 2.9462e-01
-2.2354e-06
-1.1618e-01
-1.0213e-07
 1.0679e-01
-2.5207e-01
-6.5190e-02
-7.5232e-03
 1.4403e-01
-7.3785e-02
 2.8200e-01
-1.9581e-01
-6.0297e-08
 3.9227e-01
 9.5571e-02
 1.1742e-01
-1.6532e-02
 2.5024e-01
-1.6168e-01
 9.6112e-03
[torch.FloatTensor of size 64]
), ('layer1.1.bn1.running_mean', 
-2.7727e-02
 1.2637e-01
 2.9560e-02
 1.2952e-01
 1.8322e-01
 2.1866e-02
 8.4403e-02
 1.9021e-02
-1.6468e-01
 1.6681e-01
-2.5861e-01
 9.1793e-02
 2.6331e-01
 5.5225e-09
-1.1509e-01
 4.3420e-02
 9.8333e-02
 4.4416e-04
 4.4265e-02
-7.7946e-03
-6.9111e-02
-7.6247e-02
-1.4269e-01
 1.0134e-01
 8.3184e-02
 9.7224e-02
-4.5411e-02
-5.5284e-02
 1.5879e-02
 8.7157e-02
 1.0606e-01
 3.2493e-02
 2.2955e-01
 8.2691e-02
 5.2290e-02
 1.4319e-02
-8.1695e-02
 4.5908e-02
 4.7016e-02
 1.2202e-02
 1.6606e-01
-3.8312e-03
 4.4875e-02
 7.9080e-02
 1.8086e-02
-5.0624e-07
-2.3124e-01
 2.4892e-08
-9.9863e-02
-6.0550e-02
 9.9452e-02
 1.4313e-01
-1.4510e-01
 5.7257e-02
 3.4996e-02
-1.0794e-01
 1.4221e-08
 4.7352e-01
-2.2454e-01
 4.0456e-02
 4.6600e-02
-3.1750e-02
-6.0634e-02
-5.8636e-02
[torch.FloatTensor of size 64]
), ('layer1.1.bn1.running_var', 
 1.2423e-02
 5.9035e-02
 3.7240e-02
 2.8262e-02
 2.6896e-02
 2.7312e-02
 1.4456e-02
 2.7977e-02
 2.5682e-02
 2.4689e-02
 1.4171e-02
 2.0850e-02
 3.2111e-02
 1.0137e-15
 1.8196e-02
 2.3435e-02
 3.3997e-02
 3.6771e-02
 3.2074e-02
 6.9097e-03
 1.2463e-02
 2.5295e-02
 1.9885e-02
 1.9350e-02
 2.2277e-02
 1.5480e-02
 2.2531e-02
 2.9952e-02
 1.9454e-02
 2.4399e-02
 5.8219e-02
 3.2323e-02
 2.8238e-02
 1.6362e-02
 2.5335e-02
 1.5558e-02
 5.7912e-02
 3.3419e-02
 5.4032e-02
 1.4318e-02
 6.4521e-02
 2.6188e-02
 1.4874e-02
 5.8206e-02
 1.3073e-02
 9.2327e-14
 8.7810e-03
 4.9726e-16
 1.6126e-02
 7.1383e-03
 1.5345e-02
 3.4768e-02
 1.1530e-02
 3.4949e-02
 1.5612e-02
 4.5703e-02
 4.5354e-16
 2.4729e-02
 1.8084e-02
 8.9803e-03
 3.4849e-02
 1.7153e-02
 1.1948e-02
 2.0287e-02
[torch.FloatTensor of size 64]
), ('layer1.1.conv2.weight', 
(0 ,0 ,.,.) = 
  1.5131e-02 -2.0844e-03  1.3178e-02
  9.9589e-03 -1.3180e-02 -1.4355e-02
  2.0671e-03 -1.2831e-02 -2.8570e-03

(0 ,1 ,.,.) = 
 -1.0579e-02 -3.2069e-03 -9.4352e-03
  1.3472e-02  3.1053e-02  3.7212e-02
 -1.3569e-03 -7.9770e-03 -1.2346e-02

(0 ,2 ,.,.) = 
  3.5541e-03  1.6511e-02  1.0947e-02
  8.5486e-04  6.9477e-03  7.7736e-03
 -1.6510e-03 -6.9018e-03  5.7594e-03
   ...

(0 ,61,.,.) = 
 -1.8107e-02 -3.8545e-02 -2.2902e-02
  5.7928e-02  5.5715e-02  3.9585e-02
 -3.1099e-02 -5.4442e-02 -3.3741e-02

(0 ,62,.,.) = 
  1.1299e-02  3.0649e-02  8.2794e-03
  9.0970e-03  1.4344e-02  1.9952e-02
 -6.4701e-03  8.5228e-03 -2.3314e-03

(0 ,63,.,.) = 
  1.0523e-03 -9.7366e-03  4.7732e-03
  1.3268e-02 -1.5277e-02 -1.5127e-02
 -8.2769e-03 -2.3385e-02 -4.4172e-03
     ⋮ 

(1 ,0 ,.,.) = 
 -1.7524e-02 -1.1281e-02  3.3239e-03
  8.7153e-03 -1.3304e-02 -1.8601e-03
  2.2107e-03 -8.4356e-03 -6.6629e-03

(1 ,1 ,.,.) = 
 -3.0955e-03 -3.2096e-03  1.7255e-02
  6.2367e-02  4.6211e-02  4.6354e-02
  3.3049e-02  1.2192e-02  2.4536e-02

(1 ,2 ,.,.) = 
  1.0525e-03 -1.8479e-02 -1.2107e-02
  9.8989e-03  2.1735e-03 -2.9522e-03
  3.5191e-04  5.2870e-03  4.9564e-03
   ...

(1 ,61,.,.) = 
  1.6884e-03 -1.2523e-02 -1.8005e-02
  1.4441e-03  5.8991e-03 -2.4711e-02
  1.3471e-02  2.0758e-02  9.6534e-03

(1 ,62,.,.) = 
 -4.8604e-03 -2.5021e-02 -4.2191e-02
  1.3725e-02 -2.0680e-02 -4.8109e-02
  2.4640e-02  1.4972e-02 -3.0200e-02

(1 ,63,.,.) = 
 -3.2639e-03  1.7118e-03 -1.0876e-03
 -9.8774e-03 -2.8971e-03 -3.1781e-03
  1.8190e-03 -1.2580e-02 -7.6514e-03
     ⋮ 

(2 ,0 ,.,.) = 
 -3.5578e-02  3.4780e-03  5.8688e-03
 -4.1381e-02  2.2893e-02 -1.1461e-02
 -2.9374e-02  2.5839e-03  4.8404e-03

(2 ,1 ,.,.) = 
 -3.3849e-02 -3.9511e-02 -4.7046e-02
 -5.1498e-02 -6.5836e-02 -4.1163e-02
 -3.0716e-02 -4.0902e-02 -3.2305e-02

(2 ,2 ,.,.) = 
 -3.6906e-03  1.0416e-02 -4.5497e-03
 -7.9338e-03 -1.7565e-02  1.8837e-02
 -8.6528e-03 -2.1467e-02  4.6485e-03
   ...

(2 ,61,.,.) = 
 -4.4951e-03  1.6729e-02 -2.3512e-02
 -1.5071e-02  3.1607e-02 -1.1856e-03
  2.8345e-04  1.2184e-02 -1.0772e-02

(2 ,62,.,.) = 
  1.3048e-02 -3.1117e-03 -2.6407e-03
  1.4620e-02 -1.1034e-02 -1.8682e-02
  8.3067e-03 -6.9058e-03 -8.1581e-03

(2 ,63,.,.) = 
 -3.1262e-03  2.2658e-03 -3.5499e-02
  2.9468e-02  3.6942e-02 -1.6511e-02
 -2.1330e-02 -2.0283e-03 -3.8132e-02
...   
     ⋮ 

(61,0 ,.,.) = 
  1.9015e-02 -1.9787e-03  1.1898e-02
  3.3850e-02  2.9796e-03  1.5825e-02
  2.4014e-02  1.9589e-02  7.7777e-03

(61,1 ,.,.) = 
  1.2416e-02  8.4996e-03  2.0058e-02
  2.1967e-02 -2.2207e-04  1.3904e-02
  1.7036e-02  1.0798e-02  3.2501e-02

(61,2 ,.,.) = 
  6.9128e-03  2.4620e-02  2.0987e-02
  1.3580e-02  1.8883e-02  8.9205e-03
  3.2908e-02  4.2235e-03 -7.6568e-03
   ...

(61,61,.,.) = 
 -1.1066e-02 -6.8930e-03 -2.3867e-02
  2.1260e-02  2.1823e-02 -2.9148e-02
 -2.7998e-02 -2.3903e-02 -4.1546e-02

(61,62,.,.) = 
  1.1273e-02 -4.2481e-03 -1.8486e-02
 -2.9172e-02 -3.2462e-02  3.1805e-03
 -1.5779e-02  1.5057e-03 -3.1037e-04

(61,63,.,.) = 
  1.7304e-02 -1.3856e-02  8.2595e-04
  1.4277e-02 -3.0786e-02 -1.0011e-03
  4.8532e-03  1.4432e-02  4.1100e-02
     ⋮ 

(62,0 ,.,.) = 
  4.5908e-02  3.7129e-02  1.2975e-02
  2.1326e-02  6.1570e-03 -1.9008e-02
  9.4289e-03 -2.4564e-02 -1.0380e-02

(62,1 ,.,.) = 
  2.7198e-03  1.4472e-02  1.0209e-03
  1.7572e-03  7.6154e-04  4.8318e-03
  6.8438e-03  1.4269e-02  4.6343e-03

(62,2 ,.,.) = 
 -1.6090e-03  3.8158e-03  3.3782e-02
 -1.4349e-03  1.5522e-02  6.1718e-02
  3.3080e-02  4.4800e-02  3.2992e-02
   ...

(62,61,.,.) = 
 -1.1974e-02  1.0980e-02 -4.0344e-02
  9.9280e-04  1.7168e-02 -2.6205e-02
 -1.5580e-02  4.5384e-02 -2.6620e-02

(62,62,.,.) = 
 -7.5865e-03  3.9624e-03  1.0910e-02
 -1.0516e-02 -5.6382e-03  2.5779e-02
 -9.3009e-03  1.6905e-02  2.5413e-02

(62,63,.,.) = 
  2.1498e-02  3.2472e-03 -2.0411e-02
  3.4126e-02 -1.7670e-02 -3.2316e-02
 -2.3196e-02 -2.6713e-02 -1.7770e-02
     ⋮ 

(63,0 ,.,.) = 
 -3.3570e-02 -5.8592e-03 -4.9914e-02
  4.4431e-03 -8.1181e-02 -1.0034e-02
 -2.1210e-02  1.2360e-02 -7.4776e-03

(63,1 ,.,.) = 
 -3.4235e-02  1.6652e-02 -1.4267e-03
 -8.8491e-03  6.0021e-02  5.9335e-03
 -2.1950e-02 -5.6134e-03 -1.8367e-02

(63,2 ,.,.) = 
  1.2906e-02  1.5449e-02  7.6274e-03
  1.5200e-02  8.9299e-03  1.6331e-02
  2.3584e-02  1.4231e-02  1.4983e-02
   ...

(63,61,.,.) = 
  2.4148e-02  8.2161e-03 -2.7140e-03
  2.0715e-02 -8.5727e-02  2.1260e-02
  1.4628e-03  2.1745e-02  1.2310e-02

(63,62,.,.) = 
  3.3777e-02  4.1994e-03  1.6696e-02
  3.3634e-03 -8.5029e-02  2.6096e-02
  7.1902e-03 -1.0627e-02  1.0069e-02

(63,63,.,.) = 
  2.1614e-02  1.3685e-02 -1.9401e-03
  2.8485e-02 -2.2742e-02 -9.2074e-03
  4.3265e-02  1.0341e-02  1.4337e-02
[torch.FloatTensor of size 64x64x3x3]
), ('layer1.1.bn2.weight', 
 0.3002
 0.1879
 0.1683
 0.1700
 0.2510
 0.1264
 0.1596
 0.1925
 0.1379
 0.1619
 0.1925
 0.2059
 0.2133
 0.1886
 0.1552
 0.1556
 0.2230
 0.2143
 0.1503
 0.2025
 0.1546
 0.1728
 0.1657
 0.1653
 0.1759
 0.2397
 0.1492
 0.1925
 0.1350
 0.1303
 0.2352
 0.2116
 0.2162
 0.1721
 0.1860
 0.2056
 0.1355
 0.2451
 0.1831
 0.2053
 0.1650
 0.1824
 0.2217
 0.1820
 0.2021
 0.1699
 0.2040
 0.1753
 0.1816
 0.1904
 0.1531
 0.2186
 0.1661
 0.1781
 0.1438
 0.2048
 0.1353
 0.1997
 0.2582
 0.1683
 0.1852
 0.1189
 0.1655
 0.1964
[torch.FloatTensor of size 64]
), ('layer1.1.bn2.bias', 
-0.3349
-0.1008
-0.0095
 0.0479
-0.0964
 0.3131
 0.0501
-0.0227
 0.2577
 0.1367
-0.1260
-0.0017
-0.1620
-0.0494
 0.1925
 0.0733
-0.0646
-0.1330
 0.0037
-0.1173
 0.0661
-0.1220
 0.0396
 0.0579
 0.0875
-0.2095
 0.2687
-0.1505
 0.2573
 0.2496
-0.0825
 0.0500
-0.0324
-0.0489
 0.0205
 0.0002
 0.1885
-0.0236
-0.0888
-0.0540
-0.2141
-0.0731
-0.0511
-0.0304
-0.0897
-0.0896
-0.1392
 0.0069
-0.1228
-0.1339
 0.0683
-0.0021
-0.0092
-0.0254
 0.2965
 0.0168
 0.1691
-0.0496
-0.1790
 0.1099
 0.0447
 0.1081
-0.0762
-0.0343
[torch.FloatTensor of size 64]
), ('layer1.1.bn2.running_mean', 
-0.1649
-0.0425
-0.0626
-0.1160
 0.0955
 0.0133
 0.0513
-0.1335
-0.1575
-0.0547
-0.1014
 0.0902
 0.0431
 0.0374
 0.0803
 0.0570
 0.0260
 0.0561
-0.3118
-0.0676
 0.0085
-0.0277
-0.0696
 0.0222
-0.0747
 0.0497
-0.1096
 0.0004
-0.1029
 0.0805
 0.0303
-0.0048
-0.0658
-0.1804
-0.0257
 0.0035
-0.1099
 0.1352
-0.0936
 0.0000
-0.0544
-0.0651
 0.0753
-0.1620
-0.0265
 0.0184
-0.0810
-0.0229
-0.0808
-0.0329
-0.1062
-0.0948
-0.2769
 0.0464
-0.1138
 0.0180
-0.1507
-0.0667
 0.0980
-0.0172
-0.0452
-0.0433
-0.1018
-0.0611
[torch.FloatTensor of size 64]
), ('layer1.1.bn2.running_var', 
1.00000e-02 *
  0.4941
  1.1150
  1.0733
  1.6545
  1.5780
  1.1149
  1.0387
  1.5641
  1.2111
  2.0006
  1.0641
  1.9218
  0.8372
  1.1975
  1.7643
  1.0960
  1.8567
  1.0713
  0.8426
  1.0627
  1.3369
  0.9642
  1.3323
  1.6954
  1.4635
  0.7849
  2.1182
  0.4997
  0.9477
  1.4696
  0.9714
  1.9139
  1.3041
  0.7589
  2.1161
  1.3164
  1.9353
  1.8018
  0.9208
  0.9328
  0.5662
  1.3474
  1.5454
  0.7420
  1.2087
  1.2921
  0.8584
  1.5368
  0.6517
  1.0204
  1.0683
  2.2173
  0.9659
  1.3837
  1.2310
  1.6885
  1.5271
  1.0687
  0.8818
  1.6348
  2.1353
  1.0426
  0.6147
  1.0983
[torch.FloatTensor of size 64]
), ('layer1.1.conv3.weight', 
( 0 , 0 ,.,.) = 
  6.2414e-02

( 0 , 1 ,.,.) = 
 -3.7548e-03

( 0 , 2 ,.,.) = 
 -9.5037e-04
    ... 

( 0 ,61 ,.,.) = 
 -5.4637e-03

( 0 ,62 ,.,.) = 
  2.2708e-03

( 0 ,63 ,.,.) = 
 -7.4026e-03
      ⋮  

( 1 , 0 ,.,.) = 
  9.6117e-02

( 1 , 1 ,.,.) = 
  7.1861e-03

( 1 , 2 ,.,.) = 
 -3.0478e-03
    ... 

( 1 ,61 ,.,.) = 
 -6.0525e-04

( 1 ,62 ,.,.) = 
 -1.9534e-03

( 1 ,63 ,.,.) = 
 -3.2167e-03
      ⋮  

( 2 , 0 ,.,.) = 
  5.0736e-02

( 2 , 1 ,.,.) = 
 -2.1512e-02

( 2 , 2 ,.,.) = 
 -1.5421e-02
    ... 

( 2 ,61 ,.,.) = 
  9.1875e-03

( 2 ,62 ,.,.) = 
  8.9473e-03

( 2 ,63 ,.,.) = 
  2.6045e-02
...     
      ⋮  

(253, 0 ,.,.) = 
  6.7911e-02

(253, 1 ,.,.) = 
 -1.1126e-03

(253, 2 ,.,.) = 
 -5.5242e-04
    ... 

(253,61 ,.,.) = 
 -4.6133e-04

(253,62 ,.,.) = 
  5.2510e-03

(253,63 ,.,.) = 
 -3.8238e-03
      ⋮  

(254, 0 ,.,.) = 
 -1.9320e-02

(254, 1 ,.,.) = 
  6.4255e-03

(254, 2 ,.,.) = 
  2.0596e-02
    ... 

(254,61 ,.,.) = 
  1.0755e-03

(254,62 ,.,.) = 
 -5.6299e-03

(254,63 ,.,.) = 
 -1.3803e-02
      ⋮  

(255, 0 ,.,.) = 
  4.5834e-02

(255, 1 ,.,.) = 
 -5.5471e-02

(255, 2 ,.,.) = 
  1.1134e-03
    ... 

(255,61 ,.,.) = 
 -4.7031e-03

(255,62 ,.,.) = 
 -1.9826e-02

(255,63 ,.,.) = 
 -3.8611e-02
[torch.FloatTensor of size 256x64x1x1]
), ('layer1.1.bn3.weight', 
 2.1259e-03
-6.9715e-03
 9.4608e-02
-1.1439e-02
 1.4931e-01
 1.6228e-03
 1.9745e-01
 1.8000e-02
-1.7183e-04
 1.1793e-01
 1.0572e-02
 1.5288e-01
 6.2277e-03
-9.6050e-04
 1.0127e-01
 1.4762e-01
 1.1484e-01
 2.5359e-02
 9.5329e-02
 2.8729e-03
 7.1651e-02
 2.7586e-03
 1.5039e-01
 2.7510e-01
 9.0146e-02
 1.2768e-01
 5.5667e-03
 1.3082e-01
-1.9346e-03
 5.0423e-02
 2.1226e-03
-7.5048e-03
 1.8880e-02
 1.1878e-01
 9.8673e-02
 7.3502e-02
 1.5206e-01
 8.6612e-02
 5.0623e-03
-1.2326e-03
 1.1257e-01
 5.9433e-02
 1.4774e-01
 8.9319e-02
 9.7549e-02
 6.0202e-02
 8.6732e-02
 1.5079e-01
 8.0304e-02
 2.7696e-02
-1.7456e-02
 1.7293e-01
 6.8518e-02
 1.4533e-01
 4.0339e-03
 2.5851e-02
 1.8742e-01
 6.8096e-02
 1.0810e-01
 1.1762e-01
 5.3549e-03
 7.9646e-02
-2.2026e-03
 1.5807e-01
 7.9670e-08
 3.1757e-05
 1.1837e-01
 3.6327e-02
 1.5932e-01
 3.8934e-02
 1.2485e-01
 6.9955e-06
 4.1722e-02
-1.8706e-03
 2.6218e-02
 1.5290e-01
 2.1404e-01
 1.1172e-01
-4.6150e-03
 1.1622e-01
 8.1356e-02
 4.0980e-02
 4.7201e-02
 4.9864e-02
 9.7825e-02
 7.0251e-02
 1.0111e-01
 7.0747e-02
-7.6863e-05
 3.2789e-04
 2.4123e-03
 1.1639e-01
 2.5553e-03
 1.1059e-01
 2.2347e-01
 1.6373e-01
 5.5450e-03
 1.1086e-01
 1.6115e-01
 1.3622e-01
 1.0025e-01
 1.9535e-06
 7.9823e-02
 4.2133e-03
 2.5824e-02
 1.2685e-01
 1.5052e-01
 9.5539e-02
 7.1596e-04
 6.7004e-02
 5.2219e-03
 5.9135e-02
 1.8622e-03
 5.1936e-02
 1.3801e-01
-1.0016e-02
 2.3089e-03
 1.5128e-01
 5.8505e-02
 1.8369e-03
 1.3944e-01
 1.8065e-01
 1.2049e-01
-2.3750e-05
 1.6272e-01
 5.0985e-02
 1.4397e-01
-4.7397e-05
 7.9884e-02
 5.2822e-02
 1.1337e-01
 1.5378e-01
 1.0323e-01
 1.5092e-01
 8.5652e-02
 1.3144e-01
 8.7746e-02
 1.4908e-01
 1.1047e-01
 1.5284e-01
 1.2085e-01
 8.5849e-03
 5.9879e-02
 1.0519e-01
 5.8930e-02
 7.4151e-02
-4.9627e-03
 1.2280e-01
-1.1367e-01
 7.9317e-02
 1.2471e-02
 7.1211e-02
 2.1419e-01
 5.7342e-02
 7.2397e-02
 1.8971e-01
 1.3408e-01
 2.0415e-01
 2.4954e-03
 1.2848e-03
 8.5444e-04
 6.0994e-02
 1.9005e-03
 3.4890e-03
 1.7403e-01
 1.9638e-01
 1.8264e-01
 5.0056e-03
 1.0441e-01
 1.1164e-01
 2.0009e-01
 1.5155e-02
 1.8996e-01
 1.4341e-01
 1.5146e-01
 8.5111e-02
 8.0783e-02
 1.6485e-03
 1.0222e-01
 1.4608e-01
-5.9042e-02
-1.8772e-03
 1.3604e-01
 1.9610e-01
 2.5466e-03
 2.1734e-06
-6.8739e-03
 4.4865e-03
 5.7102e-02
-5.8736e-03
 1.7767e-01
 9.2354e-04
-3.2072e-03
 2.2485e-02
 2.0056e-01
 4.5350e-03
 9.2664e-02
 8.6701e-02
 6.1892e-02
 1.6515e-01
 1.3486e-02
 8.1960e-02
 1.8972e-06
 2.1170e-02
 1.7591e-01
-2.7518e-04
 1.5886e-01
 5.5978e-02
 7.8433e-02
 9.7138e-02
 8.8362e-02
-5.5156e-02
 1.8277e-01
 1.0541e-01
 1.7125e-01
 2.3317e-02
 1.0716e-01
 1.0549e-01
 7.3666e-02
 7.0549e-02
 3.6463e-03
 8.2517e-02
 1.3419e-01
-2.9456e-03
 1.0492e-01
 1.1982e-01
 1.6136e-04
 5.0766e-02
 1.9301e-03
 1.5054e-01
 1.1902e-01
-2.0978e-03
 1.9658e-01
 4.5569e-03
 6.8229e-02
 1.3388e-02
 1.8132e-01
-5.9605e-04
 1.6247e-01
 3.2930e-02
 1.1200e-01
 1.9393e-01
 2.5808e-03
 8.3123e-02
 1.1253e-01
 1.7063e-01
 1.0440e-01
 1.7874e-01
 7.7854e-02
 1.5666e-01
 5.0905e-02
-2.7906e-03
 6.0902e-03
 8.2056e-04
 2.6993e-02
 2.0848e-01
[torch.FloatTensor of size 256]
), ('layer1.1.bn3.bias', 
 3.9980e-03
 5.0117e-03
-6.5528e-02
 1.8528e-03
-4.9248e-02
 2.5485e-03
 6.3341e-02
-6.2315e-03
 2.3204e-03
 4.1033e-02
-1.3602e-02
 9.2113e-02
 5.1501e-02
 8.3470e-03
-3.8014e-02
 2.8642e-02
-3.2982e-02
-3.1293e-03
-9.6352e-02
 2.4645e-03
 1.8199e-02
 1.1311e-04
 6.3481e-02
 1.9443e-02
-3.1420e-02
-5.5623e-02
 5.3881e-03
 1.0326e-02
 3.8256e-03
-3.4432e-02
-1.9697e-03
 7.8351e-03
 2.7125e-02
 1.1254e-01
 9.4352e-02
 1.0976e-01
-1.0558e-01
-2.2601e-02
 3.3063e-03
 3.2106e-03
-2.4838e-02
-2.3685e-02
-3.2189e-02
-7.1171e-02
 3.1299e-02
 4.1259e-02
 2.1263e-02
-6.1532e-02
-1.1498e-01
 5.9384e-02
 1.2950e-02
 9.2789e-02
-4.9363e-02
 7.3796e-03
 1.7671e-03
-4.1717e-03
 1.4550e-01
-4.8883e-02
-7.7205e-02
-7.8253e-04
 9.3112e-03
-4.0488e-02
 3.3157e-03
 6.0696e-02
-2.8872e-07
 5.0370e-03
-6.1416e-02
 3.6578e-02
-6.5839e-02
 5.3615e-03
-4.3997e-02
-5.9013e-05
 4.0512e-02
 4.0043e-03
-8.4623e-03
 7.2705e-02
 4.0860e-02
-5.0787e-02
 6.0637e-03
-1.8153e-02
 2.0342e-03
 5.1603e-02
 8.6173e-03
 9.7429e-02
 1.2427e-01
 9.1219e-02
 5.4813e-02
-2.4722e-02
 8.3121e-03
-2.7292e-03
 1.1337e-03
 9.3664e-03
 1.9196e-03
 6.6088e-03
 9.5043e-02
 5.2576e-02
 5.9158e-03
-5.3450e-02
 1.0382e-01
 1.3383e-01
 7.6762e-03
-3.7883e-05
-8.0345e-04
 2.2704e-03
 6.0525e-03
-3.0990e-02
-1.4757e-01
-3.1099e-02
 1.7915e-03
 1.7390e-02
 2.5397e-03
 1.1629e-01
 2.4081e-03
 4.4117e-03
 5.3332e-02
-6.1501e-03
 3.1353e-03
-3.3524e-03
 5.4908e-03
 3.5045e-03
-7.7928e-02
-1.3230e-01
-6.2576e-02
-5.2930e-04
-1.7536e-02
 3.4290e-02
 2.3256e-02
-7.7662e-04
-7.5449e-02
 5.0531e-03
-4.0019e-02
-1.0415e-02
 3.1160e-02
-1.5949e-02
-7.4926e-02
-1.8892e-02
-1.6239e-02
-5.3514e-02
 5.5231e-02
-1.1096e-01
-4.4315e-02
 2.6778e-03
-1.0696e-02
 1.6248e-02
-1.0240e-01
-5.0648e-02
 1.3774e-02
 1.3104e-02
-3.1626e-02
 1.5090e-02
-8.3022e-02
-4.2749e-02
-5.2607e-02
 3.0274e-03
 5.8953e-02
 5.3911e-02
-1.6867e-02
 9.5402e-03
 2.3234e-03
 3.7943e-03
 7.4951e-03
 2.6353e-04
 8.1738e-03
 2.3198e-03
 2.3122e-02
 3.4564e-02
-8.3066e-02
 9.4532e-03
-1.8433e-02
 2.1947e-02
 1.5602e-01
 6.9935e-03
-1.3894e-02
-2.8587e-02
-2.7557e-02
-1.1292e-01
-2.8270e-02
 4.6501e-03
 1.9597e-02
 9.3797e-02
 4.3781e-03
-9.2987e-03
 3.9714e-02
 4.4164e-02
 3.5342e-03
-3.3409e-06
-2.8622e-03
 6.1362e-02
-4.5803e-03
 4.8157e-03
-5.8102e-02
 4.9529e-03
 3.6021e-03
-4.0624e-05
 1.9674e-02
 5.7639e-04
-4.6084e-03
 5.8968e-03
-3.6626e-02
-3.5292e-02
-3.6024e-03
 3.8701e-03
-2.2053e-05
 1.0455e-02
-3.3503e-02
 3.2163e-03
-2.1732e-02
 6.6086e-02
 2.0005e-02
 1.3922e-01
 1.1378e-02
-6.7080e-02
 3.6496e-02
-2.4453e-02
-1.4834e-01
 7.1466e-03
-2.3019e-02
 1.9417e-02
 2.1519e-02
-2.5884e-02
 6.2051e-03
-3.8573e-02
 3.2588e-02
 6.4501e-03
 1.1272e-01
-3.1017e-02
 4.9000e-03
-3.6400e-03
 7.3899e-03
 5.2943e-02
-4.5050e-02
 6.7647e-03
 9.6522e-02
 3.5004e-03
 4.6488e-02
-4.7906e-04
-1.1251e-01
 9.4231e-03
 3.7263e-02
-1.6908e-02
-7.2099e-03
 1.2631e-01
 5.0302e-03
-4.8352e-02
 1.7477e-03
-2.7385e-02
-7.2289e-02
-8.5673e-02
-5.9411e-02
 1.8720e-03
 1.2158e-02
 3.6557e-03
 6.4777e-03
 6.6302e-03
 2.9329e-02
-7.0381e-02
[torch.FloatTensor of size 256]
), ('layer1.1.bn3.running_mean', 
-1.5355e-03
 3.5206e-03
 9.8683e-03
 8.9528e-04
-5.1610e-03
-1.3300e-03
-3.9607e-02
 1.7694e-02
-3.8125e-03
 1.2307e-02
-9.1723e-03
 8.5022e-02
-2.4051e-04
-1.0732e-03
-5.9345e-03
 9.1655e-02
-5.8496e-02
 5.6066e-03
 1.6602e-02
-1.8997e-03
 1.2003e-02
 1.1430e-03
-1.6376e-02
 1.4525e-02
-2.0649e-02
 3.6930e-02
-5.4607e-03
 1.1615e-02
 4.5927e-04
 4.8512e-03
 2.6684e-03
 3.5294e-03
 5.9788e-03
-1.2930e-02
 1.1789e-01
-2.2506e-02
 3.9974e-02
-2.3902e-02
 6.5796e-03
 3.9684e-03
-2.4845e-02
 2.7730e-02
-3.1880e-02
-2.6861e-02
 1.5417e-02
-1.9417e-03
-4.0244e-03
-2.6849e-03
-5.0031e-03
-4.7903e-03
-1.2285e-02
 8.3905e-02
 1.3105e-02
-1.4817e-02
-3.9546e-03
-6.7918e-03
-5.5754e-02
-1.2244e-03
-1.3643e-03
-8.8827e-02
-2.7345e-03
 2.4220e-02
 1.6946e-03
 4.8630e-02
 7.5132e-08
 1.1232e-03
-2.5612e-03
 6.9059e-03
-1.7572e-02
 7.4865e-03
-2.0081e-02
-2.6757e-05
-2.2158e-02
-2.0169e-03
-2.0245e-02
-6.1523e-02
 5.6430e-03
-1.4463e-02
 9.7411e-04
-4.7249e-03
-1.7861e-02
-7.9556e-03
-5.8604e-03
 1.9475e-02
 7.6682e-03
-4.5478e-02
-4.5160e-02
-3.9787e-02
-1.5623e-03
 1.6484e-03
 2.1646e-03
-2.0837e-03
-2.8157e-03
-7.5843e-02
 6.9821e-02
 8.4164e-02
-3.2983e-03
 4.0499e-02
 5.0765e-02
 3.3680e-02
 8.3257e-03
 6.5315e-06
-6.7872e-02
-2.4685e-04
-5.8981e-04
 2.6004e-02
 8.8333e-02
 1.2155e-02
-6.6995e-04
-1.9646e-02
-1.2032e-03
-2.6116e-03
-1.8256e-04
-1.6298e-03
 8.2201e-02
 4.6975e-04
 3.8456e-03
 2.0043e-02
-2.1426e-02
-9.7016e-03
 1.1856e-02
 9.6565e-02
 5.4461e-02
 2.3182e-05
 2.0334e-02
 2.4781e-02
 1.8686e-02
-5.7721e-05
 4.2822e-02
-7.6190e-02
 1.8439e-02
-3.6267e-02
-3.5468e-02
-7.6590e-02
-5.9707e-03
 1.8361e-02
-1.4986e-02
 4.0829e-02
 3.0942e-02
-4.5691e-02
-2.9050e-02
-8.7391e-03
-2.6596e-02
 4.3500e-02
-4.8679e-03
 3.5761e-03
-2.4310e-02
-6.9883e-03
-7.9184e-02
-4.2242e-02
 5.4395e-03
-8.8421e-03
-3.0235e-02
-1.7066e-02
 2.7673e-02
 4.0418e-04
 4.2599e-02
-3.6047e-02
 1.5848e-03
-9.0163e-04
-4.0619e-03
-3.2892e-02
-2.5397e-03
 8.8179e-03
-6.0541e-02
-1.4980e-02
 3.6815e-02
-2.0167e-03
 3.1623e-02
 1.7589e-02
 5.9192e-03
-3.2780e-03
 1.0303e-01
 2.8881e-02
-4.1698e-02
-1.2167e-02
-1.6177e-02
-1.5762e-03
-4.3460e-02
-2.3999e-02
 2.2221e-02
 3.8782e-04
 8.9114e-03
-5.4990e-02
 2.6895e-03
-1.0079e-06
-1.9484e-03
-1.2783e-02
-2.4957e-02
 5.0299e-03
 5.6951e-02
 3.3706e-04
-2.0571e-03
 3.1387e-03
-1.0482e-01
-2.2082e-03
 6.7033e-03
-1.3579e-03
 5.1528e-02
 4.4635e-02
-4.7119e-03
-1.7849e-02
 4.7379e-06
 1.2873e-02
 4.8295e-02
 2.2569e-03
-1.8694e-02
-4.3872e-02
 1.0160e-02
-3.4264e-02
 3.8592e-02
 1.2346e-02
 9.5528e-03
-7.7189e-02
 1.0156e-02
 1.7587e-02
 6.1361e-02
-2.1576e-02
 1.3877e-02
-1.5375e-03
 4.1898e-03
-4.5855e-02
 1.1068e-02
-5.2434e-03
-1.9940e-02
 2.1089e-02
 6.6855e-03
 2.7400e-02
-5.1136e-03
 1.3025e-02
 1.0394e-02
 5.7614e-03
 1.4352e-02
-1.7066e-03
-6.8255e-02
-7.3266e-03
-2.6218e-02
 6.0590e-04
 1.8883e-02
-5.4684e-03
 9.9604e-03
 1.1055e-01
-4.0167e-04
 1.9481e-02
-4.9484e-03
 3.3405e-02
 6.6228e-03
-1.5261e-02
-4.8901e-02
 3.5977e-02
-4.0215e-03
 9.1971e-03
 6.4801e-04
 2.7684e-03
 2.2536e-02
-2.9462e-03
[torch.FloatTensor of size 256]
), ('layer1.1.bn3.running_var', 
 1.6059e-04
 3.7171e-04
 6.0172e-04
 3.1263e-04
 1.5401e-03
 1.3912e-04
 5.1252e-03
 3.8958e-04
 1.6432e-04
 2.4001e-03
 2.1704e-04
 2.7672e-03
 2.0609e-04
 2.6188e-04
 1.7928e-03
 2.7096e-03
 2.8114e-03
 4.3027e-04
 8.9001e-04
 2.0395e-04
 1.0153e-03
 1.2456e-04
 2.3369e-03
 3.0839e-03
 4.7554e-03
 1.0958e-03
 1.1797e-04
 2.1959e-03
 1.5873e-04
 1.0983e-03
 1.7646e-04
 1.8170e-04
 4.6641e-04
 1.3129e-03
 2.0276e-03
 2.0854e-03
 3.2420e-03
 1.2724e-03
 4.4697e-04
 1.4508e-04
 1.6962e-03
 1.2825e-03
 2.7862e-03
 1.6028e-03
 2.0640e-03
 6.0061e-04
 2.3397e-03
 1.4463e-03
 3.5955e-04
 2.2135e-04
 2.2852e-04
 4.2415e-03
 4.4674e-04
 2.5317e-03
 1.3197e-04
 3.5224e-04
 4.3248e-03
 1.0168e-03
 1.9355e-03
 1.8114e-03
 1.5462e-04
 2.2153e-03
 8.7030e-05
 1.9541e-03
 7.1199e-15
 3.0818e-04
 1.1505e-03
 1.0542e-03
 1.2448e-03
 6.9630e-04
 6.7557e-04
 5.3504e-10
 1.9074e-03
 3.4480e-04
 6.6071e-04
 1.7577e-03
 3.7038e-03
 6.9306e-04
 1.2202e-04
 1.6315e-03
 2.0219e-03
 1.1096e-03
 5.5956e-04
 1.0971e-03
 2.8319e-03
 3.8107e-03
 1.7650e-03
 6.1997e-04
 2.4530e-04
 1.5738e-06
 5.1718e-04
 9.5326e-04
 1.5084e-04
 1.3310e-03
 3.5571e-03
 2.6964e-03
 1.2123e-04
 1.4777e-03
 2.4729e-03
 3.2416e-03
 1.8321e-03
 1.5474e-10
 1.5228e-03
 1.0067e-04
 5.2077e-04
 3.1192e-03
 8.9203e-04
 1.1522e-03
 7.7882e-05
 1.3583e-03
 3.1590e-04
 1.4761e-03
 1.6482e-04
 1.4982e-03
 2.5698e-03
 1.3539e-04
 1.8730e-04
 1.7006e-03
 2.6585e-03
 2.0711e-04
 2.4375e-03
 1.3295e-03
 2.5772e-03
 2.1349e-08
 5.3574e-03
 1.0283e-03
 1.8008e-03
 6.8292e-09
 1.2264e-03
 1.1297e-03
 3.3789e-03
 1.6910e-03
 1.6149e-03
 2.0707e-03
 4.9697e-04
 3.5366e-03
 1.0101e-03
 1.6570e-03
 3.7598e-03
 1.0801e-03
 1.3565e-03
 2.2261e-04
 5.7109e-04
 2.0363e-03
 5.4472e-04
 1.9129e-03
 2.4459e-04
 2.2633e-03
 1.4812e-03
 9.6511e-04
 1.8623e-04
 9.4343e-04
 2.7048e-03
 1.0206e-03
 8.0362e-04
 2.5770e-03
 3.3771e-03
 3.0025e-03
 1.3301e-04
 8.6049e-05
 2.0172e-04
 1.2300e-03
 1.1685e-04
 5.6866e-04
 1.8653e-03
 6.1761e-03
 2.5459e-03
 1.8459e-04
 1.8816e-03
 2.0060e-03
 4.4225e-03
 1.2363e-04
 2.0399e-03
 1.9949e-03
 1.3457e-03
 1.0811e-03
 1.4102e-03
 1.6148e-04
 1.7055e-03
 3.1829e-03
 9.2222e-04
 5.9365e-05
 2.0700e-03
 3.4515e-03
 1.7616e-04
 2.3745e-13
 1.8068e-04
 1.9361e-04
 1.1490e-03
 5.0123e-04
 6.3733e-03
 1.2193e-04
 3.7616e-04
 2.3350e-04
 2.9193e-03
 1.0305e-04
 3.1189e-03
 2.5093e-03
 1.0099e-03
 1.3040e-03
 5.0880e-04
 1.5035e-03
 5.7435e-11
 5.5996e-04
 2.1983e-03
 1.1178e-04
 1.6135e-03
 1.3834e-03
 1.4591e-03
 2.3481e-03
 1.2027e-03
 6.3740e-04
 1.7551e-03
 1.5039e-03
 8.3988e-04
 3.6984e-04
 1.3841e-03
 1.4800e-03
 1.5244e-03
 1.3660e-03
 9.6345e-04
 9.7534e-04
 2.0422e-03
 3.9350e-04
 1.7050e-03
 2.0491e-03
 3.3724e-04
 8.2446e-04
 3.1551e-04
 3.2863e-03
 1.0703e-03
 2.1004e-04
 5.2254e-03
 2.2960e-04
 1.3640e-03
 1.4431e-04
 2.8227e-03
 1.2794e-03
 1.8816e-03
 4.6199e-04
 1.2726e-03
 3.4149e-03
 4.1669e-04
 2.2533e-03
 2.8541e-03
 4.1364e-03
 9.1527e-04
 2.3870e-03
 7.0216e-04
 2.0830e-03
 8.6607e-04
 4.2526e-04
 1.6696e-04
 1.8284e-04
 5.9375e-04
 3.3832e-03
[torch.FloatTensor of size 256]
), ('layer1.2.conv1.weight', 
( 0 , 0 ,.,.) = 
  5.7859e-03

( 0 , 1 ,.,.) = 
  1.3108e-03

( 0 , 2 ,.,.) = 
 -4.1067e-03
    ... 

( 0 ,253,.,.) = 
 -4.2579e-03

( 0 ,254,.,.) = 
 -4.1070e-02

( 0 ,255,.,.) = 
 -3.8527e-02
      ⋮  

( 1 , 0 ,.,.) = 
  1.9597e-03

( 1 , 1 ,.,.) = 
 -1.0637e-02

( 1 , 2 ,.,.) = 
  1.8890e-02
    ... 

( 1 ,253,.,.) = 
  4.8657e-03

( 1 ,254,.,.) = 
 -1.1662e-01

( 1 ,255,.,.) = 
  7.1150e-02
      ⋮  

( 2 , 0 ,.,.) = 
 -1.3912e-02

( 2 , 1 ,.,.) = 
  1.8223e-03

( 2 , 2 ,.,.) = 
  2.2519e-02
    ... 

( 2 ,253,.,.) = 
 -2.2903e-02

( 2 ,254,.,.) = 
 -1.1148e-02

( 2 ,255,.,.) = 
  7.2517e-03
...     
      ⋮  

(61 , 0 ,.,.) = 
  2.5342e-03

(61 , 1 ,.,.) = 
 -3.2014e-03

(61 , 2 ,.,.) = 
 -1.0234e-02
    ... 

(61 ,253,.,.) = 
  1.2434e-02

(61 ,254,.,.) = 
 -1.1588e-02

(61 ,255,.,.) = 
  1.7599e-02
      ⋮  

(62 , 0 ,.,.) = 
  4.4654e-04

(62 , 1 ,.,.) = 
  5.1545e-03

(62 , 2 ,.,.) = 
 -2.1940e-02
    ... 

(62 ,253,.,.) = 
 -1.4156e-02

(62 ,254,.,.) = 
  6.5771e-02

(62 ,255,.,.) = 
 -2.3287e-02
      ⋮  

(63 , 0 ,.,.) = 
 -2.4907e-03

(63 , 1 ,.,.) = 
 -7.4796e-05

(63 , 2 ,.,.) = 
 -3.3184e-02
    ... 

(63 ,253,.,.) = 
 -9.5518e-03

(63 ,254,.,.) = 
  2.8951e-02

(63 ,255,.,.) = 
 -1.3137e-02
[torch.FloatTensor of size 64x256x1x1]
), ('layer1.2.bn1.weight', 
 0.1913
 0.1692
 0.0998
 0.1557
 0.2209
 0.2088
 0.1592
 0.1858
 0.1681
 0.1715
 0.2173
 0.1619
 0.1758
 0.2133
 0.2372
 0.1801
 0.2278
 0.1665
 0.1719
 0.1989
 0.1980
 0.1442
 0.1775
 0.2147
 0.1963
 0.1785
 0.1716
 0.1923
 0.1763
 0.2147
 0.1436
 0.1808
 0.1411
 0.2095
 0.1432
 0.2235
 0.1475
 0.2037
 0.1705
 0.1889
 0.1268
 0.2000
 0.2031
 0.1841
 0.1522
 0.1639
 0.2384
 0.1928
 0.1385
 0.1841
 0.1849
 0.1573
 0.2080
 0.1951
 0.2446
 0.1780
 0.1835
 0.1956
 0.1273
 0.2016
 0.1753
 0.1431
 0.2104
 0.1973
[torch.FloatTensor of size 64]
), ('layer1.2.bn1.bias', 
-0.0211
-0.0407
-0.0112
-0.1323
-0.1030
-0.0420
-0.0627
 0.0480
 0.0803
 0.1414
-0.1111
-0.1918
 0.0786
-0.0938
-0.1518
 0.0853
-0.1570
 0.1497
 0.0868
-0.0677
-0.0525
 0.1388
 0.0777
-0.0986
 0.0222
 0.0800
 0.0634
 0.0124
-0.0755
-0.1144
 0.0747
-0.0626
 0.1637
-0.0787
 0.1791
-0.0868
 0.1491
-0.0757
 0.0105
-0.1133
-0.0680
 0.0583
-0.0410
-0.0515
 0.0492
-0.0792
-0.1330
-0.0540
 0.0605
-0.1405
-0.0211
 0.1586
-0.1310
-0.0853
-0.1872
 0.0526
-0.0925
-0.0578
 0.0782
-0.1039
-0.0210
 0.0612
-0.1123
-0.0781
[torch.FloatTensor of size 64]
), ('layer1.2.bn1.running_mean', 
-0.1160
-0.1021
 0.1462
-0.0003
 0.1511
 0.0329
 0.0378
 0.0058
-0.1910
 0.1555
 0.0213
 0.0646
-0.0677
 0.0079
-0.0400
 0.0966
-0.5747
 0.0436
 0.0559
 0.1285
 0.1424
 0.0867
 0.0708
-0.0617
 0.0197
 0.2885
 0.0896
 0.0896
-0.0470
-0.0761
 0.1419
 0.0826
 0.0750
-0.0771
 0.1705
 0.0497
-0.0870
 0.0226
 0.1024
 0.1446
 0.0062
 0.0580
-0.1221
 0.1015
-0.0367
-0.0095
-0.1327
-0.0464
-0.2389
-0.1118
-0.0695
 0.0219
-0.0461
-0.1395
-0.0522
 0.1193
-0.0468
-0.0568
-0.2562
 0.0758
-0.0747
-0.0516
-0.0297
-0.0184
[torch.FloatTensor of size 64]
), ('layer1.2.bn1.running_var', 
1.00000e-02 *
  1.9547
  1.3131
  2.6238
  0.4289
  1.9823
  3.3334
  0.6503
  2.2718
  2.2034
  2.0131
  1.9806
  0.4805
  2.8621
  2.8290
  1.9294
  2.7039
  0.7379
  3.7644
  1.9082
  2.1781
  2.0195
  1.2359
  2.1478
  1.4274
  2.5909
  2.9041
  2.4122
  2.5449
  1.8787
  1.1984
  1.3388
  2.0437
  1.8403
  2.1252
  1.7979
  3.0131
  1.5480
  1.4082
  1.6731
  1.8842
  0.5436
  3.1316
  1.9512
  1.7993
  1.8393
  0.4709
  1.9859
  1.6802
  1.7581
  0.8550
  1.6233
  3.4044
  0.9338
  1.6302
  1.8020
  2.9786
  0.9292
  1.7732
  1.6033
  1.6265
  1.6220
  2.0594
  1.3559
  1.8368
[torch.FloatTensor of size 64]
), ('layer1.2.conv2.weight', 
(0 ,0 ,.,.) = 
  3.3096e-02 -2.2837e-03 -2.6060e-02
 -1.8888e-03 -1.4137e-02 -7.5973e-03
 -4.9556e-02  2.8547e-02 -1.2019e-02

(0 ,1 ,.,.) = 
  2.0762e-02 -6.0752e-02  1.7711e-02
  2.2331e-02 -5.3168e-02  2.7273e-02
 -1.8588e-02  7.2604e-03 -4.4545e-03

(0 ,2 ,.,.) = 
  1.1293e-02  4.2911e-03  2.0791e-02
 -2.3401e-03 -1.7693e-02  1.8178e-02
 -5.9822e-03 -9.6152e-03  8.1386e-03
   ...

(0 ,61,.,.) = 
  1.3661e-02 -1.2209e-02 -4.6657e-03
 -3.2961e-03 -3.7944e-03 -2.1736e-02
 -2.4662e-02  2.3986e-02 -1.5785e-02

(0 ,62,.,.) = 
 -4.2326e-02  5.5694e-03  1.0661e-02
 -4.7409e-02  3.7776e-02  2.1256e-02
 -9.4837e-03  1.1733e-02 -6.1717e-03

(0 ,63,.,.) = 
  3.1075e-03 -7.9047e-03 -2.5325e-02
  8.5359e-03 -6.7326e-03 -1.2789e-02
  7.2580e-03 -1.6795e-02 -7.6375e-03
     ⋮ 

(1 ,0 ,.,.) = 
  2.1006e-02 -1.9891e-02 -2.5320e-02
 -3.3499e-04  1.9618e-02 -2.0495e-02
  4.6272e-02  7.7210e-02 -5.5360e-02

(1 ,1 ,.,.) = 
 -3.7223e-02  6.7949e-02 -3.3836e-02
 -5.4784e-02  6.3341e-02 -3.2365e-02
 -5.3404e-03 -1.1135e-02 -7.8994e-04

(1 ,2 ,.,.) = 
  2.9878e-02 -5.3996e-03  9.8225e-03
 -3.4435e-03  1.1870e-03 -4.5666e-03
 -5.3928e-03 -9.0694e-03  3.1196e-03
   ...

(1 ,61,.,.) = 
 -1.8036e-02  2.4209e-02  7.9403e-03
 -8.6619e-03  2.1570e-02 -6.4812e-03
 -2.2654e-02  8.9033e-03  2.2195e-02

(1 ,62,.,.) = 
  6.1954e-03 -1.9543e-02 -1.2755e-02
  1.8484e-02 -1.3425e-02  2.4133e-05
  1.7827e-03 -5.4650e-03  3.1103e-02

(1 ,63,.,.) = 
 -1.4821e-02  1.6666e-02 -3.1383e-02
 -9.5354e-05  2.7465e-02  2.4956e-02
  7.7412e-03  2.5979e-03 -2.1601e-02
     ⋮ 

(2 ,0 ,.,.) = 
 -1.4079e-02  8.0381e-03 -1.7125e-02
  3.8214e-03 -2.4952e-02 -2.6993e-02
  2.8850e-02  1.9032e-02 -8.6727e-03

(2 ,1 ,.,.) = 
  8.9605e-03  1.1186e-02  2.7754e-02
  3.4283e-03 -6.6164e-02  2.5678e-02
  3.7452e-02 -2.1241e-02 -2.8383e-02

(2 ,2 ,.,.) = 
  1.2442e-02  5.2476e-03 -5.9283e-03
  1.4631e-02 -1.2374e-02 -2.8127e-03
 -8.1462e-04 -2.5496e-03 -6.6706e-03
   ...

(2 ,61,.,.) = 
 -1.4014e-02  2.1514e-02  5.0287e-02
 -6.9800e-02 -8.5586e-03  2.6910e-02
 -4.7790e-02 -6.6275e-02  3.5433e-02

(2 ,62,.,.) = 
 -1.2001e-03 -2.2680e-02 -7.8468e-03
 -3.3417e-02  2.9589e-02 -8.3864e-02
 -1.2242e-02  2.2470e-02  8.2582e-03

(2 ,63,.,.) = 
 -1.6228e-03  1.9872e-02 -5.9169e-03
  9.8616e-03 -4.3809e-03  1.3927e-02
 -5.0480e-03  2.1110e-02 -2.3370e-03
...   
     ⋮ 

(61,0 ,.,.) = 
 -4.9515e-02 -2.5277e-03  2.3704e-03
 -2.5449e-02  2.5781e-03  1.3212e-02
 -2.8367e-02  6.1904e-03 -7.1778e-04

(61,1 ,.,.) = 
 -1.6230e-03 -1.3667e-02  1.6481e-02
 -7.6718e-03 -3.8440e-02  4.1870e-02
 -8.3919e-04 -2.0845e-02  8.6331e-03

(61,2 ,.,.) = 
  2.7622e-02  9.3908e-03  1.2196e-02
  2.0812e-02  2.6994e-03 -4.2113e-03
  2.7357e-02  7.6224e-04 -8.0598e-03
   ...

(61,61,.,.) = 
 -4.1468e-02 -2.5943e-02  2.7411e-02
 -6.6122e-02 -4.1655e-02  5.9899e-02
 -6.1420e-02  2.5395e-02  3.8950e-02

(61,62,.,.) = 
  6.2085e-02  5.7126e-03  8.0286e-05
 -1.9108e-02 -3.2557e-02 -3.4181e-02
 -2.2809e-02 -9.4224e-03 -1.4776e-02

(61,63,.,.) = 
 -2.6983e-02 -2.4507e-02 -1.3865e-02
 -4.2552e-02 -3.6843e-02  6.1647e-02
 -3.8629e-02 -2.1726e-03  4.2447e-02
     ⋮ 

(62,0 ,.,.) = 
  7.4762e-03 -1.5865e-02  2.5887e-02
  2.6152e-02 -1.9164e-02 -6.5627e-03
 -4.7026e-05 -1.0471e-02 -1.3037e-03

(62,1 ,.,.) = 
 -4.9933e-03  1.1487e-02 -3.9681e-02
 -1.2797e-02  1.7415e-02 -2.6448e-02
  3.2297e-02  1.2037e-02 -1.5810e-02

(62,2 ,.,.) = 
  3.1153e-02  4.1138e-03  1.1098e-02
 -2.8878e-03 -4.0853e-03 -4.8585e-03
  4.2662e-03 -5.7329e-03  7.8327e-03
   ...

(62,61,.,.) = 
 -2.5365e-02  1.6538e-02  5.1379e-02
 -3.9035e-02  3.4860e-02  1.0149e-02
 -4.4507e-02  3.3185e-02 -2.4550e-02

(62,62,.,.) = 
 -2.4078e-02 -1.9366e-02 -1.0191e-02
 -1.6126e-02  1.9683e-03  5.2968e-02
 -1.1998e-03  2.4451e-02  6.5782e-03

(62,63,.,.) = 
 -5.2594e-03 -2.8607e-03  2.9125e-02
  1.5266e-02  6.4225e-03 -9.4151e-03
  3.6973e-02 -5.3924e-04 -2.3406e-02
     ⋮ 

(63,0 ,.,.) = 
  9.4780e-03  1.8678e-02  4.8434e-03
  2.7248e-02 -3.3991e-02  1.4853e-02
 -4.3406e-02  1.9014e-02 -1.0946e-02

(63,1 ,.,.) = 
  3.8813e-03 -1.0640e-02 -1.2859e-02
  1.4223e-02  1.7025e-02  1.5361e-02
 -2.6170e-02 -1.2333e-02  3.2009e-02

(63,2 ,.,.) = 
  1.1578e-02  1.5274e-04  3.5564e-03
  7.9022e-03  2.6997e-02  9.6522e-03
 -1.3545e-03 -2.9908e-04  1.0236e-02
   ...

(63,61,.,.) = 
 -4.0883e-02 -1.2606e-02  5.2883e-03
  1.9850e-02 -3.1524e-02 -4.9819e-02
  1.5679e-02  4.7178e-02 -2.1810e-02

(63,62,.,.) = 
 -1.1884e-02 -1.6430e-02  4.3056e-03
  2.1827e-02 -3.3460e-02 -1.5285e-02
 -1.9582e-04  3.6705e-02 -1.6792e-02

(63,63,.,.) = 
 -1.6154e-02  1.7052e-02  1.9213e-03
 -1.3728e-02  2.2707e-02  7.7133e-03
  2.4167e-02 -9.8935e-03 -8.9031e-03
[torch.FloatTensor of size 64x64x3x3]
), ('layer1.2.bn2.weight', 
 0.2164
 0.2510
 0.2231
 0.2292
 0.2206
 0.2179
 0.2162
 0.1634
 0.2093
 0.2222
 0.2535
 0.3154
 0.2342
 0.2426
 0.2290
 0.2137
 0.2216
 0.1808
 0.2402
 0.2401
 0.2296
 0.2416
 0.2316
 0.2086
 0.2435
 0.2377
 0.2285
 0.2185
 0.2242
 0.2128
 0.2328
 0.2420
 0.1663
 0.2489
 0.1979
 0.2088
 0.2402
 0.1934
 0.1159
 0.2452
 0.2377
 0.1765
 0.2250
 0.1855
 0.2238
 0.2285
 0.2402
 0.2207
 0.2157
 0.2389
 0.2259
 0.2190
 0.2020
 0.2269
 0.2276
 0.2248
 0.2533
 0.1996
 0.2053
 0.1992
 0.2117
 0.2513
 0.2136
 0.2317
[torch.FloatTensor of size 64]
), ('layer1.2.bn2.bias', 
-0.0306
-0.0923
-0.0414
-0.0653
-0.1028
-0.0986
-0.0136
-0.0025
-0.1152
-0.0291
-0.0657
-0.3410
-0.0687
-0.0501
-0.0541
-0.0482
-0.0685
 0.0415
-0.0591
-0.0672
-0.0274
-0.1126
-0.0099
-0.0306
-0.1327
-0.0547
-0.0480
-0.0265
-0.0448
-0.0258
-0.0761
-0.1036
 0.0966
-0.0771
 0.0057
-0.0256
-0.1545
 0.1036
 0.2601
-0.1233
-0.0618
 0.0686
-0.0530
 0.0247
-0.0409
-0.0814
-0.0808
-0.0434
-0.0131
-0.0899
-0.0204
-0.0730
-0.0149
-0.0785
-0.0759
-0.1011
-0.0613
-0.1066
 0.0035
 0.0221
-0.0881
-0.0364
-0.0522
-0.0728
[torch.FloatTensor of size 64]
), ('layer1.2.bn2.running_mean', 
-0.0085
-0.0412
-0.0886
-0.0338
-0.1198
 0.0164
-0.0765
-0.1314
-0.0600
 0.0479
 0.0140
-0.1184
-0.0319
-0.0498
-0.0347
-0.0632
-0.0067
-0.0505
-0.0154
-0.0001
-0.0657
-0.1294
 0.0039
-0.0086
-0.0623
-0.0009
-0.0315
-0.0168
-0.0154
-0.0258
-0.0186
-0.0383
-0.0221
 0.0170
-0.0300
-0.0653
-0.1698
-0.0289
 0.0073
-0.0097
-0.0330
-0.1643
 0.0742
-0.0247
-0.0264
 0.0236
-0.0605
-0.0581
 0.0426
-0.1036
-0.0680
-0.0532
-0.0685
-0.0187
-0.0132
-0.0548
-0.0118
-0.0323
-0.0248
 0.0038
 0.2349
-0.0194
-0.0205
-0.1076
[torch.FloatTensor of size 64]
), ('layer1.2.bn2.running_var', 
1.00000e-02 *
  1.8515
  2.9538
  1.6866
  1.7832
  1.1602
  1.4711
  1.6028
  0.8541
  1.3967
  2.0243
  2.6406
  0.4888
  2.2494
  2.2164
  2.2043
  1.9605
  1.8244
  1.7622
  1.9833
  2.1285
  1.9916
  1.1273
  2.7151
  2.3088
  1.2089
  2.4583
  2.2335
  1.9037
  2.4207
  1.8718
  1.9534
  2.0064
  1.0463
  2.7449
  1.6386
  1.4336
  1.4184
  1.7300
  1.1209
  1.2017
  2.7017
  1.5487
  1.7232
  1.4593
  2.3313
  2.2107
  1.9151
  1.7293
  2.2256
  1.5194
  2.1158
  1.4916
  1.7921
  1.9676
  1.7764
  1.6196
  2.2883
  1.1665
  1.7426
  1.5464
  1.2265
  2.3099
  1.7352
  1.2335
[torch.FloatTensor of size 64]
), ('layer1.2.conv3.weight', 
( 0 , 0 ,.,.) = 
  4.2877e-03

( 0 , 1 ,.,.) = 
 -2.7595e-03

( 0 , 2 ,.,.) = 
 -1.9950e-03
    ... 

( 0 ,61 ,.,.) = 
  2.0631e-03

( 0 ,62 ,.,.) = 
  5.3560e-03

( 0 ,63 ,.,.) = 
  2.3445e-03
      ⋮  

( 1 , 0 ,.,.) = 
  1.1497e-03

( 1 , 1 ,.,.) = 
  3.0026e-03

( 1 , 2 ,.,.) = 
  6.7865e-03
    ... 

( 1 ,61 ,.,.) = 
 -2.5102e-03

( 1 ,62 ,.,.) = 
  8.1134e-03

( 1 ,63 ,.,.) = 
 -6.5756e-03
      ⋮  

( 2 , 0 ,.,.) = 
 -1.9855e-02

( 2 , 1 ,.,.) = 
 -1.0414e-02

( 2 , 2 ,.,.) = 
 -6.7570e-02
    ... 

( 2 ,61 ,.,.) = 
 -2.4544e-02

( 2 ,62 ,.,.) = 
 -5.9988e-03

( 2 ,63 ,.,.) = 
 -1.2558e-01
...     
      ⋮  

(253, 0 ,.,.) = 
 -1.2564e-03

(253, 1 ,.,.) = 
  2.2247e-04

(253, 2 ,.,.) = 
  5.4108e-03
    ... 

(253,61 ,.,.) = 
  6.6900e-04

(253,62 ,.,.) = 
  8.4452e-03

(253,63 ,.,.) = 
  3.9342e-03
      ⋮  

(254, 0 ,.,.) = 
  2.4323e-04

(254, 1 ,.,.) = 
  3.5110e-03

(254, 2 ,.,.) = 
 -5.8671e-03
    ... 

(254,61 ,.,.) = 
 -5.1452e-03

(254,62 ,.,.) = 
 -4.9488e-03

(254,63 ,.,.) = 
 -2.9513e-03
      ⋮  

(255, 0 ,.,.) = 
 -4.7445e-02

(255, 1 ,.,.) = 
 -3.2359e-02

(255, 2 ,.,.) = 
  8.7406e-03
    ... 

(255,61 ,.,.) = 
  9.7492e-03

(255,62 ,.,.) = 
 -2.9358e-02

(255,63 ,.,.) = 
 -3.6021e-02
[torch.FloatTensor of size 256x64x1x1]
), ('layer1.2.bn3.weight', 
-0.0016
 0.0039
 0.2425
 0.0131
 0.2001
-0.0003
 0.1157
 0.0469
-0.0015
-0.0213
-0.0027
 0.1475
 0.2144
-0.0070
 0.1194
 0.1255
 0.0041
 0.0799
-0.0005
-0.0005
 0.1292
 0.0793
 0.1562
 0.1081
 0.0817
 0.2245
 0.0016
 0.0518
-0.0021
-0.0041
 0.0056
 0.0016
-0.0075
 0.0553
 0.1322
-0.0045
 0.0022
-0.0048
-0.0034
 0.0023
 0.3502
 0.0166
 0.1343
 0.0087
-0.0022
 0.2173
 0.0004
 0.1888
 0.2450
 0.1921
-0.0156
 0.0434
 0.2315
 0.1878
 0.0056
 0.0068
 0.0010
 0.1202
 0.0016
 0.0038
 0.0165
 0.0038
-0.0037
 0.0034
 0.3755
 0.0010
 0.2073
 0.0140
 0.0614
 0.0004
 0.1475
 0.2578
 0.0643
-0.0057
-0.0029
 0.0920
 0.1271
 0.0208
-0.0058
 0.0505
 0.0009
-0.0069
 0.0040
 0.2250
 0.1074
 0.1481
 0.1807
 0.2502
-0.0038
 0.2358
-0.0017
 0.0852
 0.0010
 0.2784
 0.1446
 0.0625
 0.0034
 0.1425
 0.0004
 0.0504
 0.0321
 0.2767
 0.1396
 0.0061
 0.0592
 0.0856
 0.1206
 0.2202
 0.0017
-0.0057
-0.0030
 0.0022
 0.0006
 0.0117
 0.0708
 0.0683
 0.0196
 0.1942
 0.1436
-0.0469
 0.0319
-0.0979
 0.0079
 0.3327
-0.0042
 0.0008
 0.1136
 0.3380
 0.1424
 0.0981
 0.1345
 0.0581
 0.1317
 0.2326
 0.1753
-0.0130
 0.1274
 0.1846
 0.0102
 0.1972
 0.1870
-0.0045
 0.0482
 0.1322
-0.0003
 0.0024
 0.0184
 0.0478
 0.0400
 0.1090
 0.0005
 0.1481
 0.1863
 0.1207
 0.0002
 0.1088
 0.0022
 0.1066
 0.0067
-0.0003
 0.0016
 0.0525
 0.0004
 0.0073
 0.1850
 0.0008
 0.1266
 0.0059
 0.0487
 0.2174
 0.0690
 0.0029
 0.1675
 0.2123
 0.1973
 0.1488
 0.0617
 0.0012
 0.1072
 0.1278
 0.2105
 0.0081
 0.0629
 0.0711
-0.0027
 0.1726
 0.0000
 0.0759
 0.0108
 0.0019
 0.0082
-0.0008
 0.0039
-0.0305
 0.1661
-0.0016
 0.0711
 0.0164
-0.0029
 0.1796
 0.0242
 0.0327
 0.2187
 0.0048
 0.1950
 0.0014
 0.1541
 0.1010
 0.0469
 0.0230
 0.1238
 0.2172
 0.1265
 0.0617
 0.0626
 0.0042
 0.1738
 0.1467
 0.0994
 0.0942
-0.0024
 0.1659
 0.1885
 0.0002
 0.0056
 0.0617
-0.0044
 0.0083
 0.0039
 0.0799
 0.2456
-0.0040
 0.0625
 0.0030
-0.0019
 0.0058
 0.1215
-0.0049
 0.0302
 0.0103
 0.0873
 0.0342
-0.0005
-0.0038
 0.0264
-0.0051
 0.2376
 0.1021
 0.1778
 0.1523
-0.0035
 0.0004
-0.0009
-0.0003
-0.0130
 0.1728
[torch.FloatTensor of size 256]
), ('layer1.2.bn3.bias', 
 0.0017
 0.0050
-0.0054
 0.0124
-0.0881
 0.0005
-0.0247
-0.0418
 0.0035
-0.0235
 0.0070
-0.1078
 0.0535
 0.0057
-0.0897
 0.0597
 0.0052
-0.0405
-0.0603
 0.0030
-0.0492
-0.0297
-0.0631
-0.0590
-0.0582
-0.0576
 0.0020
 0.0795
 0.0040
 0.0065
-0.0154
 0.0020
 0.0045
 0.0028
-0.0064
 0.0042
 0.0097
 0.0084
 0.0023
 0.0032
-0.0188
 0.0164
-0.1280
 0.0127
 0.0017
 0.0174
 0.0038
-0.0713
 0.0007
 0.0770
 0.0008
 0.0517
 0.1328
-0.0958
 0.0005
 0.0035
 0.0027
-0.0058
 0.0045
 0.0072
 0.0020
 0.0050
-0.0005
 0.0033
-0.0338
 0.0036
 0.0159
 0.0020
-0.0928
 0.0044
-0.0422
 0.0695
-0.0106
 0.0024
 0.0076
-0.0066
 0.1455
-0.0577
 0.0016
-0.0517
 0.0034
 0.0016
 0.0090
 0.0579
 0.0588
 0.0985
-0.0513
 0.0631
 0.0069
 0.1254
 0.0027
-0.0781
 0.0034
-0.0169
-0.1071
-0.0529
 0.0066
-0.1662
-0.0032
 0.0389
-0.0090
 0.0724
-0.1217
 0.0030
-0.0496
-0.0649
-0.0251
 0.1595
 0.0010
 0.0062
 0.0007
 0.0025
 0.0030
 0.0086
-0.1288
-0.0311
 0.0082
 0.0761
-0.0557
 0.0083
-0.0369
-0.1549
-0.0042
 0.0209
 0.0010
 0.0079
-0.1325
-0.0068
 0.1754
-0.0212
-0.1287
-0.0491
-0.0283
-0.0692
 0.1399
-0.0108
-0.0215
 0.0905
 0.0083
 0.1152
 0.0094
-0.0001
-0.0542
-0.0691
-0.0366
 0.0085
 0.0077
-0.0154
-0.0710
 0.0421
 0.0052
-0.0696
-0.0764
 0.0922
 0.0041
-0.1396
 0.0024
-0.1090
 0.0033
 0.0044
 0.0022
-0.0410
 0.0007
 0.0006
-0.0503
 0.0026
-0.1579
 0.0064
-0.0523
-0.0934
 0.0422
 0.0069
-0.1208
-0.0240
-0.0178
 0.0259
-0.0662
 0.0054
-0.0769
-0.0720
-0.0847
-0.0050
-0.0666
-0.1055
 0.0026
 0.1969
 0.0033
 0.0615
 0.0110
 0.0044
 0.0070
 0.0049
 0.0041
-0.0257
 0.0801
 0.0021
-0.0666
 0.0033
 0.0020
 0.0068
-0.0105
-0.0078
 0.1224
 0.0045
-0.0717
 0.0086
-0.0488
 0.1549
-0.0310
 0.0254
-0.0505
 0.0710
-0.0475
 0.0025
-0.0973
 0.0013
-0.0858
-0.1063
-0.0699
-0.0341
 0.0053
-0.1138
 0.0241
 0.0034
 0.0031
-0.0646
 0.0038
 0.0027
 0.0046
 0.0062
-0.1204
 0.0027
-0.0078
-0.0002
 0.0036
 0.0062
-0.1684
 0.0071
-0.0300
-0.0249
-0.0095
 0.0118
 0.0034
 0.0103
-0.0524
 0.0034
 0.0167
-0.1474
 0.0302
-0.0559
 0.0060
 0.0043
 0.0034
 0.0053
 0.0052
-0.1542
[torch.FloatTensor of size 256]
), ('layer1.2.bn3.running_mean', 
-0.0011
-0.0028
-0.0895
-0.0001
 0.0475
-0.0011
-0.0267
-0.0249
-0.0022
 0.0029
-0.0024
-0.0062
-0.0605
 0.0016
-0.0536
 0.0351
-0.0022
-0.0419
 0.0011
-0.0022
-0.0309
-0.0119
 0.0052
 0.0114
-0.0484
-0.0219
 0.0009
-0.0040
 0.0006
-0.0003
-0.0024
-0.0047
 0.0002
-0.0043
 0.0367
-0.0029
-0.0024
-0.0011
-0.0020
-0.0031
-0.0427
-0.0002
-0.0527
 0.0022
-0.0010
-0.0134
 0.0013
-0.0089
-0.0096
 0.0611
 0.0009
-0.0026
-0.0557
-0.0242
-0.0009
-0.0000
 0.0054
-0.0203
-0.0011
-0.0019
-0.0122
-0.0037
 0.0006
-0.0027
-0.1243
 0.0031
 0.0222
-0.0007
 0.0147
-0.0005
-0.0485
-0.0145
-0.0610
-0.0015
-0.0021
-0.0138
-0.0199
 0.0050
 0.0018
-0.0004
-0.0018
 0.0026
-0.0004
-0.0652
 0.0295
-0.0194
-0.0246
-0.0643
 0.0041
 0.0218
 0.0053
-0.0213
 0.0014
-0.0432
-0.0040
 0.0228
-0.0010
 0.0317
-0.0071
-0.0249
 0.0008
-0.0043
-0.0455
 0.0026
-0.0166
-0.0252
 0.0252
-0.0872
-0.0015
 0.0032
 0.0013
-0.0017
-0.0024
 0.0038
-0.0096
-0.0423
-0.0016
-0.0593
-0.0492
 0.0015
-0.0021
-0.0097
 0.0040
-0.0746
 0.0006
 0.0053
 0.0023
-0.0730
-0.0039
-0.0677
-0.0682
-0.0072
-0.0455
-0.0307
-0.0423
-0.0011
-0.0535
 0.0629
-0.0028
-0.0586
 0.0387
 0.0014
-0.0201
-0.0185
-0.0000
 0.0032
-0.0005
-0.0233
 0.0223
 0.0311
 0.0029
 0.0467
 0.0407
-0.0079
-0.0004
-0.0133
-0.0038
 0.0013
 0.0008
-0.0010
 0.0020
-0.0445
-0.0022
-0.0033
 0.0150
-0.0020
 0.0265
 0.0004
-0.0150
-0.0447
 0.0065
-0.0003
 0.0177
 0.0112
 0.0111
-0.0330
-0.0368
 0.0020
-0.0155
-0.0228
-0.0563
 0.0006
-0.0332
-0.0003
-0.0001
-0.0043
-0.0030
 0.0144
-0.0044
-0.0024
-0.0027
-0.0023
 0.0010
 0.0138
 0.0008
-0.0013
-0.0228
-0.0069
 0.0016
 0.0289
-0.0017
-0.0094
-0.0359
-0.0012
 0.0247
 0.0011
-0.0152
-0.0088
-0.0011
-0.0122
-0.0188
-0.0718
-0.0059
-0.0096
-0.0118
 0.0010
-0.0116
-0.0427
 0.0031
-0.0332
 0.0002
-0.0197
 0.0271
 0.0016
 0.0005
-0.0077
-0.0002
-0.0013
-0.0037
-0.0273
 0.0177
 0.0019
-0.0574
-0.0009
-0.0019
 0.0023
-0.0220
-0.0085
-0.0180
 0.0018
-0.0016
 0.0021
 0.0018
-0.0003
-0.0124
 0.0019
-0.0050
-0.0145
-0.0019
 0.0036
-0.0010
-0.0001
 0.0046
-0.0032
 0.0014
-0.0626
[torch.FloatTensor of size 256]
), ('layer1.2.bn3.running_var', 
1.00000e-03 *
  0.2252
  0.6165
  2.6186
  0.1991
  1.8141
  0.1579
  2.1673
  1.1420
  0.2450
  0.2342
  0.2464
  1.3492
  3.8478
  0.3246
  2.6012
  1.7945
  0.1334
  1.8548
  0.0628
  0.3120
  1.3117
  0.8111
  1.6412
  0.9768
  2.0576
  1.6359
  0.1495
  0.7227
  0.1860
  0.1245
  0.1359
  0.2047
  0.1829
  0.4839
  1.4273
  0.1157
  0.1661
  0.1398
  0.5399
  0.2826
  6.0868
  0.3669
  1.8738
  0.1999
  0.0665
  2.1218
  0.0938
  1.8074
  2.7463
  3.4119
  0.2241
  0.6421
  2.8110
  1.4750
  0.1220
  0.1586
  0.1661
  1.4437
  0.1307
  0.1112
  0.1677
  0.1520
  0.1117
  0.0657
  4.7605
  0.4507
  2.7628
  0.1857
  0.2896
  0.1376
  1.2723
  4.4802
  1.2122
  0.4796
  0.1829
  0.7407
  1.9709
  0.1345
  0.1007
  0.3201
  0.1243
  0.1950
  0.1508
  3.6038
  1.2361
  3.3301
  2.0488
  2.5669
  0.2429
  3.5552
  0.7978
  0.3887
  0.2193
  3.3733
  1.4910
  0.5161
  0.1781
  1.0655
  0.0783
  0.8146
  0.3761
  3.9936
  2.4538
  0.1081
  0.8957
  1.4063
  1.4270
  2.8045
  0.1254
  0.1898
  0.3727
  0.0803
  0.2306
  0.2045
  0.6007
  1.1630
  0.2488
  1.6436
  3.6781
  0.6575
  0.4476
  0.5384
  0.1419
  5.3325
  0.1548
  0.1105
  0.9438
  5.2441
  2.3804
  2.0918
  3.8741
  0.6714
  2.2407
  2.3902
  2.1607
  0.1019
  1.1760
  2.2630
  0.1534
  1.8504
  1.7240
  0.1226
  0.5045
  1.3079
  0.0898
  0.1308
  0.2230
  0.5753
  0.5020
  0.8743
  0.1969
  1.6204
  1.8693
  1.4057
  0.0585
  0.9030
  0.0970
  1.0199
  0.1339
  0.1315
  0.2066
  1.4175
  0.0742
  0.5807
  1.3813
  0.0789
  0.7939
  0.2039
  1.1737
  3.2203
  1.4402
  0.0808
  1.1164
  1.7353
  1.4476
  1.9831
  0.6932
  0.2336
  1.0503
  1.5134
  2.2918
  0.0989
  0.6077
  0.9605
  0.1780
  2.5936
  0.1268
  1.4985
  0.1251
  0.8411
  0.2399
  0.1838
  0.4760
  0.2190
  2.2870
  0.1200
  1.1028
  0.3520
  0.0947
  1.9992
  0.3155
  0.6351
  2.4682
  0.1813
  2.0843
  0.1329
  1.4374
  1.4189
  0.3883
  0.5039
  1.4408
  2.6063
  0.9066
  0.7701
  0.2542
  0.1439
  1.2304
  1.3796
  1.2840
  1.2608
  1.4531
  1.7732
  2.0216
  0.4319
  0.0568
  0.7815
  0.4714
  0.0830
  0.3537
  1.2508
  2.7990
  0.2267
  0.8405
  0.1705
  0.1019
  0.1086
  0.8564
  1.9216
  0.2558
  0.0557
  0.5833
  0.1389
  0.4578
  0.1657
  0.3431
  0.1086
  2.9301
  0.8653
  1.8098
  1.6505
  0.1041
  0.5499
  0.3143
  0.2987
  0.3482
  1.6342
[torch.FloatTensor of size 256]
), ('layer2.0.conv1.weight', 
( 0 , 0 ,.,.) = 
  1.0023e-02

( 0 , 1 ,.,.) = 
  1.5478e-02

( 0 , 2 ,.,.) = 
  2.5978e-02
    ... 

( 0 ,253,.,.) = 
  1.2872e-02

( 0 ,254,.,.) = 
 -3.1680e-02

( 0 ,255,.,.) = 
  5.8940e-03
      ⋮  

( 1 , 0 ,.,.) = 
  6.2445e-03

( 1 , 1 ,.,.) = 
  7.5096e-03

( 1 , 2 ,.,.) = 
 -2.5023e-02
    ... 

( 1 ,253,.,.) = 
  7.0060e-03

( 1 ,254,.,.) = 
 -1.8096e-02

( 1 ,255,.,.) = 
 -6.7320e-02
      ⋮  

( 2 , 0 ,.,.) = 
  2.6610e-02

( 2 , 1 ,.,.) = 
  1.8790e-03

( 2 , 2 ,.,.) = 
 -6.9333e-02
    ... 

( 2 ,253,.,.) = 
  9.8718e-03

( 2 ,254,.,.) = 
 -1.8730e-02

( 2 ,255,.,.) = 
 -3.0576e-03
...     
      ⋮  

(125, 0 ,.,.) = 
 -3.4748e-03

(125, 1 ,.,.) = 
  3.9990e-03

(125, 2 ,.,.) = 
 -8.3690e-04
    ... 

(125,253,.,.) = 
  8.5643e-03

(125,254,.,.) = 
  1.6422e-02

(125,255,.,.) = 
  4.1199e-03
      ⋮  

(126, 0 ,.,.) = 
  4.9666e-04

(126, 1 ,.,.) = 
  2.2655e-03

(126, 2 ,.,.) = 
  1.2856e-01
    ... 

(126,253,.,.) = 
 -3.1404e-03

(126,254,.,.) = 
 -7.7637e-03

(126,255,.,.) = 
  4.9525e-02
      ⋮  

(127, 0 ,.,.) = 
  1.0553e-02

(127, 1 ,.,.) = 
  8.9444e-04

(127, 2 ,.,.) = 
 -1.3065e-02
    ... 

(127,253,.,.) = 
  5.8324e-03

(127,254,.,.) = 
  2.9158e-02

(127,255,.,.) = 
 -4.8332e-02
[torch.FloatTensor of size 128x256x1x1]
), ('layer2.0.bn1.weight', 
 0.2639
 0.1874
 0.2028
 0.2016
 0.2028
 0.2079
 0.2166
 0.1769
 0.1823
 0.1636
 0.2489
 0.1522
 0.1968
 0.2547
 0.1898
 0.2004
 0.2001
 0.1535
 0.2109
 0.1407
 0.2053
 0.1700
 0.2409
 0.1672
 0.2550
 0.1277
 0.2347
 0.1519
 0.3125
 0.2502
 0.1831
 0.1560
 0.1685
 0.1832
 0.2342
 0.1888
 0.1630
 0.2165
 0.3512
 0.1941
 0.2384
 0.1560
 0.2359
 0.1366
 0.1788
 0.2182
 0.1845
 0.1800
 0.1408
 0.2376
 0.2027
 0.2410
 0.2098
 0.2116
 0.1076
 0.1869
 0.2270
 0.2286
 0.1911
 0.1916
 0.2162
 0.2079
 0.2235
 0.2123
 0.2124
 0.2164
 0.2602
 0.1321
 0.1902
 0.1735
 0.2416
 0.1990
 0.2038
 0.2215
 0.1764
 0.2177
 0.2127
 0.2317
 0.2532
 0.1651
 0.1626
 0.1963
 0.2081
 0.1926
 0.2130
 0.2600
 0.1959
 0.1637
 0.2128
 0.3089
 0.1876
 0.1892
 0.1551
 0.2224
 0.1925
 0.2220
 0.2031
 0.2480
 0.2083
 0.2479
 0.1450
 0.2119
 0.1960
 0.2159
 0.2165
 0.1843
 0.2348
 0.1779
 0.2566
 0.2222
 0.2028
 0.1639
 0.2372
 0.1999
 0.2228
 0.2090
 0.2407
 0.2054
 0.2224
 0.2153
 0.2855
 0.2180
 0.1861
 0.1533
 0.2073
 0.2611
 0.1852
 0.2868
[torch.FloatTensor of size 128]
), ('layer2.0.bn1.bias', 
-0.1599
-0.0919
-0.1868
-0.0968
-0.0293
-0.1193
-0.0766
-0.1058
-0.0625
 0.0841
-0.1555
 0.0708
-0.2071
-0.1594
-0.1094
-0.0251
-0.0778
 0.0911
-0.0694
 0.0664
-0.0250
 0.0363
-0.1849
 0.0753
-0.1483
 0.1161
-0.1218
 0.1143
-0.1213
-0.1672
 0.0653
 0.0205
-0.0308
 0.0154
-0.1299
 0.0465
 0.0827
-0.0653
-0.1969
 0.0921
-0.0810
 0.0852
-0.1423
 0.0951
 0.0946
-0.1390
 0.0414
 0.0242
 0.0930
-0.1663
-0.0784
-0.0960
-0.1124
-0.1654
-0.1033
-0.0265
-0.0821
-0.0763
-0.0890
-0.0049
-0.0760
-0.1013
-0.1025
-0.1701
-0.1045
-0.0820
-0.2578
 0.0469
-0.1343
 0.0628
-0.1718
 0.0618
-0.0345
-0.0653
 0.0661
-0.0572
-0.0401
-0.1043
-0.1747
 0.0528
 0.0223
-0.0988
-0.1399
-0.0677
-0.0476
-0.1639
-0.1153
 0.0008
-0.1069
-0.2375
-0.0232
-0.0219
-0.1606
 0.0245
 0.0211
-0.1168
-0.1102
-0.1294
-0.0866
-0.2173
 0.0296
-0.0928
-0.0338
-0.0180
-0.1379
 0.0014
-0.0956
 0.1095
-0.2016
-0.1424
-0.1554
 0.0160
-0.1674
-0.0748
-0.1505
-0.0514
-0.1245
-0.1391
-0.0912
-0.1194
-0.3035
-0.2166
-0.1192
 0.0447
-0.1237
-0.1406
 0.0175
-0.2559
[torch.FloatTensor of size 128]
), ('layer2.0.bn1.running_mean', 
-0.0819
-0.0287
-0.1639
-0.0489
 0.1545
-0.0141
-0.2023
-0.2106
 0.0381
 0.1541
-0.0458
-0.0861
 0.0335
-0.0323
-0.2135
-0.1149
-0.1638
-0.0846
-0.1560
 0.0527
-0.2503
-0.0173
 0.0857
 0.0912
 0.3618
-0.2487
 0.0676
 0.0102
-0.0271
 0.1022
 0.1529
-0.1878
-0.0824
 0.1214
-0.1132
-0.1178
-0.0284
-0.0690
-0.0520
 0.0817
-0.1547
-0.0078
-0.1839
-0.0050
 0.1416
-0.1247
-0.0138
-0.0330
-0.0275
 0.1481
-0.1651
 0.0331
-0.3055
 0.0854
 0.1463
-0.1056
-0.3797
-0.0393
-0.1407
-0.1924
-0.0624
-0.1147
 0.0743
-0.2937
-0.1366
-0.1683
-0.2852
-0.0170
-0.0667
 0.0042
-0.0822
 0.0928
 0.0396
-0.1499
 0.0817
-0.0799
-0.2501
-0.1346
 0.1491
-0.0689
 0.0163
-0.3012
-0.0861
-0.1969
 0.0200
 0.0470
-0.1722
-0.0605
-0.2578
-0.0270
-0.1938
 0.0381
 0.1164
 0.0888
-0.0691
-0.1549
-0.2395
-0.0928
 0.0092
-0.3129
-0.0425
-0.0905
-0.1890
-0.1828
-0.4003
 0.1689
-0.0966
 0.0295
-0.2171
-0.1990
-0.0821
-0.0315
-0.0246
-0.3625
-0.2200
-0.1320
-0.1322
-0.1735
-0.0782
-0.0746
-0.3356
-0.1897
-0.1163
 0.0567
-0.2372
-0.0632
 0.0162
-0.0258
[torch.FloatTensor of size 128]
), ('layer2.0.bn1.running_var', 
 0.0343
 0.0157
 0.0134
 0.0182
 0.0364
 0.0131
 0.0236
 0.0099
 0.0159
 0.0246
 0.0208
 0.0141
 0.0057
 0.0224
 0.0148
 0.0296
 0.0185
 0.0232
 0.0258
 0.0129
 0.0334
 0.0200
 0.0197
 0.0242
 0.0247
 0.0185
 0.0268
 0.0279
 0.1562
 0.0180
 0.0245
 0.0177
 0.0201
 0.0363
 0.0184
 0.0310
 0.0267
 0.0323
 0.1392
 0.0407
 0.0323
 0.0250
 0.0251
 0.0153
 0.0339
 0.0135
 0.0178
 0.0206
 0.0173
 0.0203
 0.0174
 0.0271
 0.0162
 0.0072
 0.0034
 0.0254
 0.0318
 0.0435
 0.0266
 0.0411
 0.0289
 0.0189
 0.0275
 0.0132
 0.0270
 0.0272
 0.0253
 0.0118
 0.0081
 0.0217
 0.0186
 0.0419
 0.0341
 0.0295
 0.0223
 0.0255
 0.0197
 0.0216
 0.0222
 0.0163
 0.0203
 0.0234
 0.0189
 0.0252
 0.0202
 0.0266
 0.0188
 0.0145
 0.0246
 0.0147
 0.0118
 0.0304
 0.0056
 0.0403
 0.0376
 0.0270
 0.0209
 0.0189
 0.0160
 0.0198
 0.0182
 0.0194
 0.0224
 0.0457
 0.0226
 0.0358
 0.0343
 0.0359
 0.0223
 0.0173
 0.0126
 0.0230
 0.0196
 0.0187
 0.0231
 0.0298
 0.0329
 0.0162
 0.0227
 0.0201
 0.0280
 0.0142
 0.0098
 0.0132
 0.0254
 0.0265
 0.0314
 0.0280
[torch.FloatTensor of size 128]
), ('layer2.0.conv2.weight', 
( 0 , 0 ,.,.) = 
  5.7148e-03  5.4153e-03  6.9670e-03
 -1.4907e-03  8.5546e-03  1.2571e-02
  5.9339e-03  1.9393e-02  2.3238e-02

( 0 , 1 ,.,.) = 
 -5.9089e-03 -2.7058e-02 -2.0096e-02
  8.4784e-03  1.2187e-02  1.0419e-02
  1.4465e-02  2.3546e-02  3.6228e-02

( 0 , 2 ,.,.) = 
  6.5031e-03 -1.2364e-02  3.6871e-03
 -4.0839e-03 -7.5281e-03 -3.6343e-03
  1.2317e-02  4.9689e-03  4.7140e-03
    ... 

( 0 ,125,.,.) = 
 -8.9779e-03 -9.8717e-03 -1.4437e-02
 -9.0001e-03  1.9672e-03 -3.5889e-03
 -3.8357e-03 -1.4406e-02  1.8059e-02

( 0 ,126,.,.) = 
 -4.6118e-02 -2.8350e-02 -1.6736e-02
 -2.8602e-02 -4.6270e-03 -1.8925e-02
 -2.6982e-02  2.7646e-04  2.4983e-02

( 0 ,127,.,.) = 
 -8.5529e-04 -1.3946e-03 -1.4319e-02
  1.1076e-02  1.8380e-03 -1.5675e-02
 -5.8583e-03 -2.3064e-02 -1.7122e-02
      ⋮  

( 1 , 0 ,.,.) = 
  3.1264e-02  1.4076e-02  1.3915e-02
  1.7699e-02  2.3030e-02  3.1258e-02
  1.2741e-02  1.8671e-02  6.9519e-03

( 1 , 1 ,.,.) = 
  2.0218e-03  9.5968e-03 -9.3861e-03
 -1.0374e-02 -9.1659e-04 -6.3050e-03
  4.4324e-03  6.6241e-03 -1.7597e-02

( 1 , 2 ,.,.) = 
 -6.2299e-03 -2.0637e-02  2.5680e-02
 -1.3865e-02 -2.1147e-03  1.1478e-02
 -2.1541e-02 -6.0607e-03  1.5434e-02
    ... 

( 1 ,125,.,.) = 
 -1.4400e-03 -2.9872e-02 -2.6718e-02
 -2.5183e-02 -2.0159e-02 -2.6662e-02
 -2.2512e-02 -2.2985e-02 -1.4249e-02

( 1 ,126,.,.) = 
  2.6493e-02  3.5156e-02  2.7891e-02
  1.8545e-02 -3.7416e-03  2.5659e-02
  3.0646e-02  4.1930e-02  3.6475e-02

( 1 ,127,.,.) = 
  1.8742e-02  2.4336e-02  8.5196e-04
  2.9210e-02  3.9976e-02  2.5880e-02
  1.7010e-02  3.6066e-02  1.3382e-02
      ⋮  

( 2 , 0 ,.,.) = 
  1.2474e-02 -1.8047e-02 -4.6260e-02
  2.9568e-02  3.4341e-03 -2.9960e-02
  1.6432e-02 -8.8087e-03 -2.7499e-02

( 2 , 1 ,.,.) = 
 -8.4319e-03 -9.7227e-03  1.6514e-02
 -1.3975e-03  4.4135e-03  1.8744e-03
  3.7071e-04 -9.3388e-03  1.8634e-03

( 2 , 2 ,.,.) = 
 -2.6396e-02  1.1149e-02  3.6830e-02
 -2.0837e-02  4.7213e-03  3.5836e-02
 -3.2991e-02 -1.6638e-02  1.7730e-02
    ... 

( 2 ,125,.,.) = 
 -1.5793e-02 -2.7924e-03  1.0733e-02
  1.4940e-02 -1.6132e-03  1.6853e-03
  1.0718e-03 -1.7483e-02  3.1942e-03

( 2 ,126,.,.) = 
  4.7515e-02  4.0208e-02  1.0953e-02
  2.3981e-02  3.2340e-03  7.9321e-04
  1.2609e-02  3.7900e-02 -1.8043e-03

( 2 ,127,.,.) = 
 -1.1543e-02 -1.8122e-02 -3.7408e-02
 -2.3050e-03 -1.0188e-02 -4.5527e-02
 -1.3501e-02 -2.7866e-02 -4.3160e-02
...     
      ⋮  

(125, 0 ,.,.) = 
 -8.0249e-03 -2.4573e-02 -2.3936e-02
 -2.5406e-02 -1.1022e-02 -4.0906e-02
 -1.5074e-02 -2.1930e-02 -4.7509e-02

(125, 1 ,.,.) = 
 -3.8590e-03 -8.7336e-04 -7.7881e-03
 -4.3808e-03  1.8722e-03 -1.3124e-02
 -2.7823e-03 -1.1781e-02 -1.0455e-02

(125, 2 ,.,.) = 
 -1.8092e-02 -1.3784e-02  1.5378e-02
 -2.8538e-02 -1.7802e-02  2.6612e-02
  1.4466e-02  1.9868e-02  6.0853e-02
    ... 

(125,125,.,.) = 
 -1.3183e-02  3.3895e-04  4.1337e-03
  1.2881e-03 -1.0550e-02  1.2804e-04
  3.4866e-03 -1.1056e-02 -1.6554e-02

(125,126,.,.) = 
 -1.3392e-02  2.8013e-02  1.2245e-02
 -1.9193e-02  9.0613e-03  3.3513e-03
 -3.5666e-02 -1.0572e-02 -1.8500e-02

(125,127,.,.) = 
 -6.6079e-03 -1.2106e-03 -2.2793e-02
 -9.2909e-03 -7.9433e-03 -1.9724e-02
  1.5923e-03 -9.6041e-04 -1.7361e-02
      ⋮  

(126, 0 ,.,.) = 
  1.1845e-02  8.5594e-03 -1.7909e-02
  8.7121e-03 -3.6880e-04  1.4275e-03
 -1.4048e-02 -8.8601e-05  1.3824e-02

(126, 1 ,.,.) = 
 -4.2661e-03 -9.6938e-03  3.0361e-02
 -1.6280e-02 -1.1626e-03  1.6717e-02
 -1.3279e-02  3.4856e-03 -1.8792e-02

(126, 2 ,.,.) = 
 -4.2527e-03  1.2455e-02 -1.4695e-03
  4.5873e-03  3.0026e-03 -4.5780e-03
  8.0806e-03  5.6422e-03 -5.4317e-03
    ... 

(126,125,.,.) = 
  2.3770e-03 -8.0848e-04  1.3950e-02
  6.4999e-03  8.1204e-04 -8.5034e-03
 -5.5089e-03  2.6967e-02 -2.2525e-02

(126,126,.,.) = 
  3.9749e-02 -6.4180e-04 -2.3314e-02
  1.1120e-02 -1.9094e-03 -2.6408e-02
  2.1081e-02 -2.3960e-02  1.6082e-02

(126,127,.,.) = 
 -1.9112e-02 -2.0600e-02 -8.8751e-03
 -2.0385e-02 -2.6840e-02  6.0018e-03
 -1.9251e-02 -2.0094e-02 -3.1182e-03
      ⋮  

(127, 0 ,.,.) = 
 -1.3227e-02 -2.3923e-02  4.6463e-03
 -1.7025e-02 -2.8171e-02 -1.7361e-02
 -3.2787e-02 -4.1742e-02 -2.6234e-02

(127, 1 ,.,.) = 
  1.5639e-02  2.0191e-02  1.4607e-02
  3.0779e-02  1.9010e-02  2.9254e-02
 -9.7529e-03  1.3953e-02  8.8613e-03

(127, 2 ,.,.) = 
 -3.7030e-03 -2.7738e-03 -2.1177e-03
  1.5293e-02 -3.9039e-03 -1.7507e-02
  6.5572e-03  3.6534e-02  2.9290e-02
    ... 

(127,125,.,.) = 
  2.8785e-02 -6.0905e-03 -1.0742e-02
  1.4855e-02  2.5195e-02 -5.5631e-03
 -2.0345e-02 -3.4998e-03 -8.5412e-03

(127,126,.,.) = 
 -4.8109e-03  1.5368e-02  2.5238e-02
 -2.4375e-02  1.7427e-02 -2.7583e-03
  9.9079e-03 -3.3578e-02 -3.2155e-02

(127,127,.,.) = 
 -1.0389e-02 -8.7084e-03 -7.3160e-03
  1.3486e-02 -4.3148e-03 -8.4715e-03
  5.3262e-03 -5.1335e-03 -5.3968e-03
[torch.FloatTensor of size 128x128x3x3]
), ('layer2.0.bn2.weight', 
 0.2249
 0.1925
 0.1457
 0.2466
 0.2248
 0.2264
 0.1834
 0.1699
 0.1916
 0.2087
 0.2821
 0.1886
 0.1893
 0.2142
 0.1779
 0.1907
 0.1472
 0.2304
 0.2075
 0.2326
 0.2907
 0.2000
 0.1991
 0.1983
 0.1849
 0.1865
 0.1913
 0.2084
 0.2242
 0.1907
 0.2041
 0.1485
 0.2168
 0.2858
 0.2051
 0.2691
 0.2394
 0.2259
 0.1847
 0.1548
 0.2462
 0.2430
 0.2605
 0.1886
 0.1899
 0.1853
 0.1896
 0.2061
 0.1745
 0.2505
 0.1547
 0.1989
 0.1629
 0.2077
 0.2306
 0.1965
 0.1766
 0.2596
 0.2209
 0.1635
 0.2007
 0.2241
 0.1531
 0.2027
 0.1780
 0.1609
 0.2262
 0.1849
 0.1500
 0.2041
 0.1681
 0.1448
 0.2372
 0.2724
 0.2123
 0.1837
 0.1885
 0.2116
 0.1979
 0.1482
 0.2075
 0.1950
 0.1618
 0.2237
 0.2049
 0.1529
 0.2331
 0.2129
 0.2181
 0.1866
 0.2668
 0.1958
 0.2223
 0.1585
 0.1602
 0.1935
 0.2472
 0.2191
 0.1996
 0.1802
 0.2226
 0.2441
 0.1486
 0.1764
 0.1764
 0.1997
 0.2210
 0.2333
 0.1666
 0.2179
 0.1985
 0.1715
 0.1806
 0.2031
 0.2108
 0.2498
 0.1628
 0.2023
 0.1595
 0.2039
 0.2070
 0.2019
 0.2314
 0.1813
 0.1791
 0.1618
 0.2178
 0.1765
[torch.FloatTensor of size 128]
), ('layer2.0.bn2.bias', 
-0.0598
 0.1500
 0.1551
-0.0595
-0.0690
-0.1054
 0.0655
 0.0134
-0.0076
-0.0359
-0.0323
 0.0047
-0.0527
 0.0220
-0.0195
 0.1606
 0.0690
-0.0332
-0.0221
-0.0216
-0.0347
 0.0795
-0.0550
 0.0044
 0.0412
-0.0321
-0.0166
-0.0164
-0.0345
-0.0317
-0.0186
 0.1326
-0.0612
-0.0387
 0.0007
-0.1574
 0.0054
-0.0069
-0.0222
 0.1729
-0.0216
-0.0909
-0.1080
-0.0130
 0.0267
 0.1133
-0.0435
-0.0617
 0.1678
-0.0375
 0.1510
-0.0130
 0.2467
-0.0718
-0.0950
 0.0546
 0.1932
-0.0419
 0.0211
 0.0493
-0.0233
-0.0336
 0.2257
 0.0171
 0.0563
 0.0568
-0.0530
-0.0064
 0.2224
-0.0075
 0.2084
 0.1459
-0.0542
-0.1133
-0.0431
 0.0079
 0.0875
-0.0350
-0.0037
 0.2025
-0.0222
-0.0094
 0.1412
-0.0589
-0.0062
 0.1147
-0.0543
-0.0114
-0.0473
 0.0007
-0.3161
 0.0604
 0.0197
 0.0256
 0.0456
 0.1723
-0.0516
 0.0427
 0.0975
-0.0074
-0.0679
-0.1042
 0.2585
-0.0494
 0.1126
 0.0128
-0.0250
-0.0299
 0.2048
-0.0451
-0.0578
 0.0532
 0.0499
 0.0046
-0.0285
-0.1081
 0.1042
 0.0073
 0.1768
 0.0838
 0.0246
 0.1291
-0.0364
 0.0228
 0.0882
 0.1532
-0.0570
 0.0508
[torch.FloatTensor of size 128]
), ('layer2.0.bn2.running_mean', 
-0.1366
-0.1625
 0.0117
-0.0998
-0.0597
 0.0467
-0.0030
-0.0940
-0.1024
-0.0355
-0.2674
-0.0417
-0.0732
-0.0382
-0.2125
-0.1561
-0.1190
-0.1049
 0.0154
 0.0110
-0.1779
-0.0155
-0.0390
-0.1649
-0.0382
 0.0172
-0.0291
-0.0192
-0.0731
-0.1195
-0.0048
 0.0040
-0.0486
-0.2059
-0.0678
-0.2585
 0.0004
-0.1168
-0.0516
-0.0797
-0.0841
-0.1402
-0.0829
 0.4391
-0.0764
 0.0495
-0.0155
-0.0878
-0.0545
-0.1655
-0.0004
-0.0368
 0.0427
-0.2157
-0.1725
-0.0426
 0.1378
-0.1571
 0.0106
-0.1384
-0.1108
-0.1267
 0.0340
-0.0115
-0.0542
-0.0430
 0.0791
 0.0009
-0.0311
-0.1066
 0.1033
 0.0404
-0.0699
-0.0409
-0.0019
-0.0180
-0.0086
-0.0604
-0.0440
 0.0245
-0.0593
 0.0141
-0.0371
-0.1301
-0.0715
 0.0458
-0.1313
 0.0201
-0.0583
 0.0651
-0.2321
-0.0198
-0.0690
-0.0597
-0.0023
-0.0969
 0.0058
-0.0148
 0.0393
 0.2001
-0.0671
-0.1146
-0.0062
-0.0855
-0.0607
-0.0068
-0.0631
-0.0484
 0.1064
-0.0416
-0.0402
-0.0416
-0.0422
-0.1933
-0.1654
-0.1989
-0.0696
-0.0859
 0.0367
-0.0305
-0.0026
 0.0625
-0.1346
-0.0831
-0.0057
-0.0390
 0.0235
-0.0482
[torch.FloatTensor of size 128]
), ('layer2.0.bn2.running_var', 
1.00000e-02 *
  3.0922
  8.0851
  1.7685
  3.2034
  2.7435
  2.1242
  3.5153
  1.8990
  2.2979
  2.3559
  7.7437
  2.7198
  1.5399
  3.0342
  1.8862
  8.7311
  1.4778
  3.1794
  2.1714
  3.7900
  3.2324
  2.4815
  2.0308
  4.5147
  3.7930
  1.9179
  2.4533
  1.9586
  3.1076
  2.4645
  2.2951
  3.4481
  2.7454
  7.5607
  3.0156
  3.1729
  3.0495
  3.1260
  2.0314
  3.7248
  7.2897
  2.8450
  3.1505
  2.6889
  2.7900
  3.5811
  2.0128
  1.9155
  2.8878
  6.3504
  1.7782
  2.0370
  3.3497
  2.9305
  3.1954
  2.8598
  3.8002
  3.9443
  3.1393
  2.7758
  1.8317
  3.8750
  2.5350
  2.0269
  3.0170
  1.8639
  2.3550
  2.0761
  2.5009
  2.1102
  4.3803
  2.8424
  3.4402
  2.0480
  2.1440
  2.2239
  3.1410
  1.9705
  2.2479
  2.6581
  2.0353
  2.2285
  2.8551
  2.8091
  2.7441
  2.4525
  3.4837
  2.6229
  2.7379
  2.4999
  1.3023
  5.2940
  3.0318
  1.7965
  1.7689
  7.8148
  3.7878
  2.7342
  3.1348
  3.4557
  1.9973
  3.4938
  2.6314
  1.7502
  2.8154
  2.2410
  2.1229
  3.5575
  2.9837
  1.7750
  2.0868
  3.0254
  2.4270
  3.7455
  2.0348
  4.1354
  1.7473
  2.3605
  2.5027
  5.5170
  2.9298
  4.1172
  3.1011
  2.2178
  2.4700
  2.4359
  2.2484
  2.8599
[torch.FloatTensor of size 128]
), ('layer2.0.conv3.weight', 
( 0 , 0 ,.,.) = 
  7.6208e-03

( 0 , 1 ,.,.) = 
 -6.1014e-03

( 0 , 2 ,.,.) = 
  1.3127e-02
    ... 

( 0 ,125,.,.) = 
  1.0091e-02

( 0 ,126,.,.) = 
 -1.3378e-02

( 0 ,127,.,.) = 
 -6.2687e-03
      ⋮  

( 1 , 0 ,.,.) = 
 -2.7508e-02

( 1 , 1 ,.,.) = 
 -4.3027e-03

( 1 , 2 ,.,.) = 
  2.5836e-02
    ... 

( 1 ,125,.,.) = 
  2.7279e-03

( 1 ,126,.,.) = 
 -9.3313e-03

( 1 ,127,.,.) = 
  6.1833e-02
      ⋮  

( 2 , 0 ,.,.) = 
 -1.7879e-03

( 2 , 1 ,.,.) = 
 -9.0075e-04

( 2 , 2 ,.,.) = 
 -1.7440e-04
    ... 

( 2 ,125,.,.) = 
 -1.9249e-04

( 2 ,126,.,.) = 
 -3.8543e-06

( 2 ,127,.,.) = 
 -3.3952e-04
...     
      ⋮  

(509, 0 ,.,.) = 
  1.1697e-07

(509, 1 ,.,.) = 
  3.4833e-07

(509, 2 ,.,.) = 
 -1.7422e-07
    ... 

(509,125,.,.) = 
 -1.8790e-07

(509,126,.,.) = 
 -8.5461e-08

(509,127,.,.) = 
  2.0189e-07
      ⋮  

(510, 0 ,.,.) = 
  4.2819e-02

(510, 1 ,.,.) = 
 -1.2952e-01

(510, 2 ,.,.) = 
 -5.2879e-02
    ... 

(510,125,.,.) = 
  1.2145e-01

(510,126,.,.) = 
  4.3815e-02

(510,127,.,.) = 
 -1.2587e-02
      ⋮  

(511, 0 ,.,.) = 
  5.9202e-04

(511, 1 ,.,.) = 
 -1.4324e-02

(511, 2 ,.,.) = 
  7.7022e-03
    ... 

(511,125,.,.) = 
  1.7542e-02

(511,126,.,.) = 
  6.5371e-03

(511,127,.,.) = 
  8.5100e-03
[torch.FloatTensor of size 512x128x1x1]
), ('layer2.0.bn3.weight', 
 3.8543e-03
 2.0187e-01
-1.0649e-03
 1.3887e-01
 6.4771e-03
-2.0702e-06
 2.0280e-01
 1.1607e-02
-4.1076e-03
 2.7771e-02
 8.9216e-02
 1.0155e-01
 1.0902e-01
 3.9999e-02
 4.5550e-06
 1.2153e-01
 2.9691e-01
-1.7842e-02
 1.3224e-01
 9.2594e-02
 8.6137e-02
 2.3588e-03
 2.1034e-01
 1.5372e-07
 1.6275e-01
-1.0454e-02
 1.1453e-01
 1.5497e-01
 6.6282e-06
 1.6580e-01
 3.1317e-01
-2.4331e-03
 5.2280e-02
 1.5141e-01
 8.9488e-02
 1.9418e-01
 7.4374e-02
 1.9285e-01
 2.3015e-03
 2.2057e-01
 2.4289e-01
 1.8490e-01
 6.0817e-02
 2.4235e-03
-7.2726e-03
 2.1065e-01
 1.1995e-02
 1.9941e-01
 1.5138e-01
 2.0406e-01
-3.2079e-03
 1.2165e-04
 6.7944e-02
 9.9864e-02
 2.7700e-03
 2.0632e-01
 4.6338e-02
 8.8209e-02
 2.0853e-01
-7.7076e-08
-1.2983e-03
 1.4655e-01
 1.3156e-01
 6.1504e-03
 1.5142e-01
 1.4627e-01
 6.0212e-02
 1.8846e-01
-1.0229e-06
 2.2133e-01
 2.3804e-01
 1.2794e-01
 2.4388e-01
 1.4702e-01
 3.1081e-01
 9.2273e-03
 2.3980e-01
-8.6978e-03
 1.7617e-01
 1.8940e-01
 2.2123e-01
 1.2454e-01
-1.1824e-02
 2.1235e-01
 1.0170e-01
 1.9985e-01
 1.7297e-01
 9.3104e-04
-5.2291e-04
 2.3814e-06
 6.4092e-07
 2.2321e-01
 1.6644e-01
 1.3561e-02
 2.6848e-08
 1.7824e-01
 1.9010e-01
-3.5674e-02
 1.6651e-01
-9.1091e-04
 1.2540e-01
 1.7982e-01
 9.7606e-03
 1.9054e-01
 4.5191e-03
-1.9165e-04
 1.6682e-01
 1.8502e-01
 1.4833e-01
 4.4347e-02
 1.1528e-01
 1.4835e-01
-1.0824e-07
 1.6343e-01
 1.0696e-02
 1.2262e-01
 1.8079e-01
 1.5248e-01
 1.8994e-01
 6.9889e-02
 1.0647e-01
 1.2310e-01
 6.5127e-02
 1.1183e-01
 1.0039e-01
-2.6980e-03
 2.3881e-01
 5.7536e-02
 6.6677e-02
 1.5345e-01
 1.6415e-01
 1.4292e-01
 1.0783e-01
 1.7668e-06
 9.5887e-02
 7.9922e-02
 1.4580e-01
 1.8263e-01
 4.5771e-07
 1.7064e-01
 1.1783e-01
 1.3707e-01
 2.1131e-01
 1.6595e-04
-3.2211e-02
-1.5293e-02
-1.1494e-03
 1.8486e-01
 2.6609e-01
 1.8440e-01
 2.2997e-01
 4.7652e-02
 9.2821e-02
-5.9602e-06
 5.3773e-03
 1.2814e-01
-6.3296e-04
 2.2609e-06
 1.6365e-01
 1.1699e-01
 1.9503e-01
 1.8941e-03
 1.7983e-01
 1.1410e-01
 1.8587e-01
 2.6753e-04
 1.1377e-01
 1.4556e-01
 2.0240e-01
 1.0300e-01
 2.5308e-08
 2.0455e-01
 2.3635e-01
 4.0376e-03
 1.4885e-06
 2.6631e-01
 7.2618e-02
 2.5890e-01
-4.5141e-03
 1.4874e-01
 2.5813e-01
 1.4446e-01
 2.0476e-01
 5.1917e-04
 1.3332e-01
-3.0500e-03
-2.8465e-07
 1.1479e-01
 5.7585e-03
 1.0129e-01
 2.1036e-01
 1.7736e-01
 1.0850e-01
 7.3646e-02
-8.7343e-04
 1.8930e-01
 2.5171e-08
 2.2083e-01
 3.7008e-02
-8.7625e-03
 2.6809e-06
 1.5259e-01
 2.4068e-01
-4.6558e-03
-2.5157e-08
 1.7323e-01
 1.4529e-01
 1.5122e-01
 1.3936e-02
 1.0643e-02
 1.0234e-01
 2.1276e-01
 1.1609e-01
 1.4455e-06
 7.7813e-02
 2.3590e-01
 1.0331e-01
-2.1370e-03
 2.2225e-02
 2.6490e-01
 5.3318e-02
 1.1014e-01
-4.7809e-09
 1.5605e-01
 1.3709e-01
 1.9997e-01
 1.2544e-01
 1.6009e-03
 1.7971e-01
 2.3313e-01
 1.6170e-01
 7.9911e-02
-3.7592e-03
 5.8648e-02
-5.9956e-09
 1.7753e-01
 3.2006e-06
 2.0509e-01
 2.4585e-01
 1.8359e-01
-9.5735e-06
 6.1166e-03
 1.9506e-01
-1.4920e-02
 1.2797e-01
 1.5274e-01
 1.1678e-01
 2.5840e-01
 1.8168e-01
-3.5162e-03
 1.7881e-01
 2.2759e-03
 1.7817e-01
 1.5301e-01
 1.2492e-01
 1.6297e-03
 2.2141e-01
 1.5610e-01
 1.7923e-01
 1.9679e-01
 1.1674e-01
 2.5461e-01
 2.2336e-01
-8.6711e-03
 2.1022e-01
 2.6223e-02
 5.8941e-02
 6.7842e-03
 3.9383e-04
-4.8280e-03
-8.4383e-07
 1.5757e-01
-3.3189e-07
 9.4600e-02
 2.0100e-01
-4.7420e-03
 8.0226e-02
 4.1372e-03
 1.7892e-01
 2.0364e-01
 1.6549e-01
 9.2902e-03
 1.9418e-01
 2.2028e-03
 1.3965e-03
 2.1172e-03
 2.0174e-01
 1.9476e-01
 2.1752e-01
 1.6050e-01
 1.6618e-02
 1.3520e-04
 2.1298e-01
 2.4628e-01
 2.0706e-01
 2.3657e-01
 1.8144e-01
 8.8419e-06
-1.8546e-02
 2.9771e-04
 1.6264e-01
 1.6636e-01
 1.1508e-01
 1.1071e-01
 1.4062e-01
 1.2329e-01
 1.0167e-01
 9.0378e-02
 6.5264e-02
-1.0424e-02
 1.9850e-01
-1.4600e-06
 4.6740e-08
 6.2084e-07
-4.7988e-03
 6.1632e-02
 1.6973e-01
 1.7514e-03
 7.9060e-08
-4.5426e-03
 1.7435e-01
 1.5014e-01
 1.0646e-01
 9.1071e-02
 2.1939e-01
 2.5915e-08
 1.6844e-01
 1.2294e-01
 1.1205e-01
 8.2247e-02
 1.7394e-01
 2.4874e-01
 1.5983e-07
 2.4351e-03
 1.0579e-01
 7.5377e-02
 2.2991e-01
-9.1592e-03
 2.0529e-01
 7.1128e-02
 9.9701e-04
 1.5408e-01
 2.2712e-02
 2.4965e-01
 1.3048e-03
 1.8778e-01
 8.0938e-08
 1.8622e-02
 1.2455e-01
 1.6796e-02
 1.2836e-01
 1.3399e-02
 2.4071e-01
-5.7628e-03
-8.2883e-04
 1.5164e-01
 1.2500e-01
-1.3223e-03
 1.6686e-01
 2.0293e-01
 1.2653e-01
 1.1740e-01
 1.2208e-01
-5.8454e-04
 1.1730e-05
 2.4734e-01
 9.2554e-02
 2.4105e-01
 1.6128e-01
 1.4836e-01
 1.4868e-07
 6.1657e-03
 1.6485e-01
 1.7401e-01
 2.5618e-01
 2.8015e-01
 1.1606e-01
 1.7603e-01
 1.0104e-01
 9.7931e-03
 3.0543e-01
 1.7581e-01
-3.1169e-03
 1.1331e-01
 2.3405e-01
 1.5033e-01
 1.4255e-06
-2.4860e-03
 1.0574e-01
 2.1443e-01
 2.1830e-01
 2.2319e-01
-3.5428e-03
 7.6133e-02
 2.1730e-01
-1.3519e-03
 7.5629e-03
 1.1418e-01
 1.4467e-01
 5.1706e-02
-2.0379e-07
-4.1459e-03
 2.7953e-01
 9.6022e-02
 2.5677e-01
 2.0866e-01
 4.1675e-02
 8.8582e-02
 1.5234e-01
-3.0899e-10
 1.8085e-01
 2.0183e-01
-2.3279e-03
 6.9877e-08
 8.2608e-02
 2.0111e-01
 3.8781e-02
 2.5544e-01
 4.8373e-03
 1.2000e-01
 1.7985e-01
 3.7402e-02
 1.0675e-01
 1.3236e-01
 9.0898e-02
 9.6931e-02
-4.1332e-07
 4.0971e-02
 1.6959e-01
 9.8011e-02
 1.6147e-06
 8.6560e-02
 1.9620e-01
 1.6818e-06
 3.2809e-01
 1.8875e-01
 2.0212e-01
 2.5101e-01
 1.3554e-01
 2.2584e-01
 1.0739e-01
 6.8136e-04
 6.9796e-02
 5.9475e-03
 2.8626e-02
 1.5608e-01
 6.0822e-07
 2.2908e-01
 2.1893e-01
 6.4698e-05
 1.5747e-01
 2.5414e-07
 1.9498e-08
 1.8155e-01
 1.0366e-03
 1.1361e-01
 1.8329e-01
 1.8751e-01
-3.6502e-04
 1.2180e-01
 1.4766e-01
 1.7146e-01
 4.9706e-02
 1.2225e-01
 1.0283e-02
 2.1014e-01
 1.0958e-01
 4.4404e-08
-1.3192e-02
 1.9448e-01
 1.5193e-01
 3.7440e-03
-4.5107e-08
-8.7403e-04
-2.6012e-03
 1.2968e-01
 3.4562e-02
 3.0098e-02
 1.8282e-01
 2.9391e-01
 5.8525e-02
 1.5908e-01
 2.2260e-02
 1.9951e-01
 2.3200e-01
 1.3207e-01
 5.8267e-06
 1.4268e-01
 6.0896e-03
 8.0552e-02
 1.9510e-01
 1.4327e-01
-2.2665e-02
 2.1358e-01
-3.2695e-03
 1.3043e-01
 2.0119e-01
 7.1658e-02
 9.7573e-09
 1.6613e-01
 1.6489e-02
 1.1414e-01
 1.8905e-01
 6.3744e-02
 1.3969e-01
 2.0824e-02
 5.5400e-03
 8.8336e-02
 1.4921e-01
 1.4837e-07
 1.5383e-01
 9.2749e-03
[torch.FloatTensor of size 512]
), ('layer2.0.bn3.bias', 
 5.0408e-02
-5.3801e-02
-4.9714e-03
 2.0539e-02
 4.7344e-02
-7.9189e-06
 9.5915e-03
 8.5763e-02
 3.1474e-02
 7.3589e-02
 3.1398e-03
-3.0815e-02
 4.4212e-03
 9.0432e-02
-1.7023e-05
-2.1265e-02
-3.0664e-02
 3.1546e-02
 4.3062e-02
-7.6598e-04
 4.0609e-02
 7.0054e-02
-3.2688e-03
-8.6219e-07
 2.9686e-02
 6.5213e-02
 3.7850e-03
-3.1191e-02
-3.1902e-05
 4.1727e-02
-9.7415e-02
 7.2782e-02
 3.0331e-02
-1.1470e-02
 8.0664e-03
-2.3092e-02
 9.3858e-02
 4.7561e-02
-4.6917e-03
 7.5247e-02
-2.9921e-02
 1.5897e-02
-2.7721e-02
 4.7655e-02
 7.5995e-03
-5.5337e-02
 7.0081e-02
-3.3844e-02
 6.6372e-02
-5.6138e-02
 9.5078e-02
-8.3608e-04
 5.9420e-02
 4.6229e-02
 9.6871e-02
 1.0570e-02
-1.0752e-02
-3.7556e-03
 1.1314e-02
-4.0713e-06
 4.4267e-02
-3.9143e-03
-7.9898e-03
 3.3193e-03
-1.0885e-02
 3.3494e-03
-9.2455e-02
 3.9653e-02
-7.8888e-06
-2.4530e-03
 1.3655e-02
 1.8437e-02
-1.1777e-02
-1.0059e-02
-4.4714e-02
 5.1907e-02
-2.4790e-02
 1.0142e-01
 1.8356e-02
-5.6835e-02
-1.9470e-04
 1.7989e-02
-2.3642e-02
 2.5737e-02
-2.2398e-03
 4.8911e-02
 5.5182e-02
-1.8045e-03
 8.5885e-02
-1.0144e-05
-7.6508e-06
-4.7996e-02
-5.4346e-02
 3.8387e-02
-1.5890e-07
-1.3694e-02
 2.1481e-02
 3.7525e-02
-3.2900e-03
-1.4641e-02
-5.8719e-03
-3.0566e-02
 9.1605e-02
 2.9773e-02
 1.0682e-01
-8.9958e-04
 3.5209e-02
-4.2207e-02
-3.2497e-03
-3.2672e-02
 8.3064e-02
 4.8578e-02
-2.8526e-06
-1.8117e-02
 1.0543e-01
 7.2242e-02
-7.6185e-03
 2.2616e-03
 3.7346e-02
 4.6880e-02
-7.9570e-03
 3.6065e-02
 7.7348e-02
 3.2475e-03
-3.0122e-02
 1.5840e-01
-3.3549e-02
 5.9058e-02
-5.4857e-04
-4.5232e-02
-2.0058e-02
 1.7915e-02
 1.7743e-02
-7.1322e-06
 1.1004e-01
-7.1586e-03
-7.0238e-04
 3.5450e-03
-1.4194e-06
-5.0723e-03
 1.0719e-02
-2.6846e-02
-1.5724e-02
-4.8029e-04
-4.1229e-02
 1.0429e-02
 4.7818e-02
-2.1555e-02
 2.1728e-02
-2.0473e-02
 2.5582e-05
 3.2087e-02
 6.4179e-02
-6.2677e-05
 1.1763e-01
 3.7077e-02
 8.7900e-02
-9.1733e-06
 3.5007e-02
-1.5780e-02
-1.4645e-02
 1.1493e-01
 3.5553e-02
 3.8060e-02
 5.2214e-02
-1.3884e-03
-1.9273e-02
-3.3458e-02
-3.7974e-02
-1.7768e-02
-1.3500e-06
 7.1225e-03
 9.3039e-02
 1.4888e-01
-2.7238e-05
-5.6481e-02
 1.0640e-01
-3.8111e-02
 5.7626e-02
-1.1871e-02
-3.7452e-02
-8.3534e-03
 1.3115e-02
 5.2240e-02
 1.4609e-02
 1.3691e-01
-2.8239e-06
 5.4831e-03
 7.0706e-02
-3.6691e-02
-1.7748e-03
-1.1044e-02
 2.9012e-02
 1.1499e-01
 1.5405e-01
 7.9285e-03
-1.1732e-07
-2.6413e-02
 1.2214e-02
 8.3662e-03
-6.2902e-06
 5.0012e-02
-6.5154e-03
 5.0267e-02
-7.1198e-07
-4.5093e-02
 2.5603e-02
 3.9757e-02
 4.9278e-02
-9.7368e-03
-1.0309e-03
 8.0354e-02
 4.9503e-02
-1.0135e-05
-2.0469e-02
-2.3065e-02
-1.0967e-02
 1.3992e-01
 1.3652e-02
 2.2791e-02
 5.5029e-02
-1.3392e-02
-1.2968e-07
-3.2554e-02
 2.6425e-02
 2.0118e-02
 5.8526e-02
 1.9821e-01
 2.1531e-03
 1.3833e-02
-1.2165e-02
 7.6226e-02
 1.0087e-01
-2.9128e-02
-1.1165e-07
 3.4191e-02
-2.7389e-05
-6.7306e-03
 5.2081e-02
 5.8233e-03
-1.8436e-04
 1.9059e-01
 2.4796e-02
 8.9079e-02
-2.8740e-03
-5.1246e-02
 2.3442e-02
 1.3485e-04
-4.4533e-02
 3.7772e-02
 4.5265e-02
 1.2982e-01
-7.0863e-03
 2.9466e-03
 2.1229e-02
 6.8678e-02
-2.4108e-02
-5.9673e-03
 3.3583e-02
 5.1814e-03
-3.4058e-02
 2.4528e-02
 3.2980e-03
 8.5338e-02
 4.6658e-03
 1.1495e-01
 2.1275e-02
 2.0298e-02
 6.5352e-02
 4.8504e-02
-1.2237e-04
-2.2201e-02
-3.2690e-06
 5.6547e-02
 9.2143e-03
 5.3177e-02
 1.3121e-02
 5.8778e-02
-1.0628e-02
-4.6647e-02
-3.8120e-03
 1.6281e-02
-1.6294e-02
 4.6992e-02
 8.2434e-02
-3.7641e-02
 9.0587e-03
-2.1060e-03
-2.2506e-02
-4.0034e-02
-1.7160e-02
-9.8877e-04
-4.3668e-02
-4.3654e-02
-7.9761e-02
 8.0995e-04
 3.3363e-02
-9.5408e-05
 2.4635e-02
-1.2186e-03
 8.8857e-03
-2.5290e-02
 1.5415e-02
 4.1727e-02
-1.4499e-02
-2.5007e-03
-4.2352e-02
 2.3150e-02
 1.0252e-02
-5.0827e-02
-2.9843e-02
-1.7975e-05
-2.4256e-07
-2.3200e-06
 8.7960e-02
-1.9594e-02
-1.4285e-02
 1.3767e-01
-5.4095e-07
 7.2719e-02
 1.5425e-02
 2.2584e-03
 1.6702e-02
-1.8719e-03
 4.0983e-02
-1.5115e-07
-6.9896e-02
-1.1585e-02
-3.8220e-03
 3.9209e-02
 7.9392e-02
-3.1275e-03
-6.0793e-07
-6.5054e-02
-4.6122e-03
 2.2881e-02
-1.5298e-02
 7.1332e-02
-2.5570e-02
-2.5730e-02
 1.0177e-01
-3.8346e-03
 1.1747e-01
-3.2341e-02
 1.2023e-01
 9.6849e-03
-1.8967e-07
 9.6480e-02
-3.4635e-03
 1.3868e-01
-7.0695e-04
-9.4783e-02
-4.2773e-03
 3.6939e-02
 1.4341e-01
 8.6191e-03
 3.4905e-02
 1.3142e-01
 2.1686e-02
-3.2155e-02
 2.2250e-02
 3.3670e-02
 1.1841e-03
-4.6307e-02
-6.7543e-05
-1.9290e-02
-1.0294e-03
-2.2324e-02
 6.6470e-02
-2.0465e-02
-5.0082e-06
 1.1699e-01
 3.5496e-02
 2.2371e-02
 7.6098e-02
-3.1274e-02
-2.6378e-02
 4.4431e-03
-1.3276e-02
 5.5820e-02
-2.9341e-02
-3.4241e-04
 1.2948e-01
 4.8526e-02
 1.2148e-03
 1.5532e-02
-5.9827e-05
 9.0663e-02
 6.6560e-02
-2.1857e-02
-4.2524e-04
 8.4397e-04
 7.5189e-02
-4.0382e-02
-5.7350e-03
 1.3059e-01
 8.9209e-02
-5.2390e-02
 8.3347e-03
 3.6906e-02
-1.9198e-06
-2.5754e-02
-1.8060e-02
 1.5707e-02
-7.2462e-02
-5.3932e-02
 3.3604e-02
 5.3753e-03
 9.2454e-03
-9.6343e-07
 1.1055e-02
 9.3140e-03
 1.2375e-01
-2.4052e-07
 4.7903e-03
-2.4964e-02
 7.8106e-03
-2.5364e-02
-1.7556e-02
 1.1134e-02
-1.5845e-02
-4.8787e-02
 1.7304e-03
-1.9681e-02
-1.2093e-03
-1.8620e-02
-1.9253e-06
 6.8844e-03
-1.2531e-02
-2.6413e-04
-8.7646e-06
 4.8010e-02
 8.7525e-02
-8.0243e-06
-1.7114e-01
-1.3433e-02
-4.0059e-02
-5.1796e-02
 6.7273e-03
 7.6271e-03
 1.7412e-02
 7.1451e-02
-6.3535e-02
 1.4390e-01
 5.2225e-02
-2.6857e-02
-2.3525e-06
 9.4138e-03
-2.4442e-03
-1.6837e-04
-2.2859e-02
-2.3380e-06
-4.7066e-07
 2.9032e-02
-4.4317e-03
 1.2823e-02
 6.2972e-02
-2.8694e-02
 1.0436e-01
 1.2098e-01
-6.8384e-03
 6.1803e-02
-2.1682e-02
 3.3520e-02
 1.8442e-01
 2.0452e-02
-2.2529e-02
-3.8024e-07
 4.7503e-02
-1.6849e-02
-3.5622e-02
 1.3090e-01
-1.0422e-07
 6.6521e-02
 8.8123e-02
 1.2963e-02
 6.9495e-02
 2.2898e-03
-8.1541e-03
-5.9840e-02
-3.7383e-03
 1.6682e-02
 1.8710e-02
-3.3964e-02
-6.9819e-03
-8.8785e-03
-1.6550e-05
 3.6695e-02
 7.4770e-02
-3.6443e-03
-2.7702e-02
-5.7924e-02
 5.3831e-02
 3.8519e-02
 9.7022e-02
-4.2779e-02
-1.2827e-02
-5.4793e-02
-6.0001e-08
 7.1382e-03
 6.7838e-02
-9.0207e-02
-2.3035e-02
-8.5935e-04
 2.0521e-02
 7.8742e-02
 3.6046e-02
-7.2988e-03
-4.1833e-02
-8.7138e-07
 7.5300e-02
 1.6350e-01
[torch.FloatTensor of size 512]
), ('layer2.0.bn3.running_mean', 
-6.3931e-03
 6.1354e-02
 8.7201e-04
 1.4872e-02
 1.3904e-02
 1.5515e-06
-3.5693e-02
-2.0115e-04
-6.0939e-04
 1.0020e-02
-1.9870e-02
 1.3290e-02
-3.3811e-02
 1.6620e-02
-5.3549e-06
-8.1850e-02
-6.3719e-02
 3.2087e-03
-7.2854e-02
-3.0639e-02
-8.7301e-03
 8.0179e-03
-1.7980e-02
-1.9096e-07
 7.0413e-02
-5.4967e-03
-2.3199e-02
 1.0360e-01
 1.3284e-05
 4.4919e-02
 3.4273e-03
-1.6466e-02
-8.5707e-03
 1.0031e-01
 1.8755e-02
-4.8225e-02
-6.2389e-02
-4.9177e-02
 3.8593e-03
 8.8508e-02
 1.3336e-03
 6.0312e-02
-2.8916e-02
-1.5597e-02
-8.3208e-03
-3.7735e-02
 9.5160e-04
-2.6972e-02
-2.6847e-02
-4.5765e-02
 1.1232e-02
-1.7400e-04
 4.7295e-02
 5.9741e-03
 4.9321e-03
-1.1415e-01
-2.7014e-02
 2.1399e-02
 4.6392e-02
 7.4933e-07
-1.6089e-05
 3.2314e-02
-2.7563e-02
 3.7186e-03
 7.1289e-02
 1.0495e-02
 1.5073e-02
 8.4334e-03
-1.1446e-06
 4.5761e-02
 4.6537e-02
-3.3101e-02
-5.7852e-02
 8.0803e-02
-2.7142e-02
-3.5781e-02
 1.0075e-01
 7.6771e-03
-7.0397e-02
 1.3103e-03
 1.1724e-02
 1.7269e-02
 9.1128e-04
-8.3986e-02
-3.2346e-02
 7.7291e-02
-5.2409e-02
 1.5314e-03
-8.9377e-04
 1.7334e-06
 1.6052e-06
 2.0231e-02
-3.7530e-02
 1.7775e-02
 2.2204e-08
-9.6085e-03
 2.4055e-02
-4.6919e-02
 8.2190e-02
 1.7728e-03
 9.0778e-02
 5.3455e-02
-3.6505e-03
 4.7656e-03
 2.5863e-03
-3.5330e-04
 5.0253e-02
-5.6196e-02
 6.1854e-02
-2.2060e-02
 6.8889e-03
 5.0884e-02
 4.5615e-07
 2.9974e-03
 5.1735e-03
 7.9905e-02
 8.4022e-02
 1.4522e-02
-5.8799e-02
-4.7638e-02
-1.3637e-03
-2.2772e-02
 6.4576e-02
-1.2690e-02
 2.8661e-04
 1.2497e-02
-4.4635e-02
 7.7150e-03
-2.4135e-02
 4.4870e-02
 1.7405e-03
 4.2002e-02
-4.0648e-02
-3.9646e-06
-7.1891e-02
-3.4499e-02
 5.8488e-03
-4.4477e-02
 5.4367e-07
-6.5377e-03
 2.3961e-03
 7.4597e-03
 2.7364e-02
-1.3744e-04
-5.2941e-03
-2.2672e-02
 4.8408e-05
 1.8557e-02
-2.8852e-01
 1.0634e-01
 2.1320e-02
-3.2120e-02
 4.0590e-03
-1.0809e-05
-2.0870e-03
-1.6976e-02
 3.9875e-04
-1.0293e-06
 1.4776e-02
-2.9273e-02
 1.6022e-02
-2.2480e-02
 3.4210e-02
 1.0381e-01
 1.8979e-02
-5.3453e-04
 1.1720e-01
 2.2339e-02
 6.3151e-02
 5.7852e-02
 1.1858e-07
 2.6807e-02
 1.6651e-01
-1.5954e-02
 2.2593e-06
-3.3519e-02
 7.2258e-02
 5.5791e-03
 4.6174e-03
 4.3852e-03
-6.8045e-02
-5.8884e-02
-1.5332e-02
-4.1789e-03
 6.1218e-02
-2.7989e-02
-1.2065e-06
 1.9072e-02
 9.2781e-03
 9.0425e-02
-1.2878e-02
-1.4907e-02
 4.9350e-02
-1.3282e-01
 1.8408e-02
 1.9563e-02
-4.5905e-08
 3.3915e-02
 9.4897e-03
-1.0320e-02
 4.6463e-07
 3.6432e-02
 9.6605e-02
-1.0767e-02
-3.6253e-07
-5.1742e-02
 6.6358e-02
 3.3684e-02
-3.3709e-02
-7.9369e-03
-8.5496e-02
 2.2133e-03
-9.0173e-02
 1.6742e-06
-8.3426e-03
-4.7353e-03
-5.7937e-02
 3.2340e-02
-8.5744e-03
-9.3326e-02
 2.2286e-02
 1.2690e-02
-5.6980e-08
 6.2092e-03
 4.8594e-02
 3.4460e-02
-7.0335e-03
 2.2313e-02
 3.6281e-02
-2.0363e-01
 8.2039e-02
-2.9766e-02
 5.3828e-03
 7.7500e-03
-5.7665e-08
 1.6156e-02
 1.2402e-05
 7.3842e-03
 4.8058e-02
-1.2826e-02
 1.1955e-05
 9.0345e-03
 4.9077e-02
-2.9993e-02
-1.4218e-03
-3.9373e-02
 9.0281e-03
-8.4779e-02
 3.8876e-02
-1.0114e-02
-1.8687e-02
-2.4772e-02
 5.8895e-03
-4.0576e-02
 3.5428e-02
 6.4337e-03
 2.0203e-02
 5.2309e-02
-2.0625e-02
-5.2550e-02
 7.9536e-03
-7.6314e-03
 2.3026e-02
-2.3293e-02
 4.3013e-02
-1.1043e-02
-3.3474e-02
-1.5340e-02
-2.0754e-02
-4.8816e-03
-6.3765e-06
-1.5791e-02
-8.4127e-07
-4.8150e-02
-5.5250e-02
 3.5306e-03
-6.3677e-02
-4.6748e-04
 4.0387e-02
 8.3669e-02
 9.2899e-02
-3.4781e-03
 8.8082e-03
-4.0770e-03
-1.9680e-02
 1.8462e-03
-1.2623e-02
-8.1720e-03
 1.1596e-01
-1.4053e-03
 5.6168e-03
 1.7116e-04
-2.3008e-03
 1.5932e-02
-7.5418e-02
-1.0813e-01
-1.1499e-02
-1.8518e-06
 2.6463e-03
 1.6440e-03
-3.4076e-02
-2.4480e-03
-7.5064e-03
-3.0460e-02
-8.1966e-03
-1.3349e-01
 1.2644e-02
-5.7210e-02
 1.2329e-02
-7.4399e-04
 2.7889e-02
 6.0096e-07
 2.3491e-08
 1.2784e-06
 5.3827e-03
-1.0320e-02
 1.6563e-02
 2.5329e-03
 2.7978e-07
 2.9838e-03
 1.3859e-01
-3.4349e-02
 3.5136e-02
-1.3234e-02
-1.1255e-01
 1.1794e-08
-4.7842e-02
 1.4170e-02
-4.7987e-02
 3.5123e-02
-4.0424e-02
 1.3530e-01
-9.6333e-08
-5.2883e-03
-3.6444e-02
 5.4496e-02
-7.0414e-02
 6.3611e-03
 2.3925e-03
 6.7240e-03
-5.1051e-03
-1.6096e-02
-1.9911e-02
-2.0494e-02
-3.8144e-03
-5.4088e-03
-3.8676e-08
-4.0492e-03
-5.2385e-03
 1.7643e-02
 7.8752e-02
 2.6894e-02
-1.2595e-01
 6.4774e-03
 1.0902e-02
-1.2105e-02
 2.7733e-02
 3.4428e-03
-1.2013e-01
 1.4325e-03
-6.6154e-02
 1.0766e-02
 4.3379e-03
-1.7291e-03
-1.2360e-05
-2.3994e-02
 3.1723e-02
-1.2490e-01
-1.0143e-01
-5.5396e-02
 6.4076e-07
 3.6759e-03
-4.9910e-02
-2.2973e-02
-1.9665e-02
-1.2484e-01
 4.0656e-03
 6.9022e-04
 1.6922e-02
-1.4236e-02
-2.0375e-02
-4.7365e-03
-6.1146e-03
-6.3278e-02
 1.6995e-02
 4.5639e-02
 8.4532e-06
-1.3845e-02
-2.0703e-02
-1.6262e-02
 1.1534e-02
 3.0892e-02
-1.2281e-02
 7.0951e-03
 2.6485e-02
 1.1926e-02
 1.4547e-02
-2.4506e-02
 1.1164e-02
 2.3999e-04
 2.3757e-07
-5.2049e-03
 1.4203e-02
-2.0020e-02
-2.6168e-02
 1.7952e-01
 9.8595e-03
-1.8294e-02
-7.5239e-02
 1.4533e-07
-5.0895e-02
 1.0307e-02
 5.3040e-03
-9.7377e-08
 2.3023e-02
-1.4893e-02
-6.7411e-03
-2.2701e-02
 1.0647e-02
-2.8720e-02
 3.9613e-03
 5.0778e-04
 7.6938e-02
-1.4244e-02
 7.8843e-02
 2.3266e-02
-1.2731e-09
 1.0794e-02
-2.4201e-02
-2.5035e-02
-3.3053e-06
-1.2096e-01
-1.0010e-01
 4.0978e-06
 8.1931e-02
-3.2916e-02
-5.6122e-03
-1.4927e-02
 7.6505e-02
 2.9905e-02
 2.1592e-02
 5.8007e-03
-1.4802e-02
 2.5707e-02
-1.2970e-02
-3.1370e-02
 9.6586e-07
 1.6766e-02
-3.1226e-02
 3.8163e-05
 1.7502e-01
 1.0212e-07
 6.7184e-08
 7.9482e-04
 2.0765e-05
-6.1289e-02
 5.6045e-03
 4.1521e-02
 2.7280e-02
-2.6187e-02
-6.5789e-02
 4.5817e-02
-1.6195e-03
-9.1402e-03
 6.3323e-03
-9.9011e-03
 7.0550e-02
-1.2658e-07
-9.9397e-03
-2.6167e-02
 2.4175e-02
 1.8937e-02
-9.5926e-09
 4.4674e-03
 1.0025e-02
-3.3073e-02
-6.9842e-03
 2.2974e-02
-6.4428e-02
-1.6016e-02
 9.1045e-03
 1.1708e-02
-1.1256e-03
-4.2443e-02
-1.0609e-02
 2.1397e-02
-7.9244e-06
-2.7739e-02
-1.3223e-03
 1.3695e-02
 2.9285e-02
 1.1145e-01
 1.0202e-02
-1.2739e-01
-7.1200e-03
-8.9738e-03
-3.8435e-02
 3.8037e-02
 2.8080e-08
 5.2168e-03
 1.1810e-02
-2.6658e-03
 1.6327e-02
-2.8060e-02
 2.1817e-03
 5.5590e-02
-2.4859e-04
 2.5752e-02
 1.8880e-02
 7.6664e-09
-9.0676e-03
-2.5439e-03
[torch.FloatTensor of size 512]
), ('layer2.0.bn3.running_var', 
 3.8064e-04
 3.1423e-03
 4.3738e-06
 2.1563e-03
 2.1167e-04
 1.1285e-11
 4.4644e-03
 4.4060e-04
 1.8757e-04
 5.8187e-04
 1.0121e-03
 1.7626e-03
 1.8693e-03
 3.9977e-04
 3.8014e-11
 1.6871e-03
 1.2253e-02
 2.3406e-04
 3.4453e-03
 1.2914e-03
 1.6119e-03
 2.4451e-04
 4.4246e-03
 6.2209e-14
 5.1698e-03
 2.4970e-04
 1.7599e-03
 3.5416e-03
 2.5544e-10
 5.1102e-03
 3.7740e-03
 2.9642e-04
 1.4555e-03
 2.3362e-03
 1.7457e-03
 5.3930e-03
 5.8219e-03
 5.2830e-03
 8.1687e-06
 1.3873e-02
 5.4570e-03
 4.8166e-03
 9.2437e-04
 1.9242e-04
 2.1506e-04
 3.8269e-03
 2.7367e-04
 2.6416e-03
 3.2980e-03
 2.3623e-03
 3.4507e-04
 6.6671e-08
 2.9248e-03
 1.9494e-03
 2.8155e-04
 3.2095e-03
 6.7772e-04
 9.4937e-04
 6.2780e-03
 1.4236e-12
 1.6751e-04
 2.6077e-03
 2.1798e-03
 1.8409e-04
 1.6461e-03
 2.4852e-03
 6.5711e-04
 3.5069e-03
 1.4809e-11
 3.6755e-03
 5.5394e-03
 2.8903e-03
 4.0166e-03
 2.4062e-03
 6.6308e-03
 3.8544e-04
 3.9534e-03
 3.5945e-04
 4.7538e-03
 2.2109e-03
 4.7854e-03
 2.0940e-03
 7.8394e-05
 4.5170e-03
 1.8404e-03
 6.2026e-03
 5.0749e-03
 2.2332e-04
 4.8361e-04
 1.7854e-11
 6.0408e-12
 3.1704e-03
 1.2905e-03
 2.8563e-04
 1.8265e-15
 2.1898e-03
 5.1033e-03
 6.0360e-04
 2.3189e-03
 5.5858e-04
 2.5814e-03
 2.3739e-03
 3.7656e-04
 4.2737e-03
 4.0975e-04
 2.0079e-07
 3.8818e-03
 2.1625e-03
 4.5011e-03
 4.6291e-04
 3.1922e-03
 4.9113e-03
 3.1332e-13
 2.7248e-03
 3.0774e-04
 5.9078e-03
 2.2526e-03
 3.2609e-03
 4.2299e-03
 1.5617e-03
 2.0081e-03
 2.3013e-03
 1.9584e-03
 2.2002e-03
 1.2341e-03
 6.5543e-04
 5.8659e-03
 2.0427e-03
 8.9127e-04
 3.2144e-03
 4.1782e-03
 2.4196e-03
 2.3124e-03
 1.8500e-11
 4.7240e-03
 1.1265e-03
 3.3273e-03
 4.9418e-03
 2.4807e-13
 2.7650e-03
 2.9209e-03
 2.7145e-03
 3.7660e-03
 5.8023e-08
 4.7446e-04
 2.6828e-04
 2.2460e-04
 2.5916e-03
 8.4444e-03
 3.5542e-03
 7.0863e-03
 1.2464e-03
 2.5475e-03
 2.9426e-10
 3.6271e-04
 4.9402e-03
 3.1839e-04
 1.6440e-11
 4.8314e-03
 1.5283e-03
 4.6113e-03
 5.1288e-04
 4.8828e-03
 3.2949e-03
 3.4637e-03
 2.2573e-07
 2.5453e-03
 1.6979e-03
 2.9792e-03
 1.2549e-03
 9.3303e-14
 1.0898e-02
 1.5531e-02
 5.3586e-04
 5.4220e-11
 4.5349e-03
 2.6696e-03
 7.0308e-03
 2.4837e-04
 2.1744e-03
 3.2305e-03
 3.3251e-03
 4.1283e-03
 4.6614e-04
 2.6875e-03
 2.8267e-04
 1.0361e-12
 2.4144e-03
 2.9637e-04
 2.4834e-03
 6.4697e-03
 2.6085e-03
 2.5544e-03
 3.9417e-03
 9.7352e-04
 3.5785e-03
 1.6879e-15
 3.6831e-03
 5.0160e-04
 3.0305e-04
 3.9450e-12
 4.7097e-03
 4.8607e-03
 1.3509e-04
 1.5294e-13
 1.3763e-03
 2.0345e-03
 2.8402e-03
 9.1517e-04
 8.1630e-05
 2.3501e-03
 1.0944e-02
 6.3276e-03
 1.8696e-11
 9.1108e-04
 4.1988e-03
 1.4206e-03
 7.6363e-04
 5.0429e-04
 5.8993e-03
 1.0072e-03
 1.5297e-03
 9.2827e-16
 1.3508e-03
 3.5146e-03
 5.0898e-03
 2.6430e-03
 7.0872e-04
 3.9933e-03
 8.2620e-03
 2.2910e-03
 1.8312e-03
 4.0023e-04
 4.2567e-04
 1.9069e-15
 5.1658e-03
 3.2787e-10
 5.7318e-03
 9.1441e-03
 2.4623e-03
 1.7705e-09
 1.0560e-03
 3.2427e-03
 5.0171e-04
 1.5786e-03
 2.1308e-03
 1.8247e-03
 8.0878e-03
 2.3542e-03
 2.9140e-04
 4.8865e-03
 5.5277e-04
 3.4504e-03
 3.4157e-03
 2.1082e-03
 3.1131e-04
 3.2847e-03
 3.3353e-03
 6.0946e-03
 5.8004e-03
 1.4115e-03
 6.1415e-03
 5.5618e-03
 3.6397e-04
 4.2195e-03
 1.2577e-03
 1.5282e-03
 2.0257e-04
 5.6250e-04
 1.8196e-04
 1.1441e-09
 2.3999e-03
 4.4849e-13
 2.5959e-03
 4.4704e-03
 3.8515e-04
 9.3874e-04
 2.1802e-04
 2.4137e-03
 4.9641e-03
 3.0380e-03
 2.8289e-04
 3.4305e-03
 2.1527e-04
 3.6310e-04
 3.1559e-04
 5.2639e-03
 3.4287e-03
 4.7748e-03
 1.5094e-03
 2.3327e-04
 7.2812e-08
 3.1168e-03
 4.2290e-03
 1.6211e-03
 6.0379e-03
 5.3267e-03
 1.3977e-09
 2.3536e-04
 8.2314e-07
 3.2380e-03
 1.7743e-03
 1.3643e-03
 2.2146e-03
 1.4626e-03
 2.4129e-03
 5.1287e-04
 1.9999e-03
 1.2184e-03
 1.5158e-04
 2.5637e-03
 7.9952e-12
 2.5424e-14
 9.7057e-13
 3.4562e-04
 1.3674e-03
 3.8354e-03
 3.9145e-04
 5.0887e-14
 3.6451e-04
 3.2376e-03
 3.3025e-03
 2.4749e-03
 9.1169e-04
 1.0003e-02
 2.0039e-15
 1.9325e-03
 2.8489e-03
 2.4226e-03
 2.3729e-03
 6.5024e-03
 6.4543e-03
 7.6524e-14
 1.4378e-04
 1.4431e-03
 1.4819e-03
 3.3849e-03
 3.3822e-04
 3.8741e-03
 1.1942e-03
 3.9167e-04
 3.4017e-03
 4.3754e-04
 3.9046e-03
 6.8222e-04
 5.5887e-03
 6.6222e-15
 5.3030e-04
 2.3679e-03
 7.7626e-04
 3.2144e-03
 2.4220e-04
 6.4664e-03
 2.3776e-04
 6.3967e-04
 3.6286e-03
 2.1252e-03
 5.4888e-04
 2.6567e-03
 3.5737e-03
 2.7580e-03
 3.0111e-03
 1.9151e-03
 2.6193e-04
 6.2903e-10
 5.8996e-03
 2.6376e-03
 3.6249e-03
 5.0400e-03
 1.9075e-03
 2.9899e-12
 3.8777e-04
 5.7406e-03
 5.4606e-03
 1.0127e-02
 7.8031e-03
 1.5735e-03
 4.4618e-03
 1.2604e-03
 1.6792e-04
 5.7320e-03
 3.9635e-03
 2.7072e-04
 2.4258e-03
 5.4103e-03
 3.2481e-03
 5.6877e-10
 2.8584e-04
 4.4163e-03
 4.5188e-03
 5.1010e-03
 5.3101e-03
 3.8002e-04
 3.2540e-04
 5.9162e-03
 9.0130e-04
 4.3560e-04
 1.7223e-03
 3.4952e-03
 1.2219e-03
 1.1688e-13
 3.7425e-04
 5.5125e-03
 2.6784e-03
 3.5834e-03
 8.5860e-03
 9.8517e-04
 1.6251e-03
 1.7916e-03
 1.3160e-13
 3.4244e-03
 4.6919e-03
 4.2706e-04
 8.1518e-15
 1.9655e-03
 2.4813e-03
 2.5629e-04
 4.9995e-03
 2.9911e-04
 3.5077e-03
 3.0045e-03
 5.7116e-04
 1.4471e-03
 2.5626e-03
 1.6502e-03
 1.1506e-03
 8.6924e-14
 8.9919e-04
 3.6008e-03
 1.3638e-03
 3.7918e-12
 1.4586e-03
 5.1682e-03
 2.1845e-11
 3.9012e-03
 2.3219e-03
 3.0376e-03
 4.5370e-03
 3.2255e-03
 4.3535e-03
 1.5562e-03
 4.2979e-04
 1.4176e-03
 8.6695e-04
 4.7154e-04
 3.3039e-03
 1.3876e-12
 6.1592e-03
 7.1232e-03
 7.4653e-09
 3.9861e-03
 3.4174e-13
 4.1285e-14
 4.2474e-03
 3.1306e-06
 2.4909e-03
 4.3665e-03
 2.7381e-03
 6.5056e-04
 8.0069e-03
 1.3014e-03
 3.8561e-03
 3.4137e-04
 3.0377e-03
 7.2862e-04
 5.5513e-03
 8.5091e-04
 2.4641e-14
 4.1657e-04
 3.5259e-03
 1.7260e-03
 7.1339e-04
 8.2009e-16
 3.0432e-04
 3.4486e-04
 2.8376e-03
 1.2345e-03
 4.3917e-04
 4.4523e-03
 3.6989e-03
 7.9996e-04
 4.2160e-03
 3.3543e-04
 3.9117e-03
 5.2858e-03
 1.9280e-03
 5.3877e-11
 3.1237e-03
 2.8711e-04
 2.3955e-03
 3.5086e-03
 1.8758e-03
 3.4885e-04
 5.4305e-03
 3.7811e-04
 2.8461e-03
 5.0711e-03
 1.0288e-03
 6.2058e-16
 3.6342e-03
 1.8608e-04
 1.6004e-03
 2.8572e-03
 1.1377e-03
 2.5799e-03
 6.4608e-04
 2.4028e-04
 9.3257e-04
 2.1357e-03
 1.1409e-13
 6.0096e-03
 3.7001e-04
[torch.FloatTensor of size 512]
), ('layer2.0.downsample.0.weight', 
( 0 , 0 ,.,.) = 
  6.9636e-02

( 0 , 1 ,.,.) = 
  3.6008e-03

( 0 , 2 ,.,.) = 
  1.3478e-02
    ... 

( 0 ,253,.,.) = 
  1.8134e-01

( 0 ,254,.,.) = 
  1.9458e-03

( 0 ,255,.,.) = 
  3.1590e-03
      ⋮  

( 1 , 0 ,.,.) = 
 -1.0936e-02

( 1 , 1 ,.,.) = 
 -3.4424e-03

( 1 , 2 ,.,.) = 
  1.6514e-02
    ... 

( 1 ,253,.,.) = 
  3.2527e-03

( 1 ,254,.,.) = 
 -3.9140e-02

( 1 ,255,.,.) = 
 -5.9866e-03
      ⋮  

( 2 , 0 ,.,.) = 
  8.8925e-04

( 2 , 1 ,.,.) = 
  1.0271e-03

( 2 , 2 ,.,.) = 
  1.5778e-03
    ... 

( 2 ,253,.,.) = 
  2.5194e-04

( 2 ,254,.,.) = 
 -2.1439e-03

( 2 ,255,.,.) = 
 -1.1421e-04
...     
      ⋮  

(509, 0 ,.,.) = 
  1.0406e-07

(509, 1 ,.,.) = 
  5.3282e-08

(509, 2 ,.,.) = 
  9.3874e-08
    ... 

(509,253,.,.) = 
 -3.8367e-08

(509,254,.,.) = 
 -1.5700e-07

(509,255,.,.) = 
  1.0596e-07
      ⋮  

(510, 0 ,.,.) = 
  8.8278e-03

(510, 1 ,.,.) = 
 -2.4494e-03

(510, 2 ,.,.) = 
 -4.0442e-02
    ... 

(510,253,.,.) = 
  1.2038e-02

(510,254,.,.) = 
  2.7628e-02

(510,255,.,.) = 
  1.4599e-02
      ⋮  

(511, 0 ,.,.) = 
  7.5303e-03

(511, 1 ,.,.) = 
 -4.6605e-02

(511, 2 ,.,.) = 
 -5.5020e-02
    ... 

(511,253,.,.) = 
  1.6583e-02

(511,254,.,.) = 
 -4.7406e-02

(511,255,.,.) = 
  1.2077e-02
[torch.FloatTensor of size 512x256x1x1]
), ('layer2.0.downsample.1.weight', 
 2.5894e-01
 1.0952e-01
 2.4350e-03
 9.8841e-02
 1.4828e-01
 2.2900e-06
 1.2362e-01
 2.5867e-01
 1.7806e-01
 2.6309e-02
 8.7784e-02
 2.2239e-01
 1.9097e-01
 2.2899e-01
-2.7017e-06
 1.1980e-01
 7.9462e-02
 1.2918e-01
 8.1540e-02
 1.6561e-01
 8.5212e-02
 2.0080e-01
 1.7026e-01
 3.6475e-07
 2.3972e-01
 9.7817e-02
 1.0577e-01
 9.8964e-02
-4.4109e-06
 5.2819e-02
 1.9113e-01
 1.5563e-01
 1.0492e-01
 1.6792e-01
 2.1628e-01
 1.4154e-01
 2.2748e-01
 1.5448e-01
-2.0087e-03
 1.4546e-01
 9.9183e-02
 3.7556e-02
 9.3432e-02
 2.4658e-01
 6.6383e-02
 8.1327e-02
 2.4016e-01
 1.4118e-01
 3.5700e-02
 1.1152e-01
 1.4306e-01
 1.0296e-03
 2.9085e-02
 2.2361e-01
 3.1879e-01
 4.0880e-02
 1.4030e-01
 7.4188e-02
 1.3349e-01
 8.4435e-07
 1.4750e-01
 1.4078e-01
 5.6040e-02
 2.3601e-01
 2.6426e-02
 1.0163e-01
 2.7476e-01
 7.1125e-02
 2.8428e-06
 6.3914e-02
 9.2293e-03
 1.2069e-01
 9.6343e-02
 1.0677e-01
 3.5325e-02
 2.7641e-01
 5.7064e-02
 2.2450e-01
 1.6698e-01
 8.4091e-02
 7.4761e-02
 1.1079e-01
 4.9667e-02
 7.8219e-02
 1.3103e-01
 1.2695e-01
 1.2179e-01
 7.4704e-02
 2.9422e-01
-1.7493e-06
 1.9132e-06
 9.9225e-02
 6.5375e-02
 2.0994e-01
 5.8966e-09
 3.3057e-02
 1.6077e-01
 2.2146e-01
 2.2166e-01
 3.6653e-01
 2.8574e-01
 5.7153e-02
 2.7196e-01
 8.0166e-02
 2.1129e-01
 1.2923e-04
 7.7990e-02
 1.0840e-01
 1.6766e-01
 1.7519e-01
 1.9105e-01
 8.9661e-02
 7.1795e-07
 9.4744e-02
 1.9144e-01
 1.5211e-01
 8.0793e-02
 1.7757e-01
 1.0797e-01
 1.7612e-01
 9.4192e-02
 1.4459e-01
 1.9132e-01
 1.9754e-01
 1.1739e-01
 2.7245e-01
 1.0505e-01
 2.1353e-01
 1.1512e-01
 1.0201e-01
 1.0968e-01
 6.4043e-02
 1.3702e-01
 1.5151e-06
 2.1890e-01
 9.4661e-02
 1.2666e-01
 1.8719e-01
-1.3624e-07
 1.0235e-01
 1.7126e-01
 7.2575e-02
 9.3477e-02
 8.6764e-05
 9.3516e-02
 3.4630e-01
 2.9881e-01
 1.3225e-01
 4.2841e-02
 2.5396e-01
 1.2691e-01
 5.6624e-02
 2.1234e-01
 9.4354e-06
 2.4363e-01
 2.1899e-01
 2.4067e-01
 2.0311e-06
 2.0009e-01
 1.1029e-01
 7.6456e-02
 2.2377e-01
 7.9825e-02
 2.8083e-01
 6.8383e-02
 3.1770e-04
 2.3516e-01
 1.0715e-01
 5.0745e-02
 6.1870e-02
 5.0323e-07
 1.8813e-01
 2.2219e-02
 1.8896e-01
 9.1058e-06
-9.3720e-03
 1.4908e-01
 1.2078e-01
 1.4497e-01
 2.3185e-01
 1.1183e-01
 1.3252e-01
 7.5160e-02
 2.4048e-01
 2.5597e-01
 3.1719e-01
 9.3287e-07
 1.7016e-01
 2.7553e-01
 1.1209e-01
 1.9916e-01
 1.3921e-01
 2.2639e-01
 2.5571e-01
 2.9736e-01
 1.0919e-01
-2.1788e-08
 4.1135e-02
 1.0497e-01
 2.7017e-01
-1.1039e-06
 4.1734e-02
 1.9707e-01
 9.7280e-02
 2.5079e-07
 8.4855e-02
 5.4263e-02
 1.5409e-01
 2.3644e-01
-3.3276e-03
 2.0224e-01
 8.1634e-02
 1.8215e-01
 2.9152e-06
 6.7323e-02
 8.5177e-02
 1.6566e-01
 2.6869e-01
 4.0581e-02
 6.9725e-02
 1.6717e-01
 6.8251e-02
 4.3876e-08
 6.3462e-02
 1.8687e-01
 5.9377e-02
 9.9933e-02
 2.7918e-01
 8.4557e-02
 6.4699e-02
 3.1337e-02
 2.1270e-01
 1.6158e-02
 4.6383e-02
 4.3892e-08
 5.5247e-02
 7.5510e-06
 6.5603e-02
 5.0101e-02
 2.0750e-01
 6.1390e-05
 3.1456e-01
 4.8928e-02
 2.3551e-01
 4.5786e-02
 1.4947e-01
 2.8260e-01
 1.0210e-01
 7.0082e-02
 1.9167e-01
 4.2421e-02
 2.1541e-01
 7.3094e-02
 1.2241e-01
 4.8193e-02
 2.0008e-01
 5.0354e-02
 1.3672e-01
 3.6754e-02
 1.3625e-01
 1.2990e-01
 5.0630e-02
 5.2546e-02
 2.3381e-01
 6.6546e-02
 2.4019e-01
 3.0668e-02
 3.0689e-01
 2.2720e-01
 2.3522e-01
 4.6171e-05
 8.1741e-02
 8.7562e-07
 1.3516e-01
 6.1836e-02
 2.0586e-01
 1.3580e-01
 8.8719e-02
 6.4561e-02
 1.2725e-01
 1.7426e-01
 1.8437e-01
 7.7724e-02
 1.5262e-01
 1.7132e-01
 3.1335e-01
 3.4451e-02
 1.2605e-01
 1.2839e-01
 1.0430e-01
 2.7461e-01
 3.5598e-04
 5.6617e-02
 8.7690e-02
 1.5311e-01
 1.1519e-01
 4.3404e-02
 4.2354e-05
 2.3621e-01
-7.8637e-06
 8.4913e-02
 1.4516e-01
 5.9520e-02
 1.1899e-01
 1.5344e-01
 1.2991e-01
 6.9376e-02
 1.4720e-01
 1.5015e-01
 3.4827e-01
 7.8291e-02
 2.7767e-06
 6.0657e-08
-2.3011e-07
 2.5938e-01
 8.8719e-02
 3.8580e-02
 3.3758e-01
 2.0945e-07
 1.4973e-01
 6.0445e-02
 2.0537e-01
 8.5454e-02
 7.9034e-02
 1.6697e-01
-2.9826e-08
 1.3037e-01
 1.7994e-01
 1.7999e-01
 9.8017e-02
 1.0569e-01
 6.9529e-02
 1.5887e-07
 3.4035e-01
 8.6399e-02
 1.6570e-01
 1.1431e-01
 2.3633e-01
 6.3789e-02
 1.7756e-01
 2.2179e-01
 9.1697e-02
 2.5909e-01
 2.8606e-02
 2.4261e-01
 1.7812e-01
 1.1581e-08
 1.8174e-01
 6.3036e-02
 2.2299e-01
 2.1315e-01
 2.1711e-01
 1.6508e-01
 2.9226e-01
 2.0710e-01
 1.5458e-01
 1.4579e-01
 2.2657e-01
 2.0913e-01
 5.4199e-02
 1.4851e-01
 2.0314e-01
 1.0949e-01
 3.2031e-01
 1.3787e-05
 1.8286e-01
 8.8698e-02
 9.3633e-02
 2.0066e-01
 1.6375e-01
 1.7920e-06
 2.1021e-01
 1.5132e-01
 1.2822e-01
 1.8261e-02
 9.5773e-02
 1.3455e-01
 9.0493e-02
 5.6843e-02
 2.1273e-01
-1.9481e-02
 1.1177e-01
 2.0018e-01
 1.4718e-01
 8.9305e-02
 6.1995e-02
 2.2648e-05
 2.6067e-01
 2.3732e-01
 9.6725e-02
 2.7169e-02
 2.7572e-02
 1.5351e-01
 1.1330e-01
 1.2615e-01
 3.1682e-01
 3.6433e-01
 1.5977e-01
 1.3429e-01
 5.2137e-02
 4.6438e-07
 3.5380e-01
 2.2400e-02
 2.0857e-01
 2.3714e-01
 2.0609e-01
 2.7071e-01
 1.1499e-01
 1.0280e-01
 3.3806e-07
 1.2464e-01
 7.5613e-02
 1.7810e-01
 5.9839e-08
 9.7862e-02
 4.7789e-02
 9.8830e-02
 1.3672e-02
 2.6721e-01
 1.3875e-01
 9.9412e-02
 5.8001e-02
 8.8942e-02
 1.8603e-01
 2.2661e-01
 5.3778e-02
 2.9870e-07
 7.1578e-02
 1.6625e-01
 1.3182e-01
 2.2102e-06
 6.4613e-02
 4.4592e-02
 1.4840e-06
 3.7320e-01
 8.3566e-02
 3.6263e-02
 1.2490e-01
 1.4024e-01
 6.7643e-02
 8.7781e-02
 2.4787e-01
 2.7908e-01
 2.9175e-01
 1.0252e-01
 8.8438e-02
-2.4200e-07
 8.3925e-02
 1.8358e-01
-2.7752e-05
 6.4146e-02
 5.6049e-07
 1.0697e-07
 5.9478e-02
 9.6309e-04
 2.5316e-01
 5.5721e-02
 1.0059e-01
 3.5312e-01
 2.6611e-01
 4.9717e-02
 3.2800e-02
 2.8730e-02
 1.9604e-01
 2.3787e-01
 9.8731e-02
 2.1813e-02
 7.8078e-08
 2.5368e-01
 6.4311e-02
 1.0932e-01
 3.2254e-01
 1.4187e-08
 2.1543e-01
 1.8553e-01
 1.2584e-01
 2.2427e-01
 1.3510e-01
 1.3744e-01
 2.8398e-02
 1.1102e-01
 1.4465e-01
 9.9769e-02
 1.4012e-01
 7.7053e-02
 1.0650e-01
-1.2151e-06
 1.5623e-01
 1.8287e-01
 2.6775e-01
 4.6327e-02
 8.6280e-02
 1.4811e-01
 8.9865e-02
 2.9692e-01
 2.4645e-01
 9.5585e-02
 1.9333e-01
 1.5000e-08
 1.2984e-01
 1.4060e-01
 2.0470e-01
 5.6103e-02
 1.4413e-01
 8.3059e-02
 2.8040e-01
 2.5632e-01
 1.4756e-01
 1.4561e-01
 1.8631e-07
 7.1401e-02
 2.6416e-01
[torch.FloatTensor of size 512]
), ('layer2.0.downsample.1.bias', 
 5.0408e-02
-5.3801e-02
-4.9714e-03
 2.0539e-02
 4.7344e-02
-7.9189e-06
 9.5915e-03
 8.5763e-02
 3.1474e-02
 7.3589e-02
 3.1398e-03
-3.0815e-02
 4.4212e-03
 9.0432e-02
-1.7023e-05
-2.1265e-02
-3.0664e-02
 3.1546e-02
 4.3062e-02
-7.6598e-04
 4.0609e-02
 7.0054e-02
-3.2688e-03
-8.6219e-07
 2.9686e-02
 6.5213e-02
 3.7850e-03
-3.1191e-02
-3.1902e-05
 4.1727e-02
-9.7415e-02
 7.2782e-02
 3.0331e-02
-1.1470e-02
 8.0664e-03
-2.3092e-02
 9.3858e-02
 4.7561e-02
-4.6917e-03
 7.5247e-02
-2.9921e-02
 1.5897e-02
-2.7721e-02
 4.7655e-02
 7.5995e-03
-5.5337e-02
 7.0081e-02
-3.3844e-02
 6.6372e-02
-5.6138e-02
 9.5078e-02
-8.3608e-04
 5.9420e-02
 4.6229e-02
 9.6871e-02
 1.0570e-02
-1.0752e-02
-3.7556e-03
 1.1314e-02
-4.0713e-06
 4.4267e-02
-3.9143e-03
-7.9898e-03
 3.3193e-03
-1.0885e-02
 3.3494e-03
-9.2455e-02
 3.9653e-02
-7.8888e-06
-2.4530e-03
 1.3655e-02
 1.8437e-02
-1.1777e-02
-1.0059e-02
-4.4714e-02
 5.1907e-02
-2.4790e-02
 1.0142e-01
 1.8356e-02
-5.6835e-02
-1.9470e-04
 1.7989e-02
-2.3642e-02
 2.5737e-02
-2.2398e-03
 4.8911e-02
 5.5182e-02
-1.8045e-03
 8.5885e-02
-1.0144e-05
-7.6508e-06
-4.7996e-02
-5.4346e-02
 3.8387e-02
-1.5890e-07
-1.3694e-02
 2.1481e-02
 3.7525e-02
-3.2900e-03
-1.4641e-02
-5.8719e-03
-3.0566e-02
 9.1605e-02
 2.9773e-02
 1.0682e-01
-8.9958e-04
 3.5209e-02
-4.2207e-02
-3.2497e-03
-3.2672e-02
 8.3064e-02
 4.8578e-02
-2.8526e-06
-1.8117e-02
 1.0543e-01
 7.2242e-02
-7.6185e-03
 2.2616e-03
 3.7346e-02
 4.6880e-02
-7.9570e-03
 3.6065e-02
 7.7348e-02
 3.2475e-03
-3.0122e-02
 1.5840e-01
-3.3549e-02
 5.9058e-02
-5.4857e-04
-4.5232e-02
-2.0058e-02
 1.7915e-02
 1.7743e-02
-7.1322e-06
 1.1004e-01
-7.1586e-03
-7.0238e-04
 3.5450e-03
-1.4194e-06
-5.0723e-03
 1.0719e-02
-2.6846e-02
-1.5724e-02
-4.8029e-04
-4.1229e-02
 1.0429e-02
 4.7818e-02
-2.1555e-02
 2.1728e-02
-2.0473e-02
 2.5582e-05
 3.2087e-02
 6.4179e-02
-6.2677e-05
 1.1763e-01
 3.7077e-02
 8.7900e-02
-9.1733e-06
 3.5007e-02
-1.5780e-02
-1.4645e-02
 1.1493e-01
 3.5553e-02
 3.8060e-02
 5.2214e-02
-1.3884e-03
-1.9273e-02
-3.3458e-02
-3.7974e-02
-1.7768e-02
-1.3500e-06
 7.1225e-03
 9.3039e-02
 1.4888e-01
-2.7238e-05
-5.6481e-02
 1.0640e-01
-3.8111e-02
 5.7626e-02
-1.1871e-02
-3.7452e-02
-8.3534e-03
 1.3115e-02
 5.2240e-02
 1.4609e-02
 1.3691e-01
-2.8239e-06
 5.4831e-03
 7.0706e-02
-3.6691e-02
-1.7748e-03
-1.1044e-02
 2.9012e-02
 1.1499e-01
 1.5405e-01
 7.9285e-03
-1.1732e-07
-2.6413e-02
 1.2214e-02
 8.3662e-03
-6.2902e-06
 5.0012e-02
-6.5154e-03
 5.0267e-02
-7.1198e-07
-4.5093e-02
 2.5603e-02
 3.9757e-02
 4.9278e-02
-9.7368e-03
-1.0309e-03
 8.0354e-02
 4.9503e-02
-1.0135e-05
-2.0469e-02
-2.3065e-02
-1.0967e-02
 1.3992e-01
 1.3652e-02
 2.2791e-02
 5.5029e-02
-1.3392e-02
-1.2968e-07
-3.2554e-02
 2.6425e-02
 2.0118e-02
 5.8526e-02
 1.9821e-01
 2.1531e-03
 1.3833e-02
-1.2165e-02
 7.6226e-02
 1.0087e-01
-2.9128e-02
-1.1165e-07
 3.4191e-02
-2.7389e-05
-6.7306e-03
 5.2081e-02
 5.8233e-03
-1.8436e-04
 1.9059e-01
 2.4796e-02
 8.9079e-02
-2.8740e-03
-5.1246e-02
 2.3442e-02
 1.3485e-04
-4.4533e-02
 3.7772e-02
 4.5265e-02
 1.2982e-01
-7.0863e-03
 2.9466e-03
 2.1229e-02
 6.8678e-02
-2.4108e-02
-5.9673e-03
 3.3583e-02
 5.1814e-03
-3.4058e-02
 2.4528e-02
 3.2980e-03
 8.5338e-02
 4.6658e-03
 1.1495e-01
 2.1275e-02
 2.0298e-02
 6.5352e-02
 4.8504e-02
-1.2237e-04
-2.2201e-02
-3.2690e-06
 5.6547e-02
 9.2143e-03
 5.3177e-02
 1.3121e-02
 5.8778e-02
-1.0628e-02
-4.6647e-02
-3.8120e-03
 1.6281e-02
-1.6294e-02
 4.6992e-02
 8.2434e-02
-3.7641e-02
 9.0587e-03
-2.1060e-03
-2.2506e-02
-4.0034e-02
-1.7160e-02
-9.8877e-04
-4.3668e-02
-4.3654e-02
-7.9761e-02
 8.0995e-04
 3.3363e-02
-9.5408e-05
 2.4635e-02
-1.2186e-03
 8.8857e-03
-2.5290e-02
 1.5415e-02
 4.1727e-02
-1.4499e-02
-2.5007e-03
-4.2352e-02
 2.3150e-02
 1.0252e-02
-5.0827e-02
-2.9843e-02
-1.7975e-05
-2.4256e-07
-2.3200e-06
 8.7960e-02
-1.9594e-02
-1.4285e-02
 1.3767e-01
-5.4095e-07
 7.2719e-02
 1.5425e-02
 2.2584e-03
 1.6702e-02
-1.8719e-03
 4.0983e-02
-1.5115e-07
-6.9896e-02
-1.1585e-02
-3.8220e-03
 3.9209e-02
 7.9392e-02
-3.1275e-03
-6.0793e-07
-6.5054e-02
-4.6122e-03
 2.2881e-02
-1.5298e-02
 7.1332e-02
-2.5570e-02
-2.5730e-02
 1.0177e-01
-3.8346e-03
 1.1747e-01
-3.2341e-02
 1.2023e-01
 9.6849e-03
-1.8967e-07
 9.6480e-02
-3.4635e-03
 1.3868e-01
-7.0695e-04
-9.4783e-02
-4.2773e-03
 3.6939e-02
 1.4341e-01
 8.6191e-03
 3.4905e-02
 1.3142e-01
 2.1686e-02
-3.2155e-02
 2.2250e-02
 3.3670e-02
 1.1841e-03
-4.6307e-02
-6.7543e-05
-1.9290e-02
-1.0294e-03
-2.2324e-02
 6.6470e-02
-2.0465e-02
-5.0082e-06
 1.1699e-01
 3.5496e-02
 2.2371e-02
 7.6098e-02
-3.1274e-02
-2.6378e-02
 4.4431e-03
-1.3276e-02
 5.5820e-02
-2.9341e-02
-3.4241e-04
 1.2948e-01
 4.8526e-02
 1.2148e-03
 1.5532e-02
-5.9827e-05
 9.0663e-02
 6.6560e-02
-2.1857e-02
-4.2524e-04
 8.4397e-04
 7.5189e-02
-4.0382e-02
-5.7350e-03
 1.3059e-01
 8.9209e-02
-5.2390e-02
 8.3347e-03
 3.6906e-02
-1.9198e-06
-2.5754e-02
-1.8060e-02
 1.5707e-02
-7.2462e-02
-5.3932e-02
 3.3604e-02
 5.3753e-03
 9.2454e-03
-9.6343e-07
 1.1055e-02
 9.3140e-03
 1.2375e-01
-2.4052e-07
 4.7903e-03
-2.4964e-02
 7.8106e-03
-2.5364e-02
-1.7556e-02
 1.1134e-02
-1.5845e-02
-4.8787e-02
 1.7304e-03
-1.9681e-02
-1.2093e-03
-1.8620e-02
-1.9253e-06
 6.8844e-03
-1.2531e-02
-2.6413e-04
-8.7646e-06
 4.8010e-02
 8.7525e-02
-8.0243e-06
-1.7114e-01
-1.3433e-02
-4.0059e-02
-5.1796e-02
 6.7273e-03
 7.6271e-03
 1.7412e-02
 7.1451e-02
-6.3535e-02
 1.4390e-01
 5.2225e-02
-2.6857e-02
-2.3525e-06
 9.4138e-03
-2.4442e-03
-1.6837e-04
-2.2859e-02
-2.3380e-06
-4.7066e-07
 2.9032e-02
-4.4317e-03
 1.2823e-02
 6.2972e-02
-2.8694e-02
 1.0436e-01
 1.2098e-01
-6.8384e-03
 6.1803e-02
-2.1682e-02
 3.3520e-02
 1.8442e-01
 2.0452e-02
-2.2529e-02
-3.8024e-07
 4.7503e-02
-1.6849e-02
-3.5622e-02
 1.3090e-01
-1.0422e-07
 6.6521e-02
 8.8123e-02
 1.2963e-02
 6.9495e-02
 2.2898e-03
-8.1541e-03
-5.9840e-02
-3.7383e-03
 1.6682e-02
 1.8710e-02
-3.3964e-02
-6.9819e-03
-8.8785e-03
-1.6550e-05
 3.6695e-02
 7.4770e-02
-3.6443e-03
-2.7702e-02
-5.7924e-02
 5.3831e-02
 3.8519e-02
 9.7022e-02
-4.2779e-02
-1.2827e-02
-5.4793e-02
-6.0001e-08
 7.1382e-03
 6.7838e-02
-9.0207e-02
-2.3035e-02
-8.5935e-04
 2.0521e-02
 7.8742e-02
 3.6046e-02
-7.2988e-03
-4.1833e-02
-8.7138e-07
 7.5300e-02
 1.6350e-01
[torch.FloatTensor of size 512]
), ('layer2.0.downsample.1.running_mean', 
-3.2737e-02
-2.1287e-02
-6.0250e-03
 5.3464e-02
 6.4172e-02
-8.9838e-06
-8.1428e-02
 8.7869e-03
 1.5890e-01
-2.4506e-02
-3.7405e-02
-1.1666e-01
-8.6221e-02
-1.0586e-01
-5.6859e-06
-1.0544e-01
 1.2591e-01
 8.0831e-02
-3.4221e-02
 9.8962e-03
 4.4905e-03
-1.6279e-02
-8.8771e-02
-6.0376e-07
 6.2006e-03
-7.0056e-02
-5.8643e-02
-1.9232e-02
 1.4717e-05
-6.1274e-02
 4.3950e-02
-2.3130e-01
 1.2222e-02
-1.7461e-01
 8.0400e-02
-3.5144e-03
-1.6862e-01
-7.2866e-04
-6.0447e-03
-6.5459e-02
-2.6176e-02
-8.1081e-03
-1.1765e-01
-1.0731e-01
 9.3942e-02
-4.7759e-02
 3.3601e-01
-8.0517e-03
 5.5449e-02
 4.4123e-02
 4.8269e-02
 1.0297e-03
 2.7215e-02
-1.7301e-01
-6.2817e-02
 2.3774e-02
-1.9886e-02
-3.0421e-02
-1.8671e-01
 3.6083e-06
 1.4858e-01
 4.2696e-02
 3.1203e-02
 3.0719e-01
 6.1006e-02
-1.3669e-02
-2.6655e-02
-8.9832e-02
 1.0119e-05
-9.5014e-03
-4.6550e-02
-5.2796e-02
-1.4246e-01
-5.1311e-02
-4.0434e-02
-4.3379e-01
-2.2463e-02
-5.3433e-02
-7.3515e-02
 2.2092e-02
 8.8904e-02
 6.0868e-02
-4.7923e-02
 3.5362e-02
-1.0951e-01
-9.7469e-02
-6.6871e-02
-2.9370e-02
 1.2146e-01
 4.5466e-06
-8.8221e-06
 1.7930e-02
-3.2892e-02
 1.3221e-01
 1.3709e-07
-6.7236e-02
 1.7473e-01
 1.8505e-01
 2.6410e-01
 1.8477e-01
 1.1125e-01
-1.2028e-02
 1.3034e-01
 1.0530e-01
-5.1759e-02
-4.1126e-04
 4.9490e-02
-9.4348e-02
 5.0109e-02
 2.9717e-03
-3.3944e-01
 7.1691e-02
-2.4177e-06
-7.1444e-02
-6.6497e-02
 1.2845e-01
 1.2372e-01
-1.9703e-01
-1.2470e-01
-5.3995e-02
-8.6278e-02
-3.2635e-02
 1.0811e-02
-5.9348e-03
-4.4649e-02
-1.2721e-01
-1.3688e-01
-9.3941e-02
-7.6050e-02
-6.7009e-02
-7.6703e-02
 4.1975e-03
 3.3258e-02
 3.5290e-06
-1.7353e-01
-1.0814e-01
-7.6216e-02
-3.9644e-02
 1.8117e-06
 2.5376e-03
-5.7217e-02
-4.2200e-02
-3.0467e-02
-5.0780e-05
-1.8844e-02
-2.4817e-01
-2.4295e-02
-6.7167e-02
 3.1191e-02
-1.0737e-01
 5.0400e-02
-6.2871e-02
-2.1910e-02
 5.7536e-05
 5.4207e-02
-6.8860e-02
 1.1096e-01
-3.5926e-06
 8.4501e-02
 6.2955e-02
 9.2855e-02
-2.4515e-01
-2.8121e-02
 1.3259e-02
-4.1553e-02
 1.6622e-03
-4.6903e-02
-2.3862e-02
 5.6817e-02
 1.3164e-02
 1.9022e-07
-7.1042e-02
-7.0964e-02
-2.7864e-02
-1.8005e-05
 1.3760e-02
-1.5116e-01
-6.0485e-02
 1.5730e-01
-6.5318e-02
 1.3083e-01
-7.6846e-02
 5.1015e-03
 1.0353e-01
-4.4280e-02
-1.0302e-01
-4.2063e-06
-5.8998e-03
 1.2681e-02
 1.5989e-01
-1.3633e-01
 2.6188e-02
 1.3549e-02
-1.9972e-01
 1.1085e-01
-8.0867e-02
 2.5684e-08
-3.5706e-03
-6.1373e-02
 2.2167e-01
 3.3676e-06
 1.2458e-03
 7.1704e-03
-8.0005e-02
-9.0974e-07
-1.5263e-02
-3.1873e-02
-1.1568e-01
-1.7872e-01
-1.6668e-02
 6.3214e-02
 9.3949e-03
-2.5869e-01
-9.3160e-06
-1.5933e-02
 4.2988e-02
-1.1376e-01
-1.2151e-01
-1.3603e-02
-5.5879e-02
 4.4097e-03
-3.6364e-02
-2.3434e-07
-1.8679e-02
-1.4260e-02
-4.4291e-02
 6.3491e-02
-6.3456e-02
 8.5471e-02
-1.0568e-01
 8.5579e-03
 3.3264e-02
-7.8247e-02
-3.8108e-02
-1.2369e-07
 7.5318e-02
-3.9371e-05
-8.4071e-02
-4.2443e-02
-1.6625e-01
-6.8596e-05
-2.1479e-01
 6.2513e-02
-1.1771e-01
-1.3266e-02
-7.6601e-02
 1.7086e-01
-3.7421e-02
-7.0032e-02
 4.5750e-02
 8.5106e-03
-2.1824e-01
-3.5107e-02
-9.0701e-02
-2.0008e-02
 1.5432e-01
 5.1236e-02
-3.6648e-02
 6.2914e-02
-1.6959e-01
-5.0203e-02
 8.5311e-02
-2.0259e-02
-2.5547e-01
 1.6345e-02
-1.1096e-01
-3.4939e-02
-1.0909e-01
 3.4483e-02
 3.3887e-02
 2.3554e-04
-4.8614e-02
 2.1024e-07
 4.9594e-02
-2.4444e-02
-1.3772e-01
-2.1084e-02
-2.7013e-02
-8.4289e-02
-2.7922e-02
-3.7968e-02
-7.0906e-02
 3.2247e-02
 1.2809e-01
-7.9168e-02
 6.3124e-02
-2.6810e-02
 4.4430e-02
-7.4795e-02
-5.4340e-02
 4.9486e-02
-1.5427e-03
-1.3200e-02
 5.8099e-02
-1.2695e-02
-3.0078e-02
 2.2145e-02
 1.2276e-05
 1.1489e-01
 2.2692e-04
 2.5556e-02
-1.3622e-01
-4.2008e-02
-1.1475e-01
-2.1115e-01
-1.5755e-01
 2.7435e-02
-7.7058e-02
 5.2464e-02
 1.5071e-01
-9.6252e-02
-2.9362e-06
-3.1567e-07
 1.2252e-06
-4.3247e-02
 6.2754e-02
-3.0566e-02
-1.8367e-01
-2.1209e-07
 8.3479e-02
-5.2436e-02
 8.0716e-02
 6.7060e-02
-5.8729e-02
-7.9852e-02
 1.2569e-07
-1.4361e-01
 9.0185e-04
 9.8612e-04
-3.3452e-02
-1.0508e-01
 7.8948e-02
-2.2341e-06
-1.7968e-02
-1.3028e-01
-7.7182e-02
 9.7013e-02
 8.4910e-03
 5.6490e-02
 5.8931e-02
-8.7363e-02
 1.2183e-01
-4.9770e-01
-3.6905e-03
 7.1034e-02
-9.3677e-03
-5.7753e-08
-3.2652e-01
-3.2160e-02
 1.6136e-01
-9.7924e-03
-1.3620e-02
-7.6875e-02
-2.3031e-02
-1.2856e-01
-1.1602e-01
 4.3948e-02
-2.6532e-01
-7.2533e-03
 7.7412e-03
-1.3115e-01
-1.5892e-01
-8.8112e-02
 6.6403e-02
-1.2335e-04
 8.6889e-03
-7.6402e-02
 3.8321e-02
 2.9312e-02
-1.2549e-01
 1.8301e-06
-3.0827e-01
 2.9131e-02
 1.1265e-01
 5.6533e-02
 2.0188e-01
 7.4756e-02
-6.4356e-02
-1.9618e-02
-7.0374e-02
 3.8860e-02
-8.1274e-02
-2.3738e-01
 1.2101e-01
-4.4390e-02
 1.6919e-02
-4.0266e-05
 7.2090e-02
-1.8969e-01
-1.4402e-01
 2.7650e-02
-7.7583e-02
 9.1201e-02
 7.6076e-02
-8.6289e-02
 1.9724e-02
-7.7397e-02
 6.0749e-02
-5.9004e-02
-2.1669e-02
 8.5842e-07
-3.9090e-02
 3.5149e-02
-1.4925e-01
-6.9369e-02
-4.0476e-02
-3.4951e-02
-2.6082e-02
-6.2907e-03
 2.1373e-06
-9.8384e-02
-1.6144e-01
-1.3825e-01
-1.8923e-07
 3.5019e-02
-7.3676e-03
-4.7399e-02
 2.7836e-02
-5.4551e-02
-7.4180e-02
 2.0804e-02
-8.5603e-02
-2.1558e-02
-1.0140e-01
-7.0299e-02
-2.0756e-02
-9.8422e-07
-3.9094e-02
 1.2999e-02
-1.0084e-01
-7.1278e-06
-8.1658e-02
-4.9852e-02
-2.5114e-06
 1.2291e-01
-5.5613e-03
-1.1942e-02
-2.9167e-02
-3.6817e-02
-5.8964e-02
-4.1568e-02
-4.1061e-02
 7.0349e-02
-9.9171e-02
 1.8568e-02
-9.0996e-02
-1.1379e-06
 1.9255e-02
-7.1422e-02
 3.9760e-05
 8.1508e-02
 4.6418e-07
 1.0792e-07
-2.4490e-02
-3.9535e-03
-1.7865e-03
-4.2580e-02
 2.0895e-02
 2.3170e-01
-3.2803e-01
 3.8485e-03
-3.3555e-03
-4.8574e-02
-5.2476e-02
-8.2665e-02
-5.0810e-02
 1.3509e-02
-2.0630e-08
-1.6165e-01
 4.0997e-02
 2.7949e-03
-3.3984e-02
 4.6705e-08
 1.6640e-01
-7.3936e-02
-8.4494e-02
-1.2979e-01
 8.1497e-03
 7.0294e-02
 3.3058e-02
 1.0942e-01
-1.6503e-01
 1.8657e-01
-7.4533e-02
-1.3215e-01
 1.0241e-01
-1.3243e-05
-9.7510e-02
-2.1557e-01
-2.8274e-01
 5.9999e-02
 9.0322e-02
-2.4598e-02
 7.7016e-02
-1.7579e-01
-4.0226e-02
-5.1730e-02
 7.7587e-02
 1.1972e-10
-1.5547e-01
-4.4538e-02
-3.8800e-02
 4.4522e-02
-1.2555e-02
 1.8766e-02
-4.1859e-02
 9.4575e-02
-9.7540e-02
 7.2259e-02
-4.8071e-07
 5.9653e-03
-2.4201e-01
[torch.FloatTensor of size 512]
), ('layer2.0.downsample.1.running_var', 
 3.2676e-02
 4.8875e-03
 6.5351e-05
 6.6403e-03
 1.5250e-02
 6.8156e-11
 1.1352e-02
 4.3518e-02
 1.4241e-02
 2.5196e-03
 2.8407e-03
 1.0649e-02
 1.4701e-02
 1.8040e-02
 3.1848e-11
 5.8709e-03
 7.8099e-03
 9.1514e-03
 6.2039e-03
 9.4972e-03
 6.9196e-03
 1.8764e-02
 1.8628e-02
 1.3831e-12
 2.4768e-02
 7.3765e-03
 4.9437e-03
 8.6555e-03
 1.6070e-10
 5.1838e-03
 9.5039e-03
 1.3408e-02
 1.1765e-02
 7.6733e-03
 1.3654e-02
 1.3101e-02
 7.4160e-02
 1.3368e-02
 2.1388e-05
 3.9123e-02
 7.1808e-03
 2.8523e-03
 3.2124e-03
 1.8868e-02
 3.2523e-03
 3.6739e-03
 1.3671e-02
 6.3062e-03
 2.5517e-03
 5.2048e-03
 2.4405e-02
 1.8958e-06
 3.5317e-03
 1.5649e-02
 2.9875e-02
 1.8919e-03
 7.0665e-03
 2.9811e-03
 1.2831e-02
 2.2881e-11
 1.6268e-02
 9.9992e-03
 2.8560e-03
 1.7208e-02
 1.5842e-03
 4.7469e-03
 8.1303e-03
 4.5683e-03
 1.0455e-10
 2.4145e-03
 8.7865e-04
 1.1832e-02
 5.5202e-03
 4.4449e-03
 1.7738e-03
 1.0843e-02
 1.7728e-03
 3.8095e-02
 1.7875e-02
 2.5921e-03
 4.9565e-03
 7.2794e-03
 1.3199e-03
 4.8986e-03
 8.1100e-03
 1.3422e-02
 1.1375e-02
 6.8860e-03
 5.2212e-02
 1.6453e-11
 4.9342e-11
 5.4244e-03
 2.8333e-03
 1.8612e-02
 8.6238e-15
 1.8208e-03
 1.6370e-02
 1.9272e-02
 1.2156e-02
 7.6135e-02
 1.4519e-02
 1.4382e-03
 4.4069e-02
 7.0195e-03
 3.9490e-02
 1.3011e-06
 6.7262e-03
 3.3839e-03
 2.2413e-02
 1.0953e-02
 2.6067e-02
 1.3594e-02
 3.3326e-12
 4.8619e-03
 1.9834e-02
 3.9160e-02
 5.3675e-03
 1.3798e-02
 1.0162e-02
 1.7335e-02
 4.6427e-03
 1.1566e-02
 2.1954e-02
 1.7228e-02
 5.8185e-03
 6.8230e-02
 9.2117e-03
 2.1838e-02
 5.5975e-03
 7.8737e-03
 9.3506e-03
 4.6362e-03
 1.1166e-02
 2.1474e-10
 3.2652e-02
 5.0774e-03
 9.4057e-03
 1.4257e-02
 5.1450e-13
 4.4726e-03
 1.7332e-02
 4.7382e-03
 5.3812e-03
 5.0857e-08
 3.6435e-03
 1.9217e-02
 1.5996e-02
 8.9470e-03
 4.0294e-03
 1.5583e-02
 1.0877e-02
 5.0694e-03
 2.7062e-02
 2.3873e-09
 3.4047e-02
 3.1269e-02
 3.7159e-02
 4.5789e-11
 2.1397e-02
 5.9649e-03
 4.3476e-03
 2.7427e-02
 9.5161e-03
 1.8821e-02
 2.7365e-03
 3.7616e-06
 1.8491e-02
 5.4305e-03
 2.0944e-03
 2.6439e-03
 2.0480e-12
 4.6789e-02
 3.6205e-03
 3.6816e-02
 8.2170e-10
 6.8822e-04
 1.5000e-02
 1.3449e-02
 1.4475e-02
 9.5991e-03
 5.9162e-03
 1.1274e-02
 5.0217e-03
 3.5551e-02
 1.4549e-02
 3.1157e-02
 2.0028e-11
 1.6432e-02
 2.4252e-02
 8.3483e-03
 2.0954e-02
 9.9047e-03
 1.7820e-02
 4.1771e-02
 1.0761e-01
 8.8021e-03
 3.1213e-15
 1.6205e-03
 5.1841e-03
 3.5763e-02
 3.4992e-12
 4.9946e-03
 1.6556e-02
 5.9117e-03
 1.5937e-12
 2.1153e-03
 2.2919e-03
 1.2099e-02
 6.2283e-02
 5.5795e-05
 1.5912e-02
 1.6906e-02
 3.9418e-02
 7.9723e-11
 2.5403e-03
 4.5760e-03
 1.1271e-02
 5.5206e-02
 4.4713e-03
 4.1727e-03
 1.5202e-02
 3.8182e-03
 6.3416e-15
 1.6116e-03
 2.0111e-02
 4.7002e-03
 8.4664e-03
 8.2665e-02
 7.0913e-03
 8.0147e-03
 1.7986e-03
 1.7969e-02
 1.8390e-03
 5.7680e-04
 7.4642e-15
 6.5172e-03
 2.4023e-09
 6.1206e-03
 5.2723e-03
 1.3445e-02
 1.6188e-08
 1.0362e-01
 3.1702e-03
 2.3657e-02
 1.9241e-03
 8.9042e-03
 1.9581e-02
 9.4323e-03
 2.9710e-03
 1.9457e-02
 2.2025e-03
 4.6936e-02
 4.8645e-03
 9.2705e-03
 2.8493e-03
 1.7232e-02
 2.2926e-03
 1.1453e-02
 3.7617e-03
 1.8214e-02
 4.4228e-03
 4.5857e-03
 3.6493e-03
 2.5725e-02
 5.7257e-03
 3.3629e-02
 3.0019e-03
 1.8503e-02
 3.1996e-02
 1.7278e-02
 2.4133e-08
 2.9067e-03
 8.1285e-12
 2.0854e-02
 4.5833e-03
 3.0074e-02
 1.0078e-02
 5.9793e-03
 4.0091e-03
 1.2117e-02
 9.8222e-03
 1.5522e-02
 4.5687e-03
 1.3942e-02
 2.8020e-02
 3.2455e-02
 1.9101e-03
 7.4958e-03
 6.3036e-03
 3.6653e-03
 2.4179e-02
 7.1091e-07
 2.2181e-03
 4.3700e-03
 4.3950e-03
 9.1485e-03
 4.8897e-03
 8.7155e-09
 1.4564e-02
 7.7997e-07
 6.3801e-03
 6.2402e-03
 1.6012e-03
 6.3840e-03
 6.5752e-03
 9.2417e-03
 1.1504e-03
 8.8859e-03
 1.3413e-02
 2.5858e-02
 3.4806e-03
 1.5273e-10
 1.8842e-13
 4.1798e-13
 3.3147e-02
 7.8465e-03
 3.2308e-03
 4.3789e-02
 5.0797e-13
 1.7779e-02
 3.6399e-03
 2.2804e-02
 8.2515e-03
 3.4121e-03
 2.1163e-02
 2.6152e-15
 4.2212e-03
 1.7100e-02
 1.4024e-02
 1.4115e-02
 1.3916e-02
 4.7258e-03
 7.4184e-13
 2.2543e-02
 3.7252e-03
 9.3726e-03
 4.7009e-03
 2.1826e-02
 2.5357e-03
 1.0190e-02
 3.4384e-02
 8.8646e-03
 2.3762e-02
 1.4324e-03
 5.2127e-02
 2.1836e-02
 8.0719e-15
 2.0702e-02
 5.3482e-03
 5.8868e-02
 2.7604e-02
 9.5645e-03
 1.7868e-02
 1.9605e-02
 4.3649e-02
 1.0999e-02
 1.0974e-02
 3.5751e-02
 1.5287e-02
 3.5044e-03
 1.0490e-02
 1.8927e-02
 6.4130e-03
 2.2450e-02
 1.6944e-09
 1.9783e-02
 6.1367e-03
 2.9761e-03
 2.0611e-02
 8.9193e-03
 2.3383e-11
 2.3595e-02
 2.2223e-02
 1.7645e-02
 2.6349e-03
 7.0963e-03
 5.4327e-03
 8.0926e-03
 1.4202e-03
 2.1224e-02
 8.1757e-04
 7.8315e-03
 2.1359e-02
 9.4516e-03
 6.8646e-03
 4.1933e-03
 5.3542e-09
 3.5096e-02
 2.7826e-02
 6.0440e-03
 2.6647e-03
 2.0105e-03
 1.9478e-02
 2.3951e-03
 1.2646e-02
 9.2039e-02
 2.4127e-02
 7.5323e-03
 1.2498e-02
 7.0563e-03
 1.7617e-12
 3.7749e-02
 9.3032e-04
 1.7186e-02
 1.1770e-02
 1.3943e-02
 2.3572e-02
 8.3729e-03
 6.1892e-03
 8.7686e-13
 8.8354e-03
 5.5419e-03
 2.8705e-02
 1.5633e-14
 1.0221e-02
 1.1354e-03
 4.0703e-03
 8.5144e-04
 2.6613e-02
 2.4011e-02
 5.7864e-03
 2.0228e-03
 4.0245e-03
 1.7220e-02
 9.0503e-03
 2.6327e-03
 2.0577e-12
 6.5009e-03
 1.5763e-02
 6.0606e-03
 5.4066e-11
 4.0180e-03
 5.3474e-03
 3.4693e-11
 2.3304e-02
 2.7453e-03
 1.6495e-03
 8.7636e-03
 7.3835e-03
 3.5527e-03
 3.3619e-03
 3.0092e-02
 1.3225e-02
 8.3331e-02
 9.6878e-03
 7.5722e-03
 2.8934e-12
 8.4043e-03
 2.0521e-02
 2.6827e-08
 2.5441e-03
 2.8070e-12
 6.8590e-14
 4.1061e-03
 1.3632e-05
 2.6081e-02
 4.9057e-03
 3.5429e-03
 7.1011e-02
 4.7807e-02
 2.5380e-03
 3.2253e-03
 7.2508e-04
 2.1001e-02
 6.3115e-02
 7.8972e-03
 3.7168e-04
 9.5838e-14
 1.6967e-02
 3.8489e-03
 5.7362e-03
 8.1960e-02
 1.0275e-14
 2.1296e-02
 1.4288e-02
 9.6627e-03
 1.9082e-02
 3.3890e-03
 1.1069e-02
 1.3024e-03
 5.5374e-03
 1.3044e-02
 9.5380e-03
 9.3789e-03
 5.5160e-03
 5.9639e-03
 1.3278e-10
 1.4895e-02
 2.4861e-02
 1.7949e-02
 2.2786e-03
 4.3113e-03
 1.2969e-02
 7.8214e-03
 3.6102e-02
 1.0266e-02
 1.0602e-02
 1.0799e-02
 1.1773e-15
 9.7357e-03
 1.1098e-02
 6.1807e-03
 2.6687e-03
 7.3969e-03
 5.9233e-03
 2.3250e-02
 1.6347e-02
 8.3158e-03
 1.0457e-02
 1.5360e-13
 1.0642e-02
 4.1531e-02
[torch.FloatTensor of size 512]
), ('layer2.1.conv1.weight', 
( 0 , 0 ,.,.) = 
  2.9981e-02

( 0 , 1 ,.,.) = 
  6.2468e-03

( 0 , 2 ,.,.) = 
 -7.8216e-04
    ... 

( 0 ,509,.,.) = 
 -4.9786e-09

( 0 ,510,.,.) = 
 -9.1469e-03

( 0 ,511,.,.) = 
  5.1793e-02
      ⋮  

( 1 , 0 ,.,.) = 
 -5.8691e-03

( 1 , 1 ,.,.) = 
 -6.9917e-03

( 1 , 2 ,.,.) = 
 -4.6123e-04
    ... 

( 1 ,509,.,.) = 
  3.6587e-09

( 1 ,510,.,.) = 
  2.9243e-03

( 1 ,511,.,.) = 
 -7.9964e-02
      ⋮  

( 2 , 0 ,.,.) = 
 -5.3589e-02

( 2 , 1 ,.,.) = 
 -4.0799e-03

( 2 , 2 ,.,.) = 
  2.9705e-04
    ... 

( 2 ,509,.,.) = 
 -1.7033e-08

( 2 ,510,.,.) = 
  2.0213e-03

( 2 ,511,.,.) = 
 -3.2253e-02
...     
      ⋮  

(125, 0 ,.,.) = 
  2.9383e-02

(125, 1 ,.,.) = 
  8.8350e-03

(125, 2 ,.,.) = 
  1.5486e-03
    ... 

(125,509,.,.) = 
  5.4477e-08

(125,510,.,.) = 
  3.4521e-03

(125,511,.,.) = 
 -9.2993e-04
      ⋮  

(126, 0 ,.,.) = 
 -4.2658e-02

(126, 1 ,.,.) = 
 -1.7373e-03

(126, 2 ,.,.) = 
 -3.9745e-04
    ... 

(126,509,.,.) = 
  3.8903e-09

(126,510,.,.) = 
  1.6505e-03

(126,511,.,.) = 
  1.7884e-02
      ⋮  

(127, 0 ,.,.) = 
  1.1154e-02

(127, 1 ,.,.) = 
  2.1871e-02

(127, 2 ,.,.) = 
  2.3255e-04
    ... 

(127,509,.,.) = 
  2.5520e-08

(127,510,.,.) = 
  7.0989e-04

(127,511,.,.) = 
  2.7331e-02
[torch.FloatTensor of size 128x512x1x1]
), ('layer2.1.bn1.weight', 
 0.1143
 0.0757
 0.0914
 0.1120
 0.1156
 0.1359
 0.0838
 0.1183
 0.1306
 0.0885
 0.1084
 0.1306
 0.1053
 0.1468
 0.0738
 0.1036
 0.0873
 0.1163
 0.0768
 0.0728
 0.1531
 0.0943
 0.1124
 0.1238
 0.2047
 0.0920
 0.1435
 0.1362
 0.1109
 0.2188
 0.0865
 0.1191
 0.0889
 0.1052
 0.1366
 0.1328
 0.1251
 0.1528
 0.2020
 0.1533
 0.1039
 0.1004
 0.0843
 0.1067
 0.1523
 0.1989
 0.0936
 0.0802
 0.1393
 0.1333
 0.1425
 0.1023
 0.1807
 0.1137
 0.0921
 0.0990
 0.1294
 0.0921
 0.1386
 0.1249
 0.1101
 0.1374
 0.1408
 0.0873
 0.1088
 0.0506
 0.0885
 0.1001
 0.0911
 0.0805
 0.1002
 0.1057
 0.1325
 0.1071
 0.0992
 0.1108
 0.1346
 0.1271
 0.0677
 0.1194
 0.1223
 0.1385
 0.1079
 0.1060
 0.0611
 0.1348
 0.1082
 0.1542
 0.0887
 0.1154
 0.1260
 0.1264
 0.1798
 0.1189
 0.0979
 0.0981
 0.0791
 0.1435
 0.1183
 0.1051
 0.1222
 0.1360
 0.1385
 0.1431
 0.1288
 0.1498
 0.1085
 0.1627
 0.0939
 0.1143
 0.1391
 0.0636
 0.0967
 0.1255
 0.1151
 0.0964
 0.0787
 0.1066
 0.1409
 0.1014
 0.1143
 0.0863
 0.1536
 0.0985
 0.1438
 0.0895
 0.1469
 0.1272
[torch.FloatTensor of size 128]
), ('layer2.1.bn1.bias', 
 0.0686
 0.1470
 0.0829
 0.0411
-0.0440
 0.0552
 0.1307
 0.0005
-0.0526
 0.2225
 0.2307
 0.3079
 0.0709
-0.0795
 0.1070
 0.0083
 0.0788
 0.0533
 0.0719
 0.0970
-0.0256
 0.1390
 0.0283
-0.0570
-0.0228
 0.1985
-0.0677
-0.0349
 0.0034
-0.1867
 0.0021
 0.0498
 0.1800
 0.0632
-0.0761
 0.2518
-0.0244
-0.0781
-0.1060
-0.0043
 0.0391
 0.0254
 0.0216
-0.0272
-0.0158
-0.1762
 0.1927
 0.1973
-0.0726
-0.0281
-0.0334
 0.2211
-0.0630
-0.0096
-0.0309
 0.0570
-0.0485
 0.1767
-0.0505
-0.0077
 0.0352
-0.0459
-0.0976
 0.0176
 0.1520
 0.0758
 0.1444
-0.0143
 0.2364
 0.0166
 0.0716
 0.0189
-0.0140
 0.1136
-0.0366
 0.0640
 0.1607
-0.0204
 0.0636
 0.0286
 0.0240
-0.0004
 0.0181
 0.0243
 0.0977
-0.0513
 0.0864
-0.0497
 0.1234
 0.1664
-0.0528
-0.0132
 0.3854
 0.1145
 0.0951
 0.1781
 0.0190
 0.0003
 0.0605
-0.0305
-0.0159
 0.0241
-0.0535
-0.0766
 0.3663
-0.0140
 0.0631
-0.0458
 0.0234
 0.0123
-0.0590
 0.0549
 0.2084
 0.0515
 0.0630
-0.0188
 0.0648
 0.0512
-0.0948
 0.0446
 0.0229
 0.0669
-0.0550
 0.0246
-0.0370
 0.2319
 0.2566
-0.0807
[torch.FloatTensor of size 128]
), ('layer2.1.bn1.running_mean', 
 0.2086
-0.0666
-0.0530
-0.1282
-0.0748
-0.0268
 0.1286
 0.0700
-0.0923
 0.1373
-0.1122
-0.2085
 0.0983
 0.0411
-0.0669
-0.0724
 0.1377
-0.1665
-0.0032
 0.1078
-0.0101
 0.0013
 0.0398
-0.0505
-0.1409
 0.0642
-0.0109
-0.0298
-0.1177
-0.0630
-0.0024
-0.0311
 0.1356
 0.0616
-0.0216
 0.2213
-0.1301
 0.1984
-0.0083
-0.1262
-0.0158
 0.0411
 0.0878
 0.0708
 0.0827
-0.1713
-0.0548
-0.0849
 0.0605
 0.0613
 0.0124
-0.0927
 0.0122
-0.0425
-0.0120
 0.0256
-0.0609
 0.1304
-0.0986
 0.0847
-0.0925
-0.1878
 0.0532
-0.0116
 0.1313
 0.0110
 0.1688
 0.0500
-0.1075
-0.0402
 0.0836
-0.0149
 0.0307
 0.2523
-0.0152
 0.0549
 0.0771
-0.0535
-0.0347
-0.0685
-0.0140
-0.0993
-0.0798
-0.1198
 0.0339
-0.0669
-0.0613
-0.0209
-0.0533
-0.0325
 0.0252
 0.1560
 0.3068
-0.0359
 0.0625
-0.0673
 0.0603
 0.0541
 0.1743
 0.0121
 0.0732
-0.0335
 0.0054
 0.0615
-0.3226
 0.0400
 0.0454
-0.0098
 0.0061
-0.1311
-0.2091
-0.0919
-0.3018
 0.0422
-0.0503
 0.1460
-0.0487
 0.0149
 0.1320
 0.0870
-0.0398
 0.0076
 0.0314
-0.0137
-0.0086
-0.1474
 0.1370
 0.0248
[torch.FloatTensor of size 128]
), ('layer2.1.bn1.running_var', 
 0.1164
 0.0414
 0.0690
 0.0215
 0.0145
 0.0424
 0.0845
 0.0533
 0.0101
 0.0676
 0.0347
 0.1723
 0.0593
 0.0289
 0.0284
 0.0186
 0.0246
 0.0218
 0.0164
 0.0299
 0.0174
 0.0670
 0.0248
 0.0109
 0.1002
 0.1199
 0.0108
 0.0060
 0.0290
 0.0396
 0.0138
 0.0408
 0.1157
 0.0657
 0.0204
 0.2652
 0.0194
 0.0208
 0.0349
 0.0339
 0.0361
 0.0232
 0.0079
 0.0154
 0.0148
 0.0195
 0.0729
 0.0513
 0.0127
 0.0244
 0.0198
 0.0487
 0.0222
 0.0085
 0.0142
 0.0471
 0.0375
 0.1202
 0.0261
 0.0280
 0.0247
 0.0250
 0.0264
 0.0139
 0.0370
 0.0154
 0.1097
 0.0225
 0.1203
 0.0165
 0.0570
 0.0218
 0.0228
 0.0428
 0.0089
 0.0348
 0.0341
 0.0512
 0.0209
 0.0457
 0.0344
 0.0567
 0.0184
 0.0201
 0.0223
 0.0182
 0.0273
 0.0192
 0.0371
 0.0895
 0.0131
 0.0193
 0.4243
 0.0214
 0.0591
 0.0953
 0.0174
 0.0279
 0.0545
 0.0021
 0.0127
 0.0412
 0.0440
 0.0251
 0.2988
 0.0153
 0.0681
 0.0151
 0.0233
 0.0326
 0.0204
 0.0262
 0.0499
 0.0873
 0.0369
 0.0135
 0.0461
 0.0134
 0.0334
 0.0295
 0.0168
 0.0184
 0.0173
 0.0425
 0.0194
 0.1112
 0.1536
 0.0151
[torch.FloatTensor of size 128]
), ('layer2.1.conv2.weight', 
( 0 , 0 ,.,.) = 
 -2.6612e-02 -4.0164e-02 -1.7479e-02
 -3.1620e-02 -5.3283e-03 -2.7553e-02
 -2.4886e-02 -3.2047e-02 -2.3468e-02

( 0 , 1 ,.,.) = 
 -3.5126e-03  3.2575e-03 -8.1418e-03
 -7.0056e-03  6.0254e-03 -1.0571e-02
 -6.0508e-03  7.2130e-04 -1.5162e-03

( 0 , 2 ,.,.) = 
  7.6138e-03  1.8244e-02  1.2507e-02
  2.5248e-02 -3.7504e-02  2.4530e-02
  1.5743e-02  2.3739e-02  1.7725e-02
    ... 

( 0 ,125,.,.) = 
  4.3778e-03  3.5878e-03  7.4590e-04
  4.4968e-03 -1.8396e-02  3.6347e-05
  5.4960e-03  3.2917e-03 -2.2544e-03

( 0 ,126,.,.) = 
 -6.0618e-03  1.2152e-02  1.0627e-03
  8.1596e-03  6.4134e-03  9.3486e-03
  3.8096e-03  1.8635e-02  5.5144e-03

( 0 ,127,.,.) = 
  1.5398e-02  1.1400e-02  1.6990e-02
  6.8940e-03 -2.7925e-02  4.4624e-03
  1.4584e-02  9.9359e-03  1.0279e-02
      ⋮  

( 1 , 0 ,.,.) = 
  1.4447e-02  1.0540e-02  8.6184e-03
  1.4049e-02 -5.1148e-02  1.1761e-02
  6.0427e-03  1.3258e-02  1.6813e-02

( 1 , 1 ,.,.) = 
 -9.7942e-04  1.8536e-03  1.2940e-04
  4.5484e-03  6.3970e-03 -5.7844e-03
 -5.2112e-03 -9.8360e-03 -7.9912e-03

( 1 , 2 ,.,.) = 
 -6.5372e-03 -3.0737e-03 -6.6110e-04
 -8.3917e-03 -4.3665e-03 -8.8227e-03
 -7.9809e-03 -9.1811e-03 -9.4304e-03
    ... 

( 1 ,125,.,.) = 
 -3.0811e-03  4.5446e-03 -2.2183e-03
  1.3118e-02 -2.3592e-02  1.2510e-02
 -1.3352e-03  1.6423e-02  1.7204e-03

( 1 ,126,.,.) = 
 -3.9355e-03 -1.2400e-02 -5.3888e-03
 -2.3812e-02  6.2618e-02 -3.1402e-02
 -1.5209e-02 -3.6339e-02 -2.3629e-02

( 1 ,127,.,.) = 
  1.1858e-02  1.5748e-02  5.0950e-03
  8.5850e-03 -4.5303e-02  5.7861e-03
  2.5232e-03  1.6618e-02  1.0268e-03
      ⋮  

( 2 , 0 ,.,.) = 
  1.1693e-04 -2.3821e-02 -1.0894e-02
 -1.1472e-02 -3.5137e-03 -1.0219e-02
  1.3313e-02  3.7218e-02  1.6308e-02

( 2 , 1 ,.,.) = 
  5.9613e-03  2.5387e-02  1.8554e-02
 -1.4240e-02 -1.1972e-02 -1.0432e-02
 -6.0892e-04  5.4653e-03  6.0121e-03

( 2 , 2 ,.,.) = 
  1.3706e-02  1.8274e-02  9.4376e-03
  1.8834e-03 -6.9833e-03 -7.1998e-03
 -1.0752e-02 -1.4026e-02 -1.2343e-02
    ... 

( 2 ,125,.,.) = 
 -3.5476e-02 -1.1280e-01 -5.9571e-02
 -2.7227e-02  8.1714e-03 -2.0615e-02
  4.8816e-02  1.4239e-01  8.7312e-02

( 2 ,126,.,.) = 
  7.7823e-02  2.1489e-01  1.1521e-01
  9.3477e-03 -2.4879e-02 -1.0115e-02
 -4.7276e-02 -1.6804e-01 -1.0835e-01

( 2 ,127,.,.) = 
 -2.4170e-02 -4.6353e-02 -2.7198e-02
 -1.6167e-02  5.8170e-03 -1.1249e-02
  1.6357e-02  5.5004e-02  3.2485e-02
...     
      ⋮  

(125, 0 ,.,.) = 
 -1.6305e-02 -5.0020e-02 -1.5351e-02
 -1.2441e-02  5.3947e-02 -6.3890e-03
 -2.4207e-03  2.0496e-02 -3.4866e-03

(125, 1 ,.,.) = 
  3.4448e-03  9.0776e-03  3.1396e-03
 -1.2736e-02  2.3239e-02 -1.0691e-02
 -1.9581e-02  1.0343e-03 -1.1689e-02

(125, 2 ,.,.) = 
  2.0536e-02  7.1731e-02  4.3848e-02
 -1.4356e-02 -6.6640e-02 -4.0626e-03
 -1.6326e-02 -4.7985e-02 -1.0681e-02
    ... 

(125,125,.,.) = 
  3.3409e-02  8.4426e-02  4.1283e-02
 -1.0366e-02 -6.7267e-02 -8.4304e-03
 -1.3965e-02 -3.6488e-02 -1.7230e-02

(125,126,.,.) = 
  2.5791e-02  3.6760e-02  2.9285e-02
  2.7356e-03  3.1673e-02  2.8453e-03
 -1.0855e-02  1.4385e-02  5.6611e-04

(125,127,.,.) = 
  1.0187e-02  1.0588e-02  1.7188e-02
 -3.0925e-03 -3.9582e-02  6.1720e-03
 -5.8177e-03 -1.0946e-02  6.8252e-04
      ⋮  

(126, 0 ,.,.) = 
 -1.2185e-02 -1.3895e-02 -9.7944e-03
 -1.6095e-02  2.2727e-02  7.0365e-04
 -1.6365e-02 -6.8739e-03 -7.4789e-03

(126, 1 ,.,.) = 
 -6.2751e-03 -8.8369e-03 -1.0088e-02
  4.7107e-03  5.9371e-03 -7.3752e-04
  1.7115e-03  3.0230e-03 -8.1881e-03

(126, 2 ,.,.) = 
 -2.2295e-03 -2.0647e-03 -1.7058e-03
  1.4152e-03 -1.7090e-02 -9.4503e-03
 -2.1117e-03 -7.5780e-03 -7.4742e-03
    ... 

(126,125,.,.) = 
  3.1252e-03  1.0974e-02 -8.7916e-04
  9.9409e-03 -2.5935e-02  1.6085e-03
  1.0177e-03  3.6322e-03  5.1371e-04

(126,126,.,.) = 
  9.1652e-03  4.5619e-03 -2.2218e-04
  6.9943e-03  1.6824e-02 -1.0487e-02
  9.4374e-04 -1.5150e-02 -1.9611e-02

(126,127,.,.) = 
 -1.6774e-03  6.8204e-03  3.9385e-03
 -4.7882e-04 -3.7668e-03  4.3544e-03
 -1.0460e-03  7.8029e-03  3.3921e-03
      ⋮  

(127, 0 ,.,.) = 
  1.0283e-02  5.7515e-03 -1.3164e-02
  1.0537e-02 -4.6643e-03 -2.3646e-02
  1.8466e-02 -9.4791e-04 -1.6401e-02

(127, 1 ,.,.) = 
  8.7659e-03 -1.0505e-02 -4.4822e-03
  1.2671e-04  4.8271e-03 -1.0532e-02
  3.8525e-03 -1.5733e-02 -9.0469e-03

(127, 2 ,.,.) = 
  3.0490e-04 -8.7700e-03 -2.2215e-03
  1.9254e-02  2.9673e-03 -1.2005e-02
  1.3131e-03 -3.0526e-04 -9.1385e-03
    ... 

(127,125,.,.) = 
 -1.6899e-03 -1.5691e-02  1.6644e-02
  1.1676e-02 -8.2631e-03  2.4822e-02
 -1.1040e-02 -1.7921e-02  1.1375e-02

(127,126,.,.) = 
  1.7830e-03 -1.2158e-02  1.1729e-02
  7.1818e-03  6.8397e-04  2.2245e-02
 -1.5285e-03 -9.7472e-03  1.3757e-02

(127,127,.,.) = 
  9.1870e-03  1.2355e-02 -4.5135e-03
  8.5448e-03 -7.1429e-04 -2.3917e-02
  3.4502e-03  5.6828e-03 -1.2090e-02
[torch.FloatTensor of size 128x128x3x3]
), ('layer2.1.bn2.weight', 
 0.1776
 0.1253
 0.1823
 0.1996
 0.1886
 0.1409
 0.1549
 0.1246
 0.0863
 0.2138
 0.1544
 0.1554
 0.1665
 0.1341
 0.1169
 0.1823
 0.1384
 0.1631
 0.1488
 0.1989
 0.1235
 0.1653
 0.2177
 0.1262
 0.1818
 0.1841
 0.1535
 0.1297
 0.1605
 0.2361
 0.2205
 0.2062
 0.0967
 0.2203
 0.1388
 0.2097
 0.1593
 0.1837
 0.0797
 0.1574
 0.1405
 0.1758
 0.1517
 0.1396
 0.1525
 0.1645
 0.1626
 0.2054
 0.1696
 0.1582
 0.1589
 0.1655
 0.1751
 0.1953
 0.1255
 0.1181
 0.1275
 0.1586
 0.1367
 0.1509
 0.1663
 0.2498
 0.1959
 0.1912
 0.1517
 0.1599
 0.2867
 0.1220
 0.2525
 0.1890
 0.1916
 0.1373
 0.1461
 0.1639
 0.1563
 0.1762
 0.1324
 0.1051
 0.1793
 0.1718
 0.1759
 0.1436
 0.1498
 0.1712
 0.0930
 0.1331
 0.1355
 0.2007
 0.1051
 0.1681
 0.1448
 0.1426
 0.1590
 0.2092
 0.1544
 0.1967
 0.1247
 0.1491
 0.1739
 0.1149
 0.1266
 0.1627
 0.1850
 0.1815
 0.1988
 0.2299
 0.1590
 0.0869
 0.1557
 0.1388
 0.1757
 0.1473
 0.1639
 0.1859
 0.1761
 0.1901
 0.1914
 0.1486
 0.1887
 0.1612
 0.1703
 0.2223
 0.1447
 0.2026
 0.1558
 0.1597
 0.1068
 0.2155
[torch.FloatTensor of size 128]
), ('layer2.1.bn2.bias', 
-0.0184
 0.0471
 0.0683
 0.1018
-0.0076
 0.0603
 0.0339
 0.0681
 0.0400
-0.1420
-0.0781
-0.0290
 0.1200
 0.0270
-0.0339
 0.0416
 0.0379
-0.1181
-0.0939
 0.0850
-0.0848
 0.1726
 0.0771
 0.0163
 0.0683
 0.0188
-0.0476
-0.0065
-0.0019
-0.1350
-0.1825
 0.0421
 0.0739
 0.0721
-0.0071
 0.0286
-0.0353
 0.0735
-0.0379
 0.0404
 0.0831
 0.1374
-0.0702
 0.1316
-0.0518
 0.0653
-0.0995
-0.0413
-0.1382
 0.1500
-0.1576
 0.1678
 0.0668
 0.0634
 0.0213
-0.0333
 0.0005
-0.0038
 0.0535
-0.0281
 0.0792
-0.1331
-0.1468
-0.0365
 0.0778
 0.0568
-0.1759
 0.0385
-0.1446
 0.0476
-0.1166
 0.0506
 0.0090
 0.0531
 0.0614
-0.0041
-0.0743
 0.0226
 0.0601
 0.0353
 0.0816
 0.0257
 0.0414
 0.0172
-0.0382
-0.1142
 0.0041
 0.0522
 0.0693
 0.0740
-0.0474
-0.0203
 0.0571
-0.1367
-0.0745
 0.0414
 0.0353
 0.0861
-0.0493
-0.0339
 0.0198
-0.0254
-0.0596
 0.1878
 0.0945
-0.1261
 0.0603
 0.0611
 0.0336
-0.0185
 0.1517
-0.0080
-0.0808
-0.1123
-0.0406
-0.0137
 0.1976
-0.0426
 0.0726
-0.0580
-0.0975
-0.1347
-0.0368
-0.0499
 0.0638
 0.0652
 0.0226
-0.0461
[torch.FloatTensor of size 128]
), ('layer2.1.bn2.running_mean', 
-0.0752
-0.1081
 0.1642
 0.1943
-0.2520
 0.0986
 0.1454
 0.0933
-0.1598
-0.0986
-0.0606
-0.0150
 0.0787
 0.0837
-0.0499
 0.1017
-0.0182
-0.0466
-0.1441
 0.1709
 0.0108
 0.2625
 0.2503
 0.0454
 0.2866
 0.0966
-0.0590
-0.1815
 0.0051
-0.1488
-0.1823
 0.2590
-0.2630
 0.2619
-0.1825
-0.2558
 0.0639
 0.1824
-0.1255
-0.1643
 0.0922
 0.3631
-0.0917
 0.2359
-0.0278
 0.1786
-0.0535
-0.0572
-0.1316
 0.2967
-0.1782
 0.1825
 0.1370
 0.3127
-0.0150
-0.0635
-0.0210
 0.0821
 0.1398
 0.0223
 0.1019
-0.1566
-0.1338
-0.0749
 0.0808
 0.0964
-0.6256
 0.1636
-0.6699
 0.1828
-0.0856
-0.1144
 0.0109
 0.0342
 0.1588
-0.0038
-0.0500
 0.0046
 0.1071
 0.2366
 0.3162
 0.1224
-0.0026
 0.0644
-0.0699
 0.0811
-0.1747
 0.1155
 0.0113
 0.1497
-0.0232
-0.0136
 0.2352
-0.0985
-0.1462
 0.3078
 0.0403
 0.2119
 0.0118
-0.0370
-0.0161
-0.0786
-0.1469
 0.3663
 0.3038
-0.1605
 0.1178
-0.1301
 0.1076
 0.0424
 0.1953
-0.1530
-0.0658
-0.4419
-0.1852
 0.0115
 0.4790
-0.0509
 0.3086
-0.1257
-0.0454
-0.0702
-0.1706
-0.0163
 0.0976
 0.1558
-0.0990
 0.0370
[torch.FloatTensor of size 128]
), ('layer2.1.bn2.running_var', 
1.00000e-02 *
  3.7211
  1.1833
  5.3567
  4.0570
  2.0210
  1.5370
  1.8205
  1.7006
  1.3944
  2.6442
  1.3427
  0.8050
  2.2434
  1.3065
  1.3053
  4.1369
  1.5325
  0.7822
  0.7526
  4.8610
  0.4828
  1.8561
  5.7047
  0.5622
  3.7765
  2.9338
  1.1594
  2.4279
  1.1391
  1.9654
  4.5807
  7.1333
  1.1624
  5.4109
  1.7250
  2.8444
  0.6243
  4.8083
  0.2813
  0.6943
  1.0415
  2.4677
  1.1698
  1.7683
  0.5548
  2.4331
  1.3173
  1.3977
  1.8746
  2.0362
  0.6475
  2.0428
  2.9583
  4.2586
  0.9769
  1.4297
  1.1411
  1.6497
  1.5511
  2.0556
  4.6570
  1.6105
  0.8773
  0.6140
  2.2147
  3.7355
  2.0431
  1.6343
  2.0748
  3.9656
  2.5288
  1.1450
  1.1641
  3.6595
  2.7669
  1.2773
  1.0398
  0.9329
  4.1113
  4.7218
  5.0362
  0.9552
  1.2542
  2.1106
  0.3682
  1.0592
  1.1032
  4.8024
  1.3383
  3.8641
  0.8581
  1.1333
  4.3961
  3.0476
  0.7400
  5.9239
  1.9848
  1.5870
  0.8938
  1.0560
  3.7643
  1.1449
  1.0650
  2.2919
  3.2916
  1.6592
  3.4310
  1.1715
  1.0385
  0.7004
  2.0338
  0.7819
  1.1024
  1.0584
  1.1418
  1.1848
  2.4373
  1.0418
  4.1256
  4.0440
  0.9092
  4.4235
  3.2782
  0.9535
  2.6355
  2.6577
  0.9757
  1.1905
[torch.FloatTensor of size 128]
), ('layer2.1.conv3.weight', 
( 0 , 0 ,.,.) = 
 -4.4146e-02

( 0 , 1 ,.,.) = 
  2.0839e-02

( 0 , 2 ,.,.) = 
 -3.6888e-03
    ... 

( 0 ,125,.,.) = 
 -6.1172e-03

( 0 ,126,.,.) = 
 -1.4476e-02

( 0 ,127,.,.) = 
  2.2692e-02
      ⋮  

( 1 , 0 ,.,.) = 
  6.6173e-04

( 1 , 1 ,.,.) = 
  2.4633e-04

( 1 , 2 ,.,.) = 
  4.5467e-03
    ... 

( 1 ,125,.,.) = 
 -1.3392e-03

( 1 ,126,.,.) = 
  5.3658e-03

( 1 ,127,.,.) = 
 -9.5005e-03
      ⋮  

( 2 , 0 ,.,.) = 
 -2.4932e-02

( 2 , 1 ,.,.) = 
 -5.1053e-03

( 2 , 2 ,.,.) = 
 -1.6897e-01
    ... 

( 2 ,125,.,.) = 
 -1.2921e-04

( 2 ,126,.,.) = 
  4.2860e-04

( 2 ,127,.,.) = 
  9.3305e-03
...     
      ⋮  

(509, 0 ,.,.) = 
  1.2192e-03

(509, 1 ,.,.) = 
 -3.8684e-02

(509, 2 ,.,.) = 
 -1.5341e-02
    ... 

(509,125,.,.) = 
  4.2530e-02

(509,126,.,.) = 
  4.9127e-02

(509,127,.,.) = 
  9.1526e-02
      ⋮  

(510, 0 ,.,.) = 
  4.1313e-04

(510, 1 ,.,.) = 
 -2.9892e-04

(510, 2 ,.,.) = 
 -1.2321e-02
    ... 

(510,125,.,.) = 
  9.9619e-03

(510,126,.,.) = 
  5.1484e-04

(510,127,.,.) = 
  1.3669e-02
      ⋮  

(511, 0 ,.,.) = 
  6.6747e-02

(511, 1 ,.,.) = 
 -7.0881e-03

(511, 2 ,.,.) = 
  4.9484e-03
    ... 

(511,125,.,.) = 
  2.4755e-02

(511,126,.,.) = 
  4.9751e-02

(511,127,.,.) = 
  1.0807e-02
[torch.FloatTensor of size 512x128x1x1]
), ('layer2.1.bn3.weight', 
 1.3242e-01
-3.9867e-03
 2.3998e-01
 2.0384e-02
 1.4137e-01
 1.8754e-01
 1.0087e-03
 2.8137e-01
 1.3598e-01
 1.9805e-01
 2.3073e-01
 2.4595e-02
 8.5594e-02
 1.2895e-01
 2.5740e-01
 1.2544e-02
-1.3376e-03
 1.0062e-01
 1.0290e-01
 3.5137e-02
 9.0440e-04
 1.8675e-02
-2.8032e-03
 2.5723e-01
-5.0090e-03
 1.6177e-01
 1.7371e-01
 8.2233e-03
 2.3130e-01
 1.0041e-03
 1.4092e-02
 3.2567e-03
 2.4108e-01
 5.3649e-02
 6.5720e-02
-8.4551e-05
-7.9860e-04
 5.1282e-03
-5.4507e-04
 8.2625e-03
 3.5324e-03
 8.1854e-03
 2.1627e-01
 1.1899e-01
 2.4078e-01
 1.1633e-02
 1.3839e-01
 5.9934e-03
-7.7509e-04
 5.1584e-02
 2.2356e-01
 8.2780e-05
-2.9549e-04
-2.1997e-02
 1.7908e-01
-2.0951e-03
 1.7883e-02
 5.2630e-02
 1.6310e-03
 1.1751e-01
 2.3309e-01
 2.9326e-02
-7.3030e-04
 2.7353e-03
 3.9904e-02
 3.7391e-02
 3.6750e-02
 5.0586e-03
 3.3143e-05
-4.6347e-03
 1.7130e-02
-9.5308e-03
 3.3185e-02
 2.1624e-02
 4.4300e-03
 6.1887e-02
-3.2336e-02
-3.5629e-03
 7.5005e-02
 3.2207e-02
 2.4232e-02
 1.4763e-03
 2.6425e-01
 9.5805e-04
-4.7396e-03
 3.3679e-03
 2.6861e-02
 2.3871e-01
 2.2591e-01
 1.9161e-01
 2.5083e-01
 1.4579e-02
 1.3200e-02
 1.3942e-01
 2.5476e-01
 6.2012e-03
 2.5609e-03
 4.5695e-02
 2.8415e-02
 2.0069e-01
 7.2187e-02
 1.9426e-02
 1.9531e-01
-2.7378e-03
 1.8738e-01
 1.9436e-01
-2.4769e-03
 7.5864e-03
 6.1777e-04
 1.0982e-01
-6.4235e-03
 5.4644e-03
 1.1123e-01
 5.3514e-02
 8.3084e-02
 8.9328e-03
-6.0282e-03
 1.6342e-02
-2.2930e-04
 5.4757e-02
 4.6124e-02
 5.9824e-02
 9.6306e-02
 8.2323e-03
 1.5161e-02
 2.6155e-01
 1.1137e-03
-1.9300e-02
 9.7766e-02
 1.0992e-02
 6.4315e-03
-9.2604e-03
 3.2524e-02
 1.5190e-01
 1.5307e-02
 8.5114e-02
-2.3247e-03
-7.6336e-03
 2.7737e-01
 4.9809e-02
-3.6132e-02
 1.7670e-01
-1.0334e-02
 2.6541e-01
 1.7790e-01
 1.2973e-01
 1.2085e-01
 2.8980e-02
 6.1677e-03
 3.7799e-02
 3.7248e-03
 1.6790e-01
-2.9085e-03
 2.9735e-01
 1.7773e-01
 1.4563e-02
 1.4470e-01
 2.7867e-01
 6.5782e-03
 1.3230e-01
 1.9502e-02
 3.1345e-02
-1.8164e-03
 3.7131e-02
-4.8313e-03
 1.1612e-01
 3.3529e-02
 6.1328e-02
 7.6316e-04
 1.1034e-01
 2.3557e-01
 1.9592e-03
 8.2999e-03
 1.4917e-03
 1.2939e-01
-4.3517e-03
 1.7499e-01
 2.9556e-03
 1.3388e-01
 3.5357e-02
-5.1334e-02
 1.9123e-02
-8.4551e-03
 2.2817e-01
 2.4712e-02
 1.6226e-01
 2.7650e-01
 3.5893e-02
 1.9193e-01
 8.6777e-02
 2.5617e-02
 1.8742e-02
 7.9880e-02
 1.2931e-03
 2.8268e-01
 2.9270e-03
 2.5846e-01
-1.5192e-03
 2.4546e-01
 1.7163e-01
 1.7509e-06
 5.7166e-03
 5.7714e-03
 1.2100e-01
 2.7838e-01
 2.5758e-02
 1.2000e-01
 1.8500e-02
 7.8713e-03
-6.4103e-03
 2.6456e-02
-4.5954e-03
 8.0422e-03
 2.8520e-01
 2.3045e-01
 8.0622e-03
 8.1769e-03
-2.2261e-02
 2.9511e-01
-1.6241e-02
 1.5178e-01
 2.2163e-02
 2.5932e-01
 2.4696e-02
 4.4202e-03
 8.2374e-03
 6.2302e-03
 2.8438e-01
-6.9263e-03
-9.4133e-03
 8.9109e-03
-1.2923e-02
 2.7665e-01
 1.4762e-01
 2.7578e-01
 7.0166e-03
 2.8965e-01
 3.7330e-03
-3.8535e-03
 1.1965e-02
 2.5063e-01
 3.3793e-01
 1.5274e-02
 4.6105e-02
 1.1181e-02
-4.0527e-04
 9.5739e-02
 1.2638e-02
 8.6265e-04
 1.5912e-01
 4.7056e-03
 5.2157e-02
-5.2267e-04
-1.4312e-02
-1.8230e-04
 1.5991e-01
-5.0568e-03
 5.3907e-02
-6.1297e-03
 5.7207e-03
 4.0776e-02
 5.8019e-03
 2.9346e-03
 1.3783e-01
-2.0062e-04
 3.0749e-02
 6.0224e-03
 7.1440e-02
 4.3121e-02
 3.3668e-02
 1.6093e-01
 3.8541e-02
 1.8803e-01
 1.4929e-01
 1.0741e-02
 2.2264e-01
 1.0222e-01
 1.0543e-01
 1.1332e-02
 2.3256e-04
-7.7044e-03
 1.4923e-01
-3.1706e-03
 1.4994e-01
 2.1004e-01
 1.4621e-01
 7.5962e-03
-2.3061e-02
 1.8572e-02
-3.5922e-02
 1.2876e-01
 9.6549e-02
 6.5169e-03
 3.3788e-03
 2.3291e-03
-5.7405e-03
 3.7034e-03
 2.5774e-01
 1.8605e-01
 2.5783e-05
-2.5319e-02
 4.9742e-04
 1.2592e-01
 2.2677e-02
-8.8526e-03
 1.6606e-02
 3.4801e-02
 8.2358e-02
 2.8180e-02
 7.1734e-02
-1.1104e-02
 2.1437e-01
 2.9919e-01
 1.8770e-01
 2.4306e-01
 2.7936e-01
-9.2975e-03
 1.7140e-01
 3.0950e-01
 1.6311e-01
 1.3238e-02
-1.6860e-02
 7.9346e-03
 6.2691e-02
 3.1991e-05
 3.4025e-01
 7.0419e-02
 2.1516e-02
 1.7150e-02
 2.3581e-01
-8.3994e-03
 1.4497e-02
 3.3998e-01
 8.1498e-02
 1.3729e-02
 1.0834e-01
-8.9963e-04
 5.7597e-02
 7.8389e-03
 4.7418e-02
 1.7605e-01
 2.4862e-02
 2.1751e-01
 4.1433e-03
 2.3513e-01
 1.0048e-02
 2.5893e-01
 2.9535e-02
 2.1024e-02
-2.5114e-02
-1.1503e-03
 9.1575e-02
-6.3913e-03
 1.2965e-01
 2.5463e-01
 1.1006e-02
 2.6259e-02
 4.0359e-02
 2.9381e-02
 1.2051e-03
 1.4009e-01
-1.6837e-02
 9.6909e-02
 6.2494e-02
 2.2921e-01
-2.6400e-03
 9.0973e-03
-1.1027e-02
 1.7221e-02
-1.6165e-03
 2.6043e-01
 1.6975e-03
 4.6920e-04
 1.5349e-02
 3.5653e-03
 9.3198e-03
 5.3950e-02
 5.4591e-03
 4.0830e-02
 1.3520e-01
 9.0901e-03
-1.6302e-02
 1.6068e-01
-5.1534e-03
 9.0498e-03
-3.4975e-03
 2.5242e-01
 1.3406e-01
 1.1536e-01
-3.4113e-03
-1.5016e-03
-3.3296e-03
 1.1131e-01
 4.2994e-02
 3.2412e-02
 2.8762e-01
 1.4468e-01
 1.0058e-01
 8.6172e-04
 2.2942e-01
 3.3135e-01
 1.5762e-01
 4.4236e-03
 1.1463e-03
 1.2735e-01
 4.2306e-02
-2.2193e-02
 2.6118e-02
 9.4100e-02
 1.8414e-01
 1.0708e-02
-5.2921e-03
-2.3026e-03
 1.3227e-01
 2.6541e-01
-5.8521e-03
 8.9120e-02
 3.1245e-03
-5.1983e-02
 9.6008e-02
 3.4769e-02
 3.0342e-01
 1.5691e-02
 1.4275e-02
 9.4966e-02
 5.5804e-02
 2.9243e-01
 2.6348e-01
 3.3957e-02
 1.2843e-02
 2.8020e-01
 3.3936e-02
 7.5655e-03
 2.8599e-01
-7.0982e-02
 4.6806e-04
 1.9623e-03
 1.3098e-02
 1.1181e-02
-3.9819e-03
-4.8984e-03
 1.2586e-01
 1.1303e-01
 2.5781e-01
 6.4086e-02
 1.1150e-02
 2.5754e-01
 1.4200e-03
 2.2140e-03
 2.0586e-01
 3.9090e-01
-2.5558e-06
 2.3469e-01
 2.6226e-03
 2.6051e-01
 4.0117e-02
 1.1444e-03
 1.7378e-02
 2.4208e-01
 1.1203e-02
 5.3762e-03
-3.9318e-03
 2.7326e-02
-2.1225e-03
 2.2229e-01
 2.1667e-03
 1.7954e-02
 2.7006e-01
 5.7885e-02
-8.4252e-03
 2.1156e-02
 2.9087e-01
 2.9659e-01
 1.8163e-01
 2.8335e-03
-9.4529e-03
-2.8763e-02
 1.0219e-01
 3.3065e-03
-4.2810e-04
 4.9658e-02
-2.2690e-03
 1.5966e-01
-3.5709e-03
 1.3090e-02
 1.1020e-01
 2.8386e-01
-8.1983e-03
 1.8239e-01
-2.9931e-02
-4.2410e-03
 2.9046e-02
 1.7571e-01
 3.6939e-02
 1.0678e-01
 1.6147e-01
 1.0973e-02
 8.7048e-02
 2.5472e-01
-3.0390e-03
 1.5620e-01
 1.0042e-01
 2.5713e-03
 1.3052e-01
 6.2466e-03
 4.8908e-02
 4.2711e-02
 3.3092e-02
-1.1234e-02
 2.0025e-01
-6.9633e-03
 1.8601e-01
[torch.FloatTensor of size 512]
), ('layer2.1.bn3.bias', 
-1.4303e-01
-4.1359e-02
 9.6345e-02
-1.7250e-03
-6.3559e-02
 2.4898e-02
 4.3319e-03
-1.1981e-01
-9.8809e-02
 7.8204e-02
-6.3868e-02
-1.4853e-02
-2.2071e-02
-6.8391e-02
 1.3010e-02
-3.5912e-03
 5.3247e-03
-6.2942e-03
-2.5711e-02
-2.9042e-02
 1.2719e-03
-3.5715e-02
 1.1996e-02
 5.5386e-02
 1.5934e-02
-1.4835e-02
-8.4745e-02
 5.4219e-02
 3.1276e-02
 2.3295e-03
-2.9171e-02
-7.6703e-02
 7.9058e-03
-1.7027e-02
-9.7281e-02
-2.0484e-02
 1.1954e-02
 4.7291e-03
-7.3959e-03
 1.7732e-02
-9.0220e-03
 8.9617e-07
-4.1675e-02
-9.9250e-02
-3.8707e-02
-4.0530e-02
-1.3386e-01
-1.0893e-02
 1.1389e-03
-4.7458e-03
-6.2041e-02
-3.0088e-03
 4.0795e-02
 1.8738e-03
-1.9352e-01
-2.0363e-03
 3.3288e-02
-4.4849e-02
-3.5226e-03
 5.5271e-02
-7.4215e-02
-2.4861e-02
-1.0789e-02
-1.0903e-01
-1.6468e-02
-2.5583e-02
-1.5219e-01
 8.9196e-03
-3.7539e-04
 1.6317e-03
 1.2326e-03
-3.0860e-03
-2.7546e-02
-3.7632e-02
-8.8516e-03
-3.0653e-02
-5.9028e-03
 6.1626e-04
-8.1051e-02
-4.3922e-02
-9.5106e-03
-8.9856e-03
-3.1956e-02
-1.2650e-03
-1.0191e-02
-2.3311e-03
 4.3025e-04
 2.7109e-02
-4.6026e-02
-2.3735e-02
 1.5982e-02
-1.6071e-02
-2.7669e-02
-9.0962e-02
 3.4019e-02
-1.1714e-02
 9.0030e-03
-8.5057e-02
 2.6810e-03
 8.9295e-02
-8.9216e-02
-3.3186e-02
-6.3577e-02
-7.6635e-04
-6.9414e-02
 1.0076e-01
 3.0637e-03
-1.4796e-02
 1.1292e-02
 4.2582e-02
 8.5550e-03
-6.4279e-03
 1.2091e-02
-4.9081e-02
-9.7279e-02
-2.2004e-03
-1.2812e-03
-2.2187e-04
 1.0677e-02
-4.2782e-02
-9.0975e-03
-3.4515e-02
-7.8852e-02
-8.6300e-03
-4.5601e-02
-1.5004e-01
-2.5102e-03
-8.4100e-02
-1.5815e-02
-6.7395e-03
-2.3238e-02
-1.0294e-02
-2.1451e-02
 1.2151e-01
-1.0588e-03
-1.6892e-02
-4.8180e-03
-5.2033e-03
 1.4399e-02
-2.2758e-02
-2.2461e-03
 3.0009e-03
-1.4381e-03
-9.5411e-03
 4.8236e-02
-1.0254e-01
-1.7065e-01
-8.8658e-03
 4.8361e-04
-2.5649e-02
-6.0476e-03
 9.2989e-05
-9.2383e-03
 7.3809e-04
-8.6246e-02
-1.3256e-02
-1.5951e-01
-3.1092e-02
-3.8514e-03
-3.0750e-02
 1.5446e-02
-1.6930e-03
-7.0714e-04
-3.2160e-02
 1.0015e-02
 6.4493e-02
-5.2432e-02
-5.1374e-02
-1.8091e-02
 7.4934e-03
-6.1742e-03
 3.8418e-03
 1.5771e-02
 8.2145e-03
 8.8184e-02
-6.6368e-03
-8.0168e-02
-2.0362e-03
-2.9995e-02
-2.6609e-02
-3.4397e-02
-1.7281e-02
 2.0384e-04
 7.9040e-03
-5.7181e-03
-1.8650e-01
 6.7848e-02
-1.8846e-02
-1.3311e-01
 1.5576e-01
-4.0909e-02
-7.0750e-03
-5.7762e-02
-2.7987e-03
-6.7647e-02
 5.7659e-04
 3.7205e-03
-1.0260e-02
-5.2115e-02
 7.8752e-02
-8.8769e-06
-3.9048e-03
 6.7280e-03
-6.5355e-02
 8.2187e-02
-3.9323e-02
 9.3245e-03
 1.8185e-02
 2.2181e-03
-1.6143e-02
-1.8752e-02
 3.0577e-03
 2.4022e-03
 5.2618e-02
 2.9483e-02
 4.4747e-03
-2.6920e-02
-7.9864e-02
 3.2817e-02
 3.9993e-03
-5.5510e-02
-1.6067e-02
 3.4400e-02
-2.6225e-02
 3.2143e-03
-2.5083e-03
-3.1106e-03
-1.2949e-01
 3.0907e-03
-1.6687e-03
-8.9696e-03
-9.7100e-04
 1.0100e-01
 1.8800e-03
 4.4217e-02
 1.9097e-02
 1.4490e-02
 8.7186e-03
 5.1226e-04
 1.0172e-03
 8.3062e-02
-1.4430e-01
-8.2419e-03
-1.0131e-01
-1.0135e-02
-3.0764e-02
-1.2011e-01
-7.3087e-03
-1.4960e-02
 6.7472e-02
 1.0045e-02
-1.3479e-01
-2.1359e-03
-5.5752e-03
-6.7778e-03
-1.4515e-01
 4.4448e-03
-1.4983e-02
 1.6710e-02
 1.2233e-02
-3.9357e-02
 3.9256e-03
-9.1340e-03
-9.2751e-02
-1.7270e-03
-1.2063e-01
 1.6213e-02
-1.2020e-01
-8.2898e-02
-8.8527e-02
 1.0714e-01
-4.5816e-02
 6.1812e-02
-7.3898e-02
 2.1483e-03
 6.2958e-02
-5.0742e-02
-4.7755e-04
-1.5459e-02
-2.2325e-02
-2.1244e-03
-7.8404e-02
-1.0566e-02
-3.8241e-02
-3.6252e-02
 2.3692e-02
-1.3631e-02
-2.3484e-02
-2.4290e-02
-3.8339e-02
 1.0709e-01
 2.3524e-02
-2.4475e-03
-2.7759e-03
-4.4693e-02
-4.1383e-03
 1.2485e-02
 5.5624e-02
-9.5404e-02
-1.8058e-03
-1.2196e-02
-5.6406e-03
-1.1404e-02
-5.6079e-03
-2.2945e-03
 8.6154e-03
-4.6792e-02
 2.0591e-02
 1.5601e-02
-1.6393e-01
-8.7655e-04
 8.3973e-02
 6.5980e-02
 1.9222e-02
-6.4712e-02
-8.9406e-03
-1.7300e-02
-2.3836e-01
 3.7301e-02
-1.2117e-03
 8.7472e-03
-2.0330e-02
 1.0227e-01
-5.2300e-02
 1.1779e-02
 1.1245e-01
-9.6215e-02
 2.5785e-06
-4.3196e-03
-2.5858e-02
 8.9184e-03
-8.7743e-03
 4.6334e-02
-1.6391e-01
-1.7051e-02
-7.1404e-02
-5.9554e-03
-1.7148e-01
-2.9559e-03
-1.9741e-02
-1.0768e-01
-2.9664e-03
-1.6899e-01
-2.1230e-02
-6.6083e-02
-1.2759e-02
 4.1966e-02
-4.0875e-02
 4.9974e-02
-1.2446e-01
-5.5777e-03
 8.0188e-02
 3.4674e-03
-6.7782e-02
-1.1124e-01
 3.4792e-03
-1.7723e-02
-9.6419e-02
-7.1393e-03
-1.7892e-02
-2.1882e-02
 1.2299e-02
-2.1861e-02
-1.2036e-01
-2.4058e-02
 1.1364e-02
 9.0925e-02
-1.7966e-02
 4.7963e-03
-9.1600e-04
 2.1050e-02
-1.0789e-01
 5.4525e-03
-6.9888e-03
 6.4915e-03
-5.8863e-03
-4.2944e-02
-1.1018e-03
-1.8569e-02
-2.7626e-02
-8.7634e-03
 2.8164e-03
-9.1081e-02
-4.3993e-03
-2.2357e-02
-5.4756e-03
 1.7006e-03
-1.2821e-01
 1.0890e-01
-4.2571e-03
-1.0445e-03
-5.0183e-03
-3.5273e-02
-6.6152e-02
-3.5577e-02
-7.3482e-02
-1.6489e-01
-3.3737e-02
-8.6723e-03
-2.3769e-02
 4.3920e-02
 9.2937e-02
-1.9076e-05
 2.1228e-03
-1.6235e-01
-5.3080e-02
-7.7377e-02
 1.5929e-02
-4.2757e-02
 3.0679e-02
-1.1047e-02
-4.1060e-03
 2.6029e-03
-1.1071e-02
-4.5535e-02
-1.4550e-02
-5.9604e-02
-1.3284e-02
-7.5299e-02
 3.6854e-04
-1.2853e-02
-1.3467e-03
-1.3402e-02
-2.0708e-02
-6.1365e-02
-1.0764e-02
-6.1896e-03
-3.0042e-03
-1.2082e-02
-1.0025e-03
 1.5994e-02
 4.5418e-02
 3.2360e-04
 1.0777e-02
-9.0291e-02
-1.4739e-02
-1.9130e-02
-1.3679e-02
 1.2174e-02
 7.6363e-03
-5.4397e-03
-1.5219e-01
-1.0369e-01
-1.1100e-01
 2.2357e-02
 2.5532e-02
 4.5459e-02
 5.4136e-03
 5.9550e-03
-3.8950e-02
-8.2336e-02
-7.5211e-05
 7.1751e-02
 8.0743e-03
 2.7342e-02
-4.9002e-02
 9.5575e-04
-7.5821e-03
-4.2720e-02
-1.5194e-03
-5.9977e-03
 6.5210e-03
-2.7637e-02
-1.6451e-02
-1.8698e-01
 2.4471e-03
-2.9155e-02
 4.5423e-02
-7.8423e-02
-8.4752e-03
-3.4671e-02
-1.0741e-01
 5.1450e-02
-1.2092e-01
-4.6671e-02
-2.7426e-03
-5.7088e-02
-8.7176e-02
 2.2790e-02
-1.2400e-02
-3.1559e-02
 5.8307e-03
 4.0074e-02
-8.5271e-03
 3.3860e-04
-7.4499e-02
 6.0329e-03
 7.9079e-04
-5.2139e-02
-2.6035e-02
-4.8131e-03
 1.4255e-01
-5.6571e-02
 1.6214e-02
-1.8812e-01
-1.4541e-01
-7.4016e-03
 3.8062e-02
-1.9768e-02
 3.5405e-04
-7.8156e-02
 7.2524e-02
-1.7100e-02
-1.9132e-02
-1.6783e-03
-1.7947e-01
-1.2886e-01
-6.4873e-02
-1.9259e-02
 9.7423e-02
 5.2216e-03
-1.3475e-01
[torch.FloatTensor of size 512]
), ('layer2.1.bn3.running_mean', 
-7.2266e-02
-6.4673e-03
-5.2443e-02
-5.5835e-03
-2.7993e-02
-5.8107e-02
 5.6740e-03
-4.1644e-02
-3.6627e-02
-7.1723e-02
 6.9328e-02
-2.0193e-02
-3.3140e-02
 5.3545e-03
 4.0593e-03
-1.2621e-02
 1.2421e-02
 6.6509e-03
 5.4783e-03
 1.0113e-02
 1.1846e-03
-8.4761e-03
 4.8360e-03
-5.3494e-02
-6.5576e-03
-2.6896e-03
 1.2397e-01
 1.1710e-03
-2.0764e-02
-9.5471e-03
-1.8970e-03
 1.2422e-03
-2.9563e-02
-6.8518e-03
 1.1525e-02
-1.5427e-02
 7.4534e-03
-3.2611e-03
-4.8615e-04
-3.4393e-02
 7.7539e-03
 4.7229e-03
 5.3835e-03
 6.7465e-02
-4.7015e-03
-3.2585e-03
-1.3146e-02
-6.5650e-03
-2.8728e-03
-3.6594e-02
-3.4272e-02
-2.0326e-04
 1.2454e-02
-3.9937e-03
 3.8972e-02
 2.6737e-05
 2.5910e-02
-1.0405e-02
-1.2848e-02
-5.5202e-02
-3.2137e-02
 2.0112e-02
-7.8464e-03
 2.7204e-03
-4.9303e-03
-4.9467e-03
-9.1422e-03
 8.5608e-03
-1.4557e-04
-1.6027e-03
 3.0268e-02
 5.3017e-03
-1.6410e-02
-9.7018e-03
 6.1687e-03
-6.9334e-02
-3.3819e-04
 3.4138e-03
 7.3745e-03
 1.0193e-02
 9.6027e-05
-7.6847e-04
-2.1648e-02
 7.5754e-03
 3.6479e-03
-1.9545e-03
-7.3093e-03
-1.4444e-02
-1.7043e-02
-2.4087e-02
-6.0503e-02
 3.7546e-03
-9.1603e-03
-1.4851e-02
-6.3045e-02
 2.7153e-04
-2.6921e-03
 3.8378e-02
 1.5936e-02
 4.6098e-02
 2.1077e-02
-1.6652e-02
-1.0511e-02
-1.5036e-03
-5.9020e-02
-8.0883e-02
 3.5257e-03
-4.6590e-03
 8.8074e-03
-2.4582e-02
-5.5426e-03
-7.7744e-03
-4.3746e-02
 5.6946e-03
 2.5990e-02
-8.6484e-03
 5.9785e-03
 7.1338e-03
-3.1440e-03
-1.2751e-02
 1.5782e-03
 4.9193e-03
 6.4011e-03
 3.3910e-03
-8.1112e-03
-2.9602e-02
-9.2181e-04
 3.2114e-04
-4.0874e-02
-1.1509e-02
-2.2028e-03
-2.0472e-03
-1.7095e-02
-1.6260e-01
 3.6550e-04
-2.4659e-02
-1.1163e-02
 1.7714e-02
-6.6503e-02
 1.3721e-02
-1.0566e-02
-8.0228e-03
-6.4340e-04
 2.2502e-03
-7.1369e-02
-2.8580e-02
-7.3558e-02
 1.5660e-04
 1.3431e-04
-1.4947e-03
-3.2829e-03
 1.5215e-03
 7.3190e-03
-3.6306e-02
-3.7645e-02
 5.0970e-03
 7.3463e-03
-4.5357e-02
 1.8261e-02
-2.9515e-02
 4.4864e-04
 6.6360e-03
-5.7946e-03
 1.8044e-02
 3.3787e-03
-1.0648e-02
-3.0720e-02
-4.2805e-03
 9.4857e-03
-2.1200e-02
-4.9384e-02
-1.2684e-02
-5.1652e-03
-4.9373e-03
-3.8305e-02
 5.5837e-03
-5.2732e-02
-1.7586e-03
-3.5040e-02
-4.4702e-03
-5.3136e-03
 1.4894e-03
 7.2240e-05
-1.1535e-01
 3.0519e-03
-3.4607e-02
-5.1648e-02
 3.4506e-03
 6.1300e-02
 4.5359e-02
 1.3032e-03
 1.3951e-02
-1.2170e-02
-2.3505e-02
-6.2672e-02
-1.4555e-03
-3.3213e-02
-2.6165e-03
 2.7066e-02
-6.7853e-02
-5.4322e-07
 6.3117e-03
-3.6258e-04
-1.2109e-02
-9.5193e-02
-1.2897e-02
-1.0807e-01
 4.8909e-04
-1.3628e-02
 2.4768e-03
-6.9554e-03
-4.1988e-04
 9.7074e-03
-7.8736e-02
 1.4428e-02
 6.7911e-03
-1.8478e-02
-2.3316e-02
-7.8852e-02
 2.3578e-03
 4.8011e-02
-1.1529e-03
-5.5066e-02
-3.2138e-02
-4.8869e-03
 6.3634e-03
-3.0609e-03
-8.6543e-02
 1.4743e-03
-2.0929e-03
-6.2336e-03
-1.2657e-02
-9.3436e-02
-2.4774e-02
-6.8972e-02
 5.8627e-03
 3.8397e-02
-1.4168e-02
-2.8957e-03
-1.1219e-02
-2.7901e-02
-3.3766e-03
-2.9238e-03
 1.0072e-02
-9.6302e-03
 6.0258e-04
 5.8693e-02
 2.0575e-02
-4.0824e-03
-7.0650e-03
-6.6167e-03
-1.5441e-02
-3.3382e-04
 6.6278e-03
-8.2937e-03
-1.2404e-02
-2.3992e-03
 3.8073e-03
-6.9430e-04
-1.4157e-02
-6.6125e-03
 1.8441e-04
-6.1198e-04
-3.9758e-02
-4.5931e-03
 2.8928e-03
 3.2436e-03
 6.1383e-02
-4.2892e-03
 2.3683e-02
-7.5185e-02
-6.6148e-03
-4.0886e-02
-1.6284e-02
-3.1880e-04
-5.1265e-02
-1.1023e-02
 1.4364e-03
 1.0818e-03
 1.5226e-02
-1.5386e-03
-6.5689e-03
-1.5678e-03
 1.4822e-02
-4.6331e-02
 1.3069e-02
 1.3752e-02
 3.1954e-03
 7.4682e-03
-1.7888e-02
 3.9036e-02
-3.4013e-02
-4.7790e-03
 4.7424e-03
 1.0214e-03
 6.2584e-03
 9.3313e-03
-4.5246e-02
-4.2020e-02
 3.9260e-04
-1.1660e-02
 2.4049e-03
-6.3644e-02
 1.5541e-03
 1.2435e-02
 1.0588e-02
-8.6570e-03
 1.4149e-02
-6.7475e-03
 6.4190e-03
-1.4329e-02
-1.1835e-01
-5.5870e-02
-5.0771e-02
-8.0703e-02
-5.9746e-02
 1.0144e-02
-1.3451e-04
-7.8142e-02
-2.1601e-02
-4.9364e-03
 5.2051e-03
 5.5447e-03
-1.1835e-02
-7.2142e-03
-7.0564e-02
 1.3282e-02
 4.2354e-03
-5.0141e-03
-5.6211e-02
-1.5877e-02
-2.4816e-04
-6.2627e-02
 2.7363e-02
-7.8316e-03
-2.4857e-02
-3.6721e-03
 1.1172e-02
 6.6587e-04
 9.0494e-03
-6.6504e-03
-3.0997e-03
-4.8756e-02
-6.9492e-03
-3.3569e-02
-9.6887e-03
-5.3878e-02
 8.0170e-03
-3.3071e-03
-1.9787e-02
-3.1942e-03
 2.7551e-02
 1.7478e-02
-1.8954e-02
-2.0023e-02
-7.9424e-04
 3.4800e-03
 1.2310e-02
-1.9330e-02
-1.0334e-02
 4.8853e-03
-1.8012e-02
-2.1827e-02
-3.4210e-02
-1.9191e-02
 2.5772e-03
-1.6992e-03
-5.0063e-03
 1.5367e-02
 1.8647e-03
 1.4979e-02
 3.1172e-04
 3.3767e-03
-6.5494e-03
 3.4082e-03
 1.1414e-02
 2.5611e-03
-9.6575e-04
-2.3138e-02
-3.9041e-02
 5.8807e-03
-5.1987e-03
 8.0767e-02
 6.3452e-03
 4.8996e-03
-1.5553e-03
-7.7427e-02
-4.7274e-02
-5.2442e-03
-5.0876e-03
-2.0079e-03
 1.0976e-03
-4.1572e-02
-1.9498e-02
-1.3954e-02
-7.5138e-03
-6.0788e-03
-9.0641e-02
-3.5527e-03
-3.7341e-02
-5.6924e-02
 4.3519e-03
-4.3288e-03
 2.0005e-03
 7.7917e-02
-7.5330e-03
-6.4094e-03
-8.8224e-03
 1.5067e-02
-4.3802e-02
 2.5268e-03
-1.2316e-02
 2.6427e-03
-5.0081e-02
-1.1255e-02
 9.5909e-03
-4.0263e-03
 1.0744e-02
 2.8834e-02
-1.8160e-02
 4.0137e-03
 1.4257e-02
-4.5974e-03
-5.5450e-03
-7.1062e-03
-8.0758e-03
-2.7996e-02
-4.6369e-02
-5.1124e-03
-8.4070e-03
-2.0599e-02
-1.8179e-02
-1.9948e-03
-2.8674e-02
-3.5192e-02
 4.1612e-04
-1.1630e-03
 1.5524e-02
-5.2456e-03
 2.9902e-03
-6.4891e-03
-6.7089e-02
-8.4858e-03
-4.0071e-02
 5.8231e-02
-4.4647e-03
-5.7514e-02
-4.9318e-03
-9.6572e-03
 7.5385e-03
-2.2297e-02
-7.8505e-05
-2.0419e-01
-2.9672e-04
-3.9975e-02
-3.7706e-02
-6.3099e-03
 7.8989e-03
-7.4229e-02
 5.9960e-03
 1.5312e-02
-1.2326e-04
-2.9710e-03
 2.3323e-03
-4.0302e-02
-1.1928e-02
-1.7088e-02
-5.1354e-02
-3.6080e-02
-6.6567e-04
 2.6229e-02
-4.4309e-02
-5.7853e-02
-1.1922e-02
 7.0904e-03
-3.7209e-03
-1.8635e-02
-1.6917e-02
 5.7391e-03
 8.3925e-03
-1.3726e-03
 1.8239e-02
-1.5846e-01
 1.2348e-02
 1.4146e-03
 1.1827e-02
-3.3081e-02
 1.9488e-03
-2.9203e-02
-3.7523e-02
-4.1759e-03
 1.3272e-02
-4.3525e-02
-2.5879e-02
-7.0244e-02
-1.4757e-01
-3.4929e-03
-7.9931e-03
-6.1793e-02
-9.2482e-03
-4.7260e-02
 2.0772e-03
 7.6976e-03
-8.1737e-03
-6.8678e-03
-7.7498e-03
-4.4376e-03
-7.4256e-03
 1.1163e-04
-2.1816e-02
 1.0362e-02
-2.2461e-02
[torch.FloatTensor of size 512]
), ('layer2.1.bn3.running_var', 
 1.6284e-03
 1.7029e-04
 5.7889e-03
 2.8674e-04
 1.9907e-03
 2.2460e-03
 2.8328e-04
 5.8827e-03
 1.3283e-03
 3.3379e-03
 2.6487e-03
 2.9974e-04
 1.5388e-03
 1.9177e-03
 4.1063e-03
 1.6044e-04
 4.4188e-04
 1.0647e-03
 1.2174e-03
 5.6032e-04
 1.4149e-04
 1.5553e-04
 2.0146e-04
 8.3540e-03
 2.9055e-04
 2.8059e-03
 2.5522e-03
 3.3695e-04
 5.4664e-03
 2.6071e-04
 1.4602e-04
 2.4437e-04
 7.4368e-03
 5.6370e-04
 5.3253e-04
 2.0230e-04
 7.7274e-04
 2.2702e-04
 3.8611e-06
 7.7127e-04
 1.8603e-04
 1.6540e-04
 3.5178e-03
 1.1367e-03
 4.7081e-03
 1.7164e-04
 1.3710e-03
 1.2326e-04
 2.2904e-04
 7.1774e-04
 4.5221e-03
 1.5653e-07
 2.8830e-04
 2.9603e-04
 2.8718e-03
 1.2482e-04
 4.0334e-04
 4.9945e-04
 3.8824e-04
 2.1954e-03
 3.1094e-03
 4.1796e-04
 9.9308e-05
 1.0337e-04
 2.9293e-04
 3.6202e-04
 1.7478e-04
 1.8922e-04
 4.5271e-08
 1.1555e-04
 2.6740e-04
 2.3370e-04
 4.6194e-04
 3.4957e-04
 2.8494e-04
 4.7571e-04
 3.6736e-04
 3.1627e-04
 2.1773e-03
 1.5416e-04
 3.5073e-04
 1.4871e-04
 4.4283e-03
 1.5585e-04
 1.2870e-04
 2.7392e-04
 5.2303e-04
 6.9328e-03
 3.7573e-03
 2.4527e-03
 6.3284e-03
 2.0665e-04
 1.3272e-04
 9.3243e-04
 9.2655e-03
 1.0211e-04
 2.4059e-04
 9.3671e-04
 2.8857e-04
 5.2609e-03
 6.7352e-04
 1.8278e-04
 3.0030e-03
 2.6287e-04
 2.9015e-03
 4.3496e-03
 1.4855e-04
 1.2040e-04
 3.3053e-04
 2.0582e-03
 2.4192e-04
 2.9524e-04
 2.0840e-03
 7.5308e-04
 1.0003e-03
 4.4000e-04
 1.3636e-04
 2.6177e-04
 1.9672e-04
 8.2524e-04
 5.6728e-04
 5.0529e-04
 1.6646e-03
 1.4958e-04
 1.7042e-04
 5.6432e-03
 2.7934e-04
 1.8100e-04
 1.2124e-03
 1.7152e-04
 1.9681e-04
 2.6688e-04
 2.3270e-04
 2.7787e-03
 5.2005e-04
 1.3957e-03
 1.8626e-04
 2.5162e-04
 9.8798e-03
 7.5840e-04
 3.9906e-04
 4.0535e-03
 1.5024e-04
 5.1024e-03
 2.2029e-03
 9.4543e-04
 1.0895e-03
 3.2439e-04
 2.2819e-04
 7.2473e-04
 2.6225e-04
 4.2995e-03
 3.6079e-04
 5.4688e-03
 2.3667e-03
 4.5328e-04
 1.6361e-03
 6.8476e-03
 2.2038e-04
 1.8450e-03
 3.0565e-04
 5.9191e-04
 3.2116e-04
 4.2319e-04
 3.2519e-04
 1.3166e-03
 3.2620e-04
 6.5689e-04
 1.0865e-04
 1.3156e-03
 5.8626e-03
 6.8843e-04
 8.1093e-04
 3.8145e-04
 2.6297e-03
 2.0457e-04
 2.2225e-03
 2.9679e-04
 1.7142e-03
 2.6449e-04
 3.5525e-04
 3.4163e-04
 1.7839e-04
 3.1338e-03
 3.1254e-04
 2.5065e-03
 1.0082e-02
 5.8736e-04
 3.2584e-03
 1.9772e-03
 8.3555e-04
 1.4708e-04
 7.5063e-04
 3.5478e-04
 8.9815e-03
 1.8598e-04
 5.0218e-03
 1.4894e-04
 4.6682e-03
 3.3356e-03
 6.2257e-13
 3.1104e-04
 1.8881e-04
 2.4119e-03
 8.7712e-03
 1.4171e-04
 1.5689e-03
 3.3547e-04
 8.4871e-04
 7.7825e-06
 4.1073e-04
 3.4826e-04
 6.0902e-04
 8.6703e-03
 3.8405e-03
 1.4216e-04
 3.5555e-04
 3.7701e-04
 1.0094e-02
 2.2256e-04
 1.5792e-03
 2.0613e-04
 4.6353e-03
 1.7087e-04
 2.1305e-04
 2.7353e-04
 1.8634e-04
 6.6136e-03
 1.6226e-04
 3.4628e-04
 1.7341e-04
 2.2746e-04
 6.7409e-03
 1.9544e-03
 1.0684e-02
 1.8328e-04
 6.3438e-03
 2.9865e-04
 3.4744e-04
 2.1371e-04
 7.5714e-03
 8.1217e-03
 2.1487e-04
 5.5469e-04
 1.0628e-04
 8.5292e-05
 1.4373e-03
 3.9474e-04
 1.1541e-04
 2.7725e-03
 2.4883e-04
 1.5292e-03
 1.5685e-04
 2.3891e-04
 1.5136e-04
 1.3049e-03
 1.3823e-04
 7.4893e-04
 3.2937e-04
 4.3385e-04
 4.0505e-04
 2.3887e-04
 2.0745e-04
 3.3622e-03
 2.2737e-04
 3.4706e-04
 1.7490e-04
 5.3310e-04
 9.1573e-04
 3.1916e-04
 4.3389e-03
 2.5057e-04
 5.0290e-03
 4.2402e-03
 2.4142e-04
 3.8610e-03
 1.5804e-03
 1.2183e-03
 1.6037e-04
 1.4269e-04
 1.7592e-04
 1.6375e-03
 1.6826e-04
 1.4395e-03
 3.2154e-03
 2.8082e-03
 2.8930e-04
 2.5596e-04
 2.6481e-04
 1.6968e-04
 2.5519e-03
 1.1012e-03
 1.2178e-04
 2.0805e-04
 8.3841e-05
 2.5788e-04
 2.5000e-04
 7.9195e-03
 1.3626e-03
 1.6780e-07
 3.5555e-04
 7.7920e-05
 1.5290e-03
 3.0797e-04
 1.4703e-04
 3.0486e-04
 1.6985e-04
 1.2514e-03
 3.0406e-04
 3.5817e-04
 1.8105e-04
 3.9061e-03
 1.0065e-02
 3.6888e-03
 2.9602e-03
 7.3473e-03
 1.1231e-04
 1.8920e-03
 8.3303e-03
 2.5970e-03
 1.7140e-04
 2.8186e-04
 3.7198e-04
 7.4309e-04
 2.8639e-04
 1.4345e-02
 4.7790e-04
 2.4741e-04
 1.8003e-04
 6.7821e-03
 4.8910e-04
 2.3817e-04
 1.1427e-02
 4.0112e-04
 1.6352e-04
 1.0391e-03
 1.9306e-04
 4.7841e-04
 1.9175e-04
 6.3135e-04
 3.0727e-03
 2.4080e-04
 3.3931e-03
 1.4949e-04
 4.7494e-03
 4.9480e-04
 8.6736e-03
 3.3267e-04
 3.1624e-04
 6.1230e-04
 2.7279e-04
 1.7710e-03
 3.6923e-04
 1.7549e-03
 4.6006e-03
 1.9639e-04
 3.4440e-04
 5.6364e-04
 2.4848e-04
 1.9463e-04
 1.5929e-03
 3.9344e-04
 1.3919e-03
 1.9868e-04
 3.8706e-03
 3.7263e-04
 3.2268e-04
 2.0949e-04
 3.8036e-04
 2.1451e-04
 6.5237e-03
 1.8985e-04
 3.5573e-04
 2.9907e-04
 4.4666e-04
 2.0281e-04
 3.7954e-04
 1.6786e-04
 3.6082e-04
 1.8063e-03
 2.5767e-04
 2.0306e-04
 2.6506e-03
 1.3761e-04
 2.2581e-04
 1.5305e-04
 6.0953e-03
 2.8861e-03
 2.8433e-03
 1.1705e-04
 1.7413e-04
 1.8924e-04
 1.4899e-03
 1.1505e-04
 1.0871e-03
 7.1042e-03
 2.1444e-03
 9.4250e-04
 1.8197e-04
 5.1670e-03
 1.1022e-02
 3.4556e-03
 2.7713e-04
 1.3635e-04
 1.2827e-03
 6.0558e-04
 2.6537e-04
 2.6801e-04
 1.1272e-03
 3.0077e-03
 2.4568e-04
 1.9990e-04
 2.6099e-04
 2.2290e-03
 6.8429e-03
 1.1975e-04
 5.7814e-04
 1.9016e-04
 3.2923e-04
 3.8029e-03
 2.1115e-04
 7.7350e-03
 1.3202e-04
 1.8656e-04
 4.9595e-04
 8.5993e-04
 5.5768e-03
 7.3568e-03
 5.7550e-04
 1.6944e-04
 4.7750e-03
 4.9799e-04
 3.2508e-04
 4.3663e-03
 4.8213e-04
 1.4620e-04
 1.3663e-04
 2.6558e-04
 1.7164e-04
 1.6603e-04
 1.4002e-04
 1.8054e-03
 7.4891e-04
 5.4385e-03
 8.9542e-04
 2.7594e-04
 6.5069e-03
 2.9598e-04
 3.0899e-04
 2.4417e-03
 1.1516e-02
 2.0432e-09
 2.9651e-03
 2.0213e-04
 4.5312e-03
 6.2046e-04
 2.4725e-04
 1.4981e-04
 5.0426e-03
 5.5597e-04
 1.4901e-04
 2.2793e-04
 9.3801e-05
 1.8824e-04
 4.1840e-03
 2.1473e-04
 7.9476e-05
 8.5305e-03
 7.5183e-04
 1.5216e-04
 1.9774e-04
 6.4609e-03
 1.3228e-02
 1.5556e-03
 1.6156e-04
 1.8934e-04
 2.8537e-04
 3.9499e-04
 2.4570e-04
 2.0499e-04
 5.7088e-04
 2.1585e-04
 2.7616e-03
 1.6902e-04
 3.1133e-04
 9.5689e-04
 6.0766e-03
 2.6693e-04
 2.2510e-03
 4.3416e-04
 1.3138e-04
 6.3606e-04
 2.8670e-03
 6.1856e-04
 1.3220e-03
 1.1322e-03
 2.1415e-04
 1.5715e-03
 7.3782e-03
 2.5430e-04
 1.5295e-03
 1.2132e-03
 1.6837e-04
 2.0668e-03
 1.5639e-04
 5.2642e-04
 2.0811e-04
 5.2004e-04
 2.2496e-04
 3.9384e-03
 3.7138e-04
 4.8937e-03
[torch.FloatTensor of size 512]
), ('layer2.2.conv1.weight', 
( 0 , 0 ,.,.) = 
  7.4153e-03

( 0 , 1 ,.,.) = 
  6.2506e-03

( 0 , 2 ,.,.) = 
 -2.0115e-02
    ... 

( 0 ,509,.,.) = 
  1.9778e-02

( 0 ,510,.,.) = 
  4.9028e-03

( 0 ,511,.,.) = 
  2.3533e-02
      ⋮  

( 1 , 0 ,.,.) = 
  3.6302e-04

( 1 , 1 ,.,.) = 
 -3.4109e-02

( 1 , 2 ,.,.) = 
 -1.8630e-02
    ... 

( 1 ,509,.,.) = 
  8.8631e-03

( 1 ,510,.,.) = 
  1.2520e-02

( 1 ,511,.,.) = 
 -2.4100e-02
      ⋮  

( 2 , 0 ,.,.) = 
 -3.1197e-03

( 2 , 1 ,.,.) = 
 -2.0085e-02

( 2 , 2 ,.,.) = 
  1.7950e-02
    ... 

( 2 ,509,.,.) = 
  2.2827e-02

( 2 ,510,.,.) = 
 -4.4781e-02

( 2 ,511,.,.) = 
  8.4873e-04
...     
      ⋮  

(125, 0 ,.,.) = 
  5.7927e-03

(125, 1 ,.,.) = 
 -4.4353e-04

(125, 2 ,.,.) = 
 -1.4330e-03
    ... 

(125,509,.,.) = 
  3.3877e-02

(125,510,.,.) = 
  8.9764e-03

(125,511,.,.) = 
  5.4745e-02
      ⋮  

(126, 0 ,.,.) = 
  1.2431e-02

(126, 1 ,.,.) = 
  3.7703e-03

(126, 2 ,.,.) = 
 -3.3974e-03
    ... 

(126,509,.,.) = 
  5.1701e-02

(126,510,.,.) = 
  3.8852e-02

(126,511,.,.) = 
 -7.2459e-03
      ⋮  

(127, 0 ,.,.) = 
  2.5396e-02

(127, 1 ,.,.) = 
  1.4263e-03

(127, 2 ,.,.) = 
  5.8049e-02
    ... 

(127,509,.,.) = 
 -7.5972e-03

(127,510,.,.) = 
  1.3941e-02

(127,511,.,.) = 
 -2.7615e-02
[torch.FloatTensor of size 128x512x1x1]
), ('layer2.2.bn1.weight', 
 0.1067
 0.1330
 0.1966
 0.2041
 0.1228
 0.1568
 0.1682
 0.1349
 0.2218
 0.1687
 0.1892
 0.1968
 0.1744
 0.1540
 0.2011
 0.1394
 0.1831
 0.1286
 0.1294
 0.1838
 0.1811
 0.1735
 0.1889
 0.2164
 0.1570
 0.1866
 0.1985
 0.1465
 0.1640
 0.1273
 0.2273
 0.1822
 0.1239
 0.1486
 0.1905
 0.1189
 0.1204
 0.1201
 0.2163
 0.1234
 0.1987
 0.2345
 0.1419
 0.1329
 0.2138
 0.1867
 0.1982
 0.1370
 0.2064
 0.1887
 0.1521
 0.1634
 0.1765
 0.1685
 0.1898
 0.2198
 0.1706
 0.1577
 0.1609
 0.1754
 0.1328
 0.1600
 0.1973
 0.1794
 0.1991
 0.1767
 0.1928
 0.1742
 0.1744
 0.1367
 0.2161
 0.2227
 0.1319
 0.1864
 0.2205
 0.1355
 0.2003
 0.1855
 0.1881
 0.1726
 0.2034
 0.1853
 0.1983
 0.2177
 0.1946
 0.1802
 0.1104
 0.2102
 0.1276
 0.1824
 0.1731
 0.1329
 0.1180
 0.1595
 0.1543
 0.1727
 0.1970
 0.1179
 0.1991
 0.1621
 0.1657
 0.1119
 0.2403
 0.2070
 0.2018
 0.2244
 0.1570
 0.1642
 0.1892
 0.1862
 0.1603
 0.1726
 0.1623
 0.1524
 0.1493
 0.1494
 0.1380
 0.1604
 0.2158
 0.1389
 0.1482
 0.1677
 0.2042
 0.1284
 0.2151
 0.1155
 0.1354
 0.1563
[torch.FloatTensor of size 128]
), ('layer2.2.bn1.bias', 
 0.1375
 0.0225
-0.0772
 0.0191
 0.0966
 0.0495
-0.0310
 0.0633
-0.0968
 0.0818
-0.0752
-0.0253
-0.0412
 0.0308
-0.0137
 0.2680
 0.0622
-0.0029
 0.0698
-0.0520
 0.0482
-0.0226
-0.0084
-0.0744
-0.0868
-0.0652
 0.0399
 0.0512
 0.0236
 0.0933
-0.0298
 0.0088
 0.0235
 0.0286
 0.0584
 0.1792
 0.0735
 0.1293
-0.1163
 0.2136
-0.1089
-0.0553
 0.0383
 0.0160
-0.0720
-0.0421
-0.0587
 0.0376
 0.0621
-0.0326
 0.0225
-0.0269
 0.0393
-0.0053
-0.0348
 0.0317
 0.0158
 0.0437
-0.0442
-0.0017
 0.0745
 0.0727
-0.0454
-0.0722
 0.0237
-0.0422
-0.0320
-0.0718
-0.0261
 0.0509
-0.0768
 0.0078
 0.2535
 0.0508
-0.2020
 0.0159
-0.0641
-0.0309
-0.0480
-0.0262
 0.0270
 0.0093
-0.0015
-0.0898
 0.0010
 0.0276
 0.0528
-0.0555
 0.0416
-0.0096
-0.0605
 0.0835
 0.0930
 0.0408
 0.0333
-0.0152
-0.0223
 0.0766
-0.1066
-0.0693
 0.0344
 0.0913
-0.1199
-0.0562
-0.1034
-0.0769
 0.0707
-0.0081
-0.0790
 0.0056
 0.0025
-0.0215
-0.0310
 0.0065
-0.0154
-0.0476
 0.1121
 0.0382
-0.0474
 0.0490
-0.0386
 0.0157
-0.1338
 0.0006
-0.1198
 0.0793
 0.0444
 0.0185
[torch.FloatTensor of size 128]
), ('layer2.2.bn1.running_mean', 
-0.0514
-0.1254
-0.0629
 0.0193
-0.2494
-0.0917
-0.1214
-0.1213
 0.0008
-0.2548
-0.0773
-0.0290
 0.0046
 0.0873
 0.0865
-0.1331
-0.1341
-0.1487
 0.0007
 0.0670
-0.1657
 0.0262
-0.0801
-0.0557
-0.1297
 0.0867
-0.0399
 0.1199
 0.0008
-0.1512
-0.0711
-0.0555
 0.0214
-0.0256
-0.0835
-0.0724
 0.0343
 0.1537
 0.0144
-0.1098
 0.0227
-0.1219
-0.0362
 0.0096
 0.0076
-0.0192
 0.0829
 0.1298
-0.1207
 0.0774
-0.1170
-0.2001
-0.0320
 0.0081
-0.0325
 0.0311
 0.0562
 0.0613
 0.2119
-0.0750
-0.1563
-0.0936
-0.1118
 0.0114
-0.2314
-0.1847
 0.1282
-0.1086
 0.0505
-0.0253
-0.3427
-0.2029
-0.0250
 0.0038
-0.1306
 0.0197
-0.0649
-0.0602
-0.0715
 0.0711
 0.1030
 0.1036
-0.0536
-0.0724
-0.2156
 0.0563
 0.1713
 0.3215
-0.0315
 0.0895
-0.0831
 0.0195
-0.1799
-0.1281
 0.0230
-0.1260
-0.0437
-0.0012
-0.0332
-0.0746
-0.0608
-0.1021
-0.0173
-0.0619
-0.0816
-0.0246
-0.0714
-0.0497
 0.0320
-0.1544
-0.0537
-0.0801
 0.1660
 0.1510
-0.0021
-0.0204
 0.0884
-0.0149
-0.0801
-0.1872
-0.0259
-0.0875
-0.0711
-0.1231
 0.0383
 0.1261
-0.0151
-0.0103
[torch.FloatTensor of size 128]
), ('layer2.2.bn1.running_var', 
 0.0279
 0.0230
 0.0224
 0.0354
 0.0217
 0.0278
 0.0214
 0.0209
 0.0428
 0.0575
 0.0183
 0.0196
 0.0182
 0.0543
 0.0257
 0.1077
 0.0360
 0.0099
 0.0255
 0.0239
 0.0337
 0.0266
 0.0320
 0.0248
 0.0105
 0.0267
 0.0326
 0.0227
 0.0311
 0.0485
 0.0307
 0.0356
 0.0181
 0.0280
 0.0396
 0.0545
 0.0187
 0.0185
 0.0192
 0.0703
 0.0152
 0.0463
 0.0265
 0.0231
 0.0242
 0.0252
 0.0211
 0.0234
 0.0303
 0.0189
 0.0203
 0.0147
 0.0204
 0.0261
 0.0395
 0.0339
 0.0302
 0.0329
 0.0202
 0.0253
 0.0217
 0.0490
 0.0261
 0.0125
 0.0511
 0.0296
 0.0259
 0.0193
 0.0254
 0.0254
 0.0301
 0.0378
 0.1115
 0.0318
 0.0103
 0.0179
 0.0198
 0.0296
 0.0227
 0.0183
 0.0369
 0.0262
 0.0357
 0.0315
 0.0299
 0.0288
 0.0191
 0.0199
 0.0129
 0.0295
 0.0302
 0.0465
 0.0157
 0.0241
 0.0242
 0.0214
 0.0285
 0.0300
 0.0220
 0.0118
 0.0481
 0.0193
 0.0283
 0.0451
 0.0269
 0.0334
 0.0286
 0.0217
 0.0216
 0.0373
 0.0247
 0.0180
 0.0235
 0.0176
 0.0210
 0.0241
 0.0429
 0.0228
 0.0278
 0.0319
 0.0202
 0.0253
 0.0142
 0.0250
 0.0178
 0.0180
 0.0163
 0.0222
[torch.FloatTensor of size 128]
), ('layer2.2.conv2.weight', 
( 0 , 0 ,.,.) = 
 -3.1302e-03 -3.1556e-03  3.8333e-04
  1.0679e-02 -3.3345e-03  1.0498e-02
  1.7142e-03 -8.1773e-03  4.2616e-03

( 0 , 1 ,.,.) = 
  1.6654e-02  9.1589e-03  1.0449e-02
  1.4159e-02  1.4123e-02 -5.6379e-03
  1.0631e-02  1.1550e-02  1.4759e-02

( 0 , 2 ,.,.) = 
  1.3936e-02  1.1762e-02  7.6583e-03
  5.1115e-03 -2.7663e-03  2.6938e-02
 -7.7951e-04  6.5254e-03 -5.8126e-03
    ... 

( 0 ,125,.,.) = 
  9.4068e-03  3.3847e-04 -6.2770e-03
 -1.9364e-03 -5.7068e-03 -5.2139e-03
 -1.5536e-02  3.0893e-03  8.7088e-04

( 0 ,126,.,.) = 
 -2.6588e-02 -1.3738e-02  1.9362e-03
 -1.1497e-02  5.4170e-03  2.2290e-02
 -1.5209e-02  1.0042e-02  1.4244e-02

( 0 ,127,.,.) = 
  2.0787e-02  1.2619e-02  8.0365e-03
  2.5076e-02  1.5249e-02  3.9060e-02
  1.4655e-02  1.3425e-02  2.6900e-02
      ⋮  

( 1 , 0 ,.,.) = 
 -1.3546e-02  1.0102e-02 -5.6306e-03
  1.8917e-03  4.0935e-02  2.9032e-04
 -3.2169e-02 -5.6877e-02 -3.7226e-02

( 1 , 1 ,.,.) = 
 -9.4402e-03 -1.2173e-02  2.2989e-03
  5.8581e-04 -1.7501e-03  1.4888e-02
  4.2108e-03 -8.8788e-04  1.6264e-02

( 1 , 2 ,.,.) = 
  9.6043e-03  3.3679e-03 -9.1322e-03
 -3.7454e-03 -2.5192e-02 -3.1959e-02
 -1.4426e-02 -4.6583e-03 -1.8655e-02
    ... 

( 1 ,125,.,.) = 
 -1.9185e-02 -1.0943e-02 -1.8933e-02
 -1.2623e-02  3.7866e-03 -1.0680e-02
  2.0189e-03  7.9173e-03  2.6661e-02

( 1 ,126,.,.) = 
  1.3142e-02 -2.9681e-03 -6.9075e-03
  2.0208e-02  2.5027e-03 -1.6389e-02
  1.4347e-03 -7.2946e-03 -1.0424e-02

( 1 ,127,.,.) = 
 -1.7231e-02 -6.5504e-03 -1.3695e-02
 -1.1624e-02  4.5325e-03  1.2338e-03
 -7.9682e-03  4.0773e-04 -1.0477e-03
      ⋮  

( 2 , 0 ,.,.) = 
 -1.4542e-02 -2.2976e-02 -1.3867e-02
 -6.8874e-05  6.3162e-03  7.4062e-03
 -5.3631e-03 -1.9375e-03  8.8525e-03

( 2 , 1 ,.,.) = 
 -7.8354e-03 -9.8175e-03 -9.5295e-03
 -9.0836e-03 -7.8326e-03  3.8246e-03
  1.2576e-02  2.6117e-02 -1.9496e-03

( 2 , 2 ,.,.) = 
  3.4312e-03  1.4886e-02  1.9845e-02
 -1.7894e-02 -1.6905e-02 -5.7597e-03
  6.9308e-03 -2.9813e-03 -1.3229e-02
    ... 

( 2 ,125,.,.) = 
 -6.5217e-03 -1.5325e-02  1.5252e-02
  2.3357e-02 -4.1022e-02 -4.0224e-03
  1.3244e-02  4.9131e-02 -1.4938e-02

( 2 ,126,.,.) = 
 -2.7648e-03 -1.0411e-02  5.6259e-04
 -1.8668e-03  3.0416e-02  7.0544e-03
 -2.5378e-02 -1.2816e-02  1.4620e-02

( 2 ,127,.,.) = 
 -2.3961e-02 -4.3054e-02 -1.8946e-02
  4.6608e-02  8.2388e-03  2.4859e-02
  1.4446e-02  1.1414e-02  4.3087e-02
...     
      ⋮  

(125, 0 ,.,.) = 
 -1.2832e-02 -9.8999e-03 -9.7872e-03
 -1.4953e-02 -3.4269e-03 -1.3721e-02
 -2.8918e-02  5.7522e-02  1.9740e-02

(125, 1 ,.,.) = 
 -8.8964e-03 -1.9896e-02 -1.0263e-02
 -9.1614e-03 -1.4733e-02 -2.9040e-03
 -3.1857e-02  4.7644e-02  5.3215e-02

(125, 2 ,.,.) = 
 -2.4541e-02 -1.3963e-02 -1.5121e-02
 -2.0705e-02 -1.2160e-02 -2.5758e-02
  1.2093e-02  9.5702e-03  1.2815e-02
    ... 

(125,125,.,.) = 
  1.4657e-02  2.1536e-02  1.2741e-03
  6.3637e-03  5.7634e-04  1.0379e-02
  7.6955e-03 -1.1966e-02  6.8439e-04

(125,126,.,.) = 
  4.8690e-03  4.9937e-03  1.3563e-03
 -1.6994e-02 -2.6711e-02 -2.6102e-02
 -1.1411e-02 -2.1785e-02 -1.4453e-02

(125,127,.,.) = 
  2.4642e-02  1.8539e-02 -3.5493e-04
  2.0089e-02  1.7848e-02 -7.1624e-03
  1.9272e-02 -1.4525e-02 -4.6722e-02
      ⋮  

(126, 0 ,.,.) = 
  2.2050e-02  1.9969e-02  1.4083e-02
  1.4105e-02 -1.0612e-02 -8.6998e-03
  1.2431e-03  1.5315e-02 -6.0682e-03

(126, 1 ,.,.) = 
  2.2922e-04 -6.6886e-03  1.1839e-03
  8.1434e-04  4.4861e-03 -6.8180e-03
 -2.7884e-03  3.1447e-02  2.0861e-02

(126, 2 ,.,.) = 
  1.0742e-02 -2.3188e-02 -8.3067e-03
  8.8557e-02  3.8402e-03 -1.1184e-01
  2.5634e-02 -2.0695e-02 -2.0699e-02
    ... 

(126,125,.,.) = 
  3.1371e-03 -1.8001e-02 -1.7856e-02
 -1.1404e-02  9.1344e-03  3.5242e-02
  8.3053e-04  1.2192e-02  8.2051e-03

(126,126,.,.) = 
  2.5233e-02  9.5351e-04 -6.2553e-03
 -1.5711e-02 -2.0540e-02 -3.1693e-02
 -6.6347e-03 -1.9945e-02 -1.5864e-02

(126,127,.,.) = 
 -9.2498e-03  5.4429e-03  3.4973e-03
 -4.1350e-02  1.3857e-02  2.2276e-02
  7.5377e-03  1.8090e-02  2.7004e-02
      ⋮  

(127, 0 ,.,.) = 
 -3.6390e-03 -1.6915e-02 -1.4160e-02
  3.0303e-03  1.6015e-03  9.6799e-03
  1.8470e-02  2.3476e-02  1.8043e-02

(127, 1 ,.,.) = 
 -1.8460e-02 -4.0362e-02 -1.1782e-02
 -1.6825e-02  1.9933e-02  1.5326e-03
 -6.4067e-04 -1.1057e-02  5.4452e-03

(127, 2 ,.,.) = 
  4.8412e-03  1.7270e-03  8.2047e-03
  6.2839e-03 -1.5603e-02 -1.3443e-02
  1.8032e-02  3.6521e-02  3.5794e-02
    ... 

(127,125,.,.) = 
 -4.0975e-02 -6.5356e-02 -1.6572e-02
 -7.1748e-02  2.4892e-02 -1.2404e-02
 -5.6559e-03 -3.4650e-03  2.3677e-02

(127,126,.,.) = 
  2.8632e-03  2.8907e-02  4.2334e-03
  1.6220e-02 -1.9336e-02  3.0459e-04
 -2.2507e-02  1.5587e-02  9.9927e-03

(127,127,.,.) = 
 -9.9456e-03  9.8812e-03  2.4514e-03
 -2.0043e-02 -2.1091e-03 -2.1618e-02
  1.1337e-02  1.8307e-02  1.1379e-02
[torch.FloatTensor of size 128x128x3x3]
), ('layer2.2.bn2.weight', 
 0.2214
 0.1721
 0.2176
 0.1959
 0.1742
 0.2177
 0.1805
 0.2151
 0.1563
 0.1343
 0.1279
 0.1864
 0.1404
 0.1347
 0.2264
 0.2030
 0.2185
 0.1454
 0.1638
 0.2064
 0.1385
 0.1764
 0.1671
 0.1952
 0.2158
 0.1879
 0.1612
 0.1774
 0.1355
 0.2028
 0.1216
 0.2157
 0.2119
 0.1582
 0.1622
 0.2311
 0.1990
 0.1647
 0.1607
 0.1608
 0.2214
 0.1909
 0.1611
 0.2305
 0.1727
 0.1899
 0.1830
 0.2176
 0.1987
 0.1904
 0.1906
 0.2067
 0.2098
 0.1184
 0.2161
 0.1449
 0.2168
 0.1576
 0.1104
 0.2230
 0.2038
 0.2066
 0.1992
 0.1962
 0.1973
 0.1898
 0.1507
 0.1287
 0.1824
 0.2102
 0.1911
 0.1703
 0.1391
 0.1661
 0.1218
 0.2001
 0.2231
 0.1790
 0.1083
 0.1760
 0.2181
 0.1710
 0.1995
 0.1270
 0.1417
 0.1999
 0.1815
 0.1847
 0.1651
 0.2537
 0.2222
 0.2202
 0.1587
 0.2057
 0.1708
 0.1237
 0.2065
 0.2275
 0.1513
 0.1368
 0.2161
 0.2136
 0.1753
 0.2129
 0.1749
 0.1589
 0.1498
 0.1623
 0.1935
 0.2039
 0.1690
 0.2222
 0.2065
 0.1721
 0.1346
 0.2136
 0.1893
 0.2136
 0.2059
 0.1386
 0.2209
 0.1666
 0.1613
 0.2016
 0.1986
 0.2145
 0.2138
 0.1231
[torch.FloatTensor of size 128]
), ('layer2.2.bn2.bias', 
-0.0689
-0.0313
-0.0733
-0.0350
-0.0190
-0.0620
 0.0035
 0.0132
-0.0292
 0.2037
 0.1439
-0.0762
 0.0221
 0.1385
-0.0421
-0.0142
-0.0809
 0.0121
-0.0230
-0.0762
 0.0541
-0.0423
-0.0161
-0.0310
-0.0601
 0.0999
 0.0646
 0.1196
 0.0753
-0.1081
 0.1915
-0.0441
-0.0390
 0.0091
-0.0384
-0.0733
-0.0219
 0.0743
-0.0316
 0.0027
-0.0706
-0.0089
 0.0214
-0.0509
 0.0323
-0.0528
-0.0301
-0.1541
-0.0387
-0.0589
-0.0495
-0.0700
-0.0180
 0.0271
-0.1037
-0.0268
-0.1122
 0.0781
 0.2120
-0.0242
-0.0506
-0.0394
-0.0469
-0.1692
-0.0786
 0.0449
 0.0841
 0.2805
-0.0102
-0.0755
 0.0087
-0.0163
 0.0084
-0.0440
 0.1122
-0.0906
-0.0639
 0.0545
 0.1696
 0.0667
-0.0352
-0.0517
-0.0681
 0.1267
 0.1488
-0.0290
-0.0383
-0.0328
 0.0788
-0.1638
-0.1247
-0.0539
-0.0066
 0.0618
 0.0331
 0.0680
-0.0730
-0.1106
 0.0354
 0.0932
-0.0082
-0.1190
-0.0311
-0.0941
-0.0076
 0.1077
 0.0925
 0.0727
 0.0321
 0.0552
-0.0851
-0.0713
-0.0749
 0.0496
 0.0787
-0.1726
 0.0821
-0.1025
-0.0183
 0.0164
-0.1447
 0.0868
 0.0499
-0.0962
-0.0720
-0.0975
-0.0382
 0.1936
[torch.FloatTensor of size 128]
), ('layer2.2.bn2.running_mean', 
-0.0591
 0.0124
-0.0581
 0.0495
-0.0942
-0.1735
 0.1631
 0.0423
-0.0066
 0.0189
 0.0891
-0.2310
-0.0576
 0.0874
 0.1579
 0.0498
-0.0756
 0.0140
-0.1982
-0.1230
-0.0812
-0.0707
-0.0675
 0.1296
-0.0839
 0.1742
 0.1879
 0.2474
-0.0512
-0.1137
-0.3139
 0.1263
-0.1176
-0.0260
-0.1498
-0.0779
 0.0846
-0.0646
-0.0473
 0.0026
 0.1177
 0.0732
-0.0883
-0.0093
 0.1001
 0.0479
-0.0452
-0.1080
 0.0641
-0.0864
-0.0349
 0.0609
 0.0424
-0.1445
-0.1227
 0.0661
-0.0780
 0.0458
 0.0736
 0.0525
-0.1580
-0.1160
 0.0101
 0.0164
-0.0623
 0.1967
 0.0562
-0.1387
 0.1123
-0.0753
-0.0911
-0.0738
-0.0760
 0.1707
-0.1008
-0.1118
-0.1868
 0.0319
-0.1070
 0.1138
 0.0738
-0.0336
-0.0591
 0.1584
 0.0637
-0.0736
-0.1029
-0.0730
 0.2193
 0.0085
-0.0947
-0.0734
 0.0244
 0.1455
 0.1502
-0.1635
-0.1085
 0.0114
-0.3456
-0.1314
 0.0242
-0.0025
 0.1046
-0.0508
-0.0319
 0.1255
 0.1088
 0.1984
 0.1305
 0.1066
-0.1605
-0.0461
-0.1520
-0.1885
-0.1614
 0.1387
 0.1251
 0.0005
 0.0814
 0.0255
-0.1698
 0.1753
-0.1875
-0.1406
 0.0143
-0.0058
 0.0799
-0.1516
[torch.FloatTensor of size 128]
), ('layer2.2.bn2.running_var', 
1.00000e-02 *
  2.3178
  1.4274
  1.9559
  1.6377
  2.3413
  3.0237
  2.8460
  2.6807
  1.5017
  2.6458
  1.7024
  1.6659
  1.0888
  2.0755
  3.7896
  2.6207
  1.8282
  0.8808
  1.2931
  1.5238
  1.0993
  1.3848
  1.2854
  1.9453
  2.1486
  4.9051
  2.4914
  4.2083
  2.1096
  3.3875
  1.5147
  3.6398
  2.3931
  1.3646
  1.2640
  2.5107
  1.8668
  1.7272
  1.6710
  1.8618
  3.1584
  1.9171
  2.0505
  2.4640
  2.6026
  1.8238
  1.3146
  1.4545
  2.5515
  1.9955
  1.8056
  1.7347
  2.8648
  0.8687
  1.5020
  1.4369
  3.6077
  2.9250
  1.3042
  3.6065
  2.3555
  2.7548
  1.5277
  1.2602
  1.4661
  3.2495
  2.1858
  2.2799
  2.3136
  2.1223
  2.6344
  2.0005
  2.7938
  1.3468
  1.4534
  2.6693
  3.0804
  2.9637
  1.0599
  2.4905
  2.2316
  1.1876
  1.3256
  2.1022
  2.4092
  2.1370
  1.7539
  1.4205
  2.7369
  1.0579
  1.4694
  2.6952
  1.6244
  4.0712
  3.4174
  1.7110
  1.8793
  1.9578
  2.3279
  1.6489
  2.8588
  2.2776
  1.3780
  1.9458
  1.7878
  2.4771
  2.1152
  3.3163
  2.3930
  5.2537
  0.7157
  2.3435
  1.9529
  2.2899
  1.8547
  1.0357
  4.1210
  1.8899
  2.1052
  1.3027
  1.2813
  3.1255
  1.7647
  1.4215
  1.2303
  2.0297
  2.6063
  1.9622
[torch.FloatTensor of size 128]
), ('layer2.2.conv3.weight', 
( 0 , 0 ,.,.) = 
 -1.7472e-02

( 0 , 1 ,.,.) = 
  2.6974e-02

( 0 , 2 ,.,.) = 
 -6.1007e-03
    ... 

( 0 ,125,.,.) = 
 -4.4693e-03

( 0 ,126,.,.) = 
 -2.8153e-04

( 0 ,127,.,.) = 
  1.4522e-02
      ⋮  

( 1 , 0 ,.,.) = 
  2.0458e-03

( 1 , 1 ,.,.) = 
 -1.0016e-02

( 1 , 2 ,.,.) = 
 -3.2662e-03
    ... 

( 1 ,125,.,.) = 
  2.6745e-02

( 1 ,126,.,.) = 
 -1.7600e-02

( 1 ,127,.,.) = 
 -2.3394e-02
      ⋮  

( 2 , 0 ,.,.) = 
 -2.1447e-02

( 2 , 1 ,.,.) = 
  3.7622e-03

( 2 , 2 ,.,.) = 
  1.4855e-02
    ... 

( 2 ,125,.,.) = 
 -1.2352e-02

( 2 ,126,.,.) = 
 -1.1636e-04

( 2 ,127,.,.) = 
 -5.3608e-04
...     
      ⋮  

(509, 0 ,.,.) = 
  2.4283e-02

(509, 1 ,.,.) = 
 -2.2397e-02

(509, 2 ,.,.) = 
 -1.1301e-02
    ... 

(509,125,.,.) = 
  6.5993e-03

(509,126,.,.) = 
  6.2060e-03

(509,127,.,.) = 
  1.1466e-02
      ⋮  

(510, 0 ,.,.) = 
  5.3356e-03

(510, 1 ,.,.) = 
  7.6939e-03

(510, 2 ,.,.) = 
  1.3788e-02
    ... 

(510,125,.,.) = 
 -5.2157e-03

(510,126,.,.) = 
  8.2105e-03

(510,127,.,.) = 
  6.3159e-03
      ⋮  

(511, 0 ,.,.) = 
  7.1847e-03

(511, 1 ,.,.) = 
  3.5737e-03

(511, 2 ,.,.) = 
  1.9639e-02
    ... 

(511,125,.,.) = 
 -1.3127e-02

(511,126,.,.) = 
 -6.6329e-03

(511,127,.,.) = 
  1.2715e-03
[torch.FloatTensor of size 512x128x1x1]
), ('layer2.2.bn3.weight', 
 0.0572
 0.0513
 0.0701
 0.2007
 0.2130
 0.1163
 0.0012
 0.1823
 0.1187
 0.1230
 0.1162
 0.0789
 0.0843
 0.0609
 0.1762
 0.0784
 0.0554
 0.1829
 0.0934
 0.1814
 0.0428
 0.0973
 0.0335
 0.1376
 0.0642
 0.2001
 0.1781
 0.1554
 0.1482
 0.1378
 0.0466
 0.1781
 0.0446
 0.1049
 0.0964
 0.0300
 0.0043
 0.0754
 0.2078
 0.0043
 0.0759
 0.0932
 0.2199
 0.0826
 0.1367
 0.1090
 0.0942
 0.1007
 0.0665
 0.0806
 0.1017
 0.1144
 0.2540
 0.0861
 0.0667
 0.0575
 0.1358
 0.0983
 0.0041
 0.2154
 0.0973
 0.0605
 0.1822
 0.0933
 0.0918
 0.1295
 0.1022
 0.0661
 0.0623
 0.0581
 0.0637
 0.1190
 0.0459
 0.1767
 0.0168
 0.1007
 0.0239
 0.0068
 0.0625
 0.0865
 0.0413
 0.0604
 0.0558
 0.1156
 0.1678
 0.0521
 0.0745
 0.1013
 0.0580
 0.2115
 0.0911
 0.0517
 0.0509
 0.1069
 0.1632
 0.1126
 0.0647
 0.0572
 0.0580
 0.0552
 0.1158
 0.0740
-0.0273
-0.0082
 0.2625
 0.1106
 0.1173
 0.1051
 0.0448
 0.1251
 0.0505
 0.1710
 0.2576
 0.1376
 0.0867
 0.0014
 0.0809
 0.0327
 0.0336
 0.1216
 0.1788
 0.1153
 0.0822
 0.1186
 0.2659
 0.1741
 0.0052
 0.0337
 0.1619
 0.1166
 0.1025
 0.1151
 0.1388
 0.0770
 0.0053
 0.2740
 0.0846
 0.1296
 0.1493
 0.1267
 0.0597
 0.1591
 0.1063
 0.1961
 0.0483
 0.0920
 0.1002
 0.0645
 0.0543
 0.1088
 0.0278
 0.1533
 0.1285
 0.1820
 0.1611
 0.0110
 0.1421
 0.0983
 0.0916
 0.1161
 0.1526
 0.0565
 0.0151
 0.0767
 0.0427
 0.2274
 0.0219
 0.2281
 0.0649
 0.1308
 0.1560
-0.0069
 0.0039
-0.0311
 0.1364
 0.0609
 0.0975
 0.0395
 0.1814
 0.0876
 0.1118
 0.1000
 0.0369
 0.0798
 0.0906
 0.0694
 0.1220
 0.0516
 0.0508
 0.1456
 0.0288
 0.0925
 0.1705
 0.1075
 0.1666
 0.0618
 0.0837
 0.0783
 0.1867
 0.0379
 0.3287
 0.1095
 0.0732
 0.2327
 0.1283
 0.1315
 0.1142
 0.0504
 0.0154
 0.3249
 0.0779
 0.0358
 0.0198
 0.1475
 0.1205
 0.0879
 0.2058
 0.0115
 0.0814
 0.0804
 0.0946
 0.1902
 0.0837
 0.1229
 0.0143
 0.1563
 0.0703
 0.1395
 0.0855
 0.0359
 0.1170
 0.1338
 0.1362
 0.2025
 0.1494
 0.0991
 0.0771
 0.0020
-0.0063
 0.1195
 0.0837
 0.1724
 0.0672
 0.0377
 0.1593
 0.0878
 0.0502
 0.0175
 0.0965
 0.1385
 0.1657
 0.1162
 0.1028
 0.0725
 0.1250
 0.1454
-0.0084
 0.0637
 0.1134
 0.0136
 0.1180
 0.0247
 0.0537
-0.0115
 0.1539
-0.0120
 0.3069
 0.0624
 0.0089
 0.0784
 0.1168
 0.1698
 0.1871
 0.0929
 0.1473
 0.0847
 0.1326
 0.1473
 0.1228
 0.1336
 0.0743
 0.2222
 0.0883
 0.1828
 0.1132
 0.0704
 0.1213
 0.0902
 0.0415
 0.1138
 0.0352
 0.2897
 0.0916
 0.0631
 0.0687
 0.0172
 0.1139
 0.1199
 0.1306
 0.1355
 0.1588
 0.0467
 0.1177
 0.1674
 0.0644
 0.0893
 0.2456
 0.1993
 0.1414
 0.0956
 0.0945
 0.0540
 0.1131
 0.1612
 0.1691
 0.0390
 0.1526
 0.1769
 0.1023
 0.1605
 0.0868
 0.0815
 0.1840
 0.2672
 0.0122
 0.1325
 0.1417
 0.1323
 0.1021
 0.1256
 0.0109
 0.0460
 0.1077
 0.1256
 0.1203
 0.1260
 0.0761
 0.0358
 0.0550
 0.1311
 0.1372
 0.1867
 0.1151
 0.1249
 0.1045
 0.0580
 0.1403
 0.0596
 0.1390
-0.0181
 0.0823
 0.1477
 0.0617
 0.0796
 0.1584
 0.0956
 0.1101
 0.1017
 0.0879
 0.1078
 0.0771
 0.0438
 0.1673
 0.0882
 0.1667
 0.0447
 0.1636
-0.0518
 0.0370
 0.0470
 0.0539
 0.0540
 0.0883
 0.0215
 0.0168
 0.0360
 0.1388
 0.1197
 0.2181
 0.0667
-0.0053
 0.0495
 0.0450
 0.1803
 0.0378
 0.1474
 0.1313
 0.0679
 0.0353
 0.0910
 0.0611
 0.0460
 0.0867
 0.0864
 0.0310
 0.1489
 0.0776
 0.1239
 0.0854
 0.1083
 0.1216
 0.0186
 0.0180
 0.1009
 0.1910
 0.0753
 0.0302
 0.1190
 0.1746
 0.2461
 0.0172
 0.0771
 0.1705
 0.2232
 0.0322
 0.0642
 0.2283
 0.0553
 0.1161
 0.1704
 0.1194
 0.0243
 0.0603
 0.0632
 0.1568
 0.2214
 0.0624
 0.0438
 0.0822
 0.1046
 0.0695
 0.1556
 0.0564
 0.0446
 0.1318
 0.0841
 0.1011
 0.0388
 0.1426
 0.1567
 0.1475
 0.0517
 0.0927
 0.1638
 0.1908
 0.0565
 0.1558
 0.0968
 0.0198
 0.1799
 0.1659
 0.2310
 0.1778
 0.0523
 0.1440
 0.1129
 0.0765
 0.0757
 0.0904
 0.0721
 0.0746
 0.0185
 0.1771
 0.1030
 0.2082
 0.0926
 0.0816
 0.0759
 0.0496
 0.0541
 0.1026
 0.0844
 0.1304
 0.0859
 0.1788
 0.0786
-0.0647
 0.1403
 0.0357
 0.0527
 0.1047
 0.1177
 0.1492
 0.0794
 0.0375
 0.1701
 0.0531
 0.0670
 0.1271
 0.0515
 0.0727
 0.1686
 0.1360
 0.0909
 0.0615
 0.1081
 0.0807
 0.1218
 0.1666
 0.0098
 0.1504
 0.0429
 0.1308
 0.1624
 0.1157
 0.0948
 0.0596
 0.2338
 0.0812
 0.0434
 0.0457
 0.0894
[torch.FloatTensor of size 512]
), ('layer2.2.bn3.bias', 
-0.0656
 0.0623
-0.1095
-0.0543
-0.1288
 0.0748
-0.0087
-0.2070
-0.1142
-0.0899
-0.0936
-0.0628
-0.1292
 0.0590
-0.0655
-0.0571
 0.0550
-0.0526
-0.0630
 0.0080
-0.0005
-0.0885
-0.0307
-0.1267
-0.0520
-0.0959
-0.0952
 0.0678
-0.0959
-0.0740
-0.0278
-0.1276
-0.0307
-0.1096
-0.0940
 0.0040
-0.0002
-0.0428
 0.0963
 0.0013
-0.0750
-0.0390
-0.0059
-0.1175
-0.0447
-0.0469
-0.1235
-0.0833
-0.0262
 0.0108
-0.1414
 0.0294
 0.0362
-0.0755
-0.1037
 0.0097
 0.0370
 0.0668
-0.0107
-0.0486
-0.1051
-0.0508
-0.0455
-0.1553
-0.0128
-0.0391
-0.0374
-0.0178
 0.0127
-0.0422
-0.0403
-0.0207
-0.0456
-0.0370
 0.0010
-0.1166
-0.0340
-0.1577
-0.1075
-0.0915
-0.0241
-0.0279
-0.0631
-0.0276
-0.0567
-0.0367
-0.0473
-0.0849
-0.0920
-0.0569
-0.0460
-0.0465
-0.0230
-0.1320
-0.1337
-0.0459
-0.0368
-0.0858
-0.0256
 0.0244
-0.1281
-0.0437
-0.0698
 0.0004
-0.2142
 0.0014
-0.0459
-0.0186
-0.0032
-0.0119
-0.0258
-0.0281
 0.0127
-0.0855
-0.0993
-0.0281
-0.0504
-0.0157
-0.0182
-0.0746
-0.0540
-0.0915
-0.0878
-0.0552
-0.0401
-0.2473
-0.0075
 0.0006
-0.1237
 0.0053
-0.0039
-0.0345
-0.0493
-0.0154
 0.0169
-0.0434
-0.0039
-0.1301
-0.0935
-0.0836
-0.0659
 0.0098
-0.1002
-0.0746
 0.0687
-0.1072
-0.0738
-0.0420
-0.0593
-0.0801
-0.0153
-0.1203
-0.0517
-0.0463
-0.1568
-0.0071
-0.1911
-0.0159
-0.1125
-0.0606
-0.0200
-0.0475
-0.0016
-0.1033
-0.0176
-0.0340
-0.0299
-0.0067
-0.0590
-0.0333
-0.1070
 0.0107
 0.0087
 0.0058
 0.0171
-0.0699
-0.1343
-0.0112
-0.0922
-0.0580
-0.1174
-0.0538
-0.0270
-0.1041
-0.0816
-0.1074
-0.1203
-0.0518
-0.1062
 0.1657
-0.0258
-0.0724
-0.1844
-0.1733
-0.0928
-0.0231
-0.0777
-0.0377
-0.0879
-0.0200
 0.0291
-0.1082
-0.0710
-0.1176
-0.1808
-0.0343
-0.0676
-0.0296
 0.0085
 0.0265
-0.0141
-0.0271
-0.0298
-0.1274
 0.0342
-0.0807
-0.0610
-0.0050
-0.1240
-0.0361
-0.0976
-0.0336
-0.0889
-0.0407
-0.0147
-0.0681
-0.0429
-0.1204
-0.0312
-0.0209
-0.0666
-0.1509
-0.1203
-0.0113
-0.1406
 0.0105
-0.0592
-0.0009
-0.0041
-0.0572
-0.0998
-0.2547
-0.0238
-0.0774
-0.0390
-0.0858
-0.0564
-0.0048
-0.0580
-0.0574
-0.0793
-0.1391
-0.0698
-0.0473
-0.0204
-0.1036
-0.0036
-0.0555
 0.0171
-0.0010
-0.0866
-0.0006
-0.0281
-0.0358
-0.0767
-0.0007
 0.0130
-0.1123
-0.0186
-0.0881
 0.0657
-0.0644
-0.0657
-0.0724
-0.0484
-0.0282
-0.0598
-0.0328
-0.0359
-0.0460
-0.0572
-0.0811
 0.0089
-0.0568
-0.0813
-0.0852
-0.0294
-0.0569
-0.0049
-0.0644
 0.0474
-0.0951
-0.0773
-0.0731
-0.0529
-0.0084
-0.0034
-0.1180
-0.1336
 0.0490
-0.1214
-0.0563
-0.0184
-0.0599
-0.0625
-0.0171
-0.0155
-0.0850
 0.0478
-0.1303
-0.1027
-0.0287
-0.1192
-0.1356
-0.1505
-0.0838
 0.0006
-0.2332
-0.1373
-0.0274
-0.0007
-0.0795
 0.1128
-0.0532
-0.0032
-0.2691
-0.1173
-0.0292
-0.0591
-0.1105
-0.0044
-0.0189
-0.1673
-0.1041
-0.0846
-0.0492
-0.0390
-0.0709
 0.0135
 0.0003
-0.1606
 0.0275
-0.1237
-0.0938
-0.1119
-0.0370
-0.1266
-0.0587
 0.0407
-0.0700
-0.0517
 0.0351
-0.0505
-0.0921
-0.2230
-0.0779
-0.0945
-0.1070
-0.0558
-0.0325
-0.0540
-0.0275
-0.0892
-0.1113
 0.0114
-0.0397
 0.0964
-0.0294
-0.0294
-0.0181
-0.0070
-0.0693
-0.0592
-0.0170
-0.0032
-0.0309
-0.0706
-0.0504
 0.0011
-0.0789
 0.0048
-0.0064
-0.0959
-0.1322
-0.0162
-0.0680
-0.0693
-0.1036
 0.0471
-0.0888
-0.0303
-0.0452
-0.1000
-0.0800
-0.0333
-0.0536
-0.1074
-0.0422
-0.0190
-0.0688
-0.1408
-0.0471
-0.0313
-0.0708
-0.2162
-0.0389
-0.0847
 0.0167
-0.0667
-0.0491
-0.0157
-0.0647
-0.1543
-0.0526
-0.0474
-0.0306
-0.0212
-0.0108
-0.1527
-0.0932
-0.0771
-0.0352
-0.0696
-0.0689
-0.1471
-0.0780
-0.0958
-0.0506
-0.0843
-0.0394
-0.1198
 0.0236
-0.0374
-0.0605
-0.1516
-0.0415
-0.1013
 0.0375
-0.0141
-0.0659
 0.0275
-0.0584
-0.1379
-0.0271
 0.0199
 0.0512
-0.1530
-0.0874
 0.0169
-0.0876
-0.1972
 0.0223
-0.0600
 0.0066
-0.0800
-0.0635
-0.0453
-0.0803
-0.0316
-0.1109
-0.0635
-0.0078
 0.0633
-0.0805
-0.2414
-0.0595
 0.0132
-0.0745
-0.0975
-0.0193
-0.0836
-0.1500
-0.1593
-0.0926
-0.1080
-0.0348
-0.0726
-0.0226
-0.0030
-0.0651
 0.0457
-0.0648
-0.0374
-0.0789
-0.0342
-0.1050
-0.0697
-0.0372
-0.1122
-0.1033
 0.0137
 0.1496
-0.0535
-0.0457
-0.1078
-0.0657
-0.0461
-0.0396
-0.1002
 0.0032
-0.0886
-0.0430
-0.1154
-0.0426
-0.0749
-0.1382
-0.0942
-0.0085
 0.0473
-0.0135
-0.0202
-0.1279
[torch.FloatTensor of size 512]
), ('layer2.2.bn3.running_mean', 
-0.0089
-0.0189
 0.0058
-0.0430
-0.0251
 0.0374
 0.0003
 0.0019
 0.0035
 0.0010
-0.0264
-0.0608
-0.0342
 0.0089
-0.0528
-0.0226
 0.0403
-0.0082
 0.0209
-0.0237
 0.0101
 0.0163
 0.0098
 0.0120
-0.0314
 0.0034
 0.0313
-0.0050
 0.0115
-0.0431
-0.0127
 0.0489
 0.0020
 0.0514
 0.0090
-0.0133
-0.0079
-0.0128
 0.0308
-0.0219
 0.0087
-0.0181
-0.0334
-0.0011
-0.0049
-0.0298
-0.0034
-0.0208
 0.0047
-0.0279
-0.0034
-0.0241
 0.0424
-0.0223
 0.0099
 0.0204
-0.0574
-0.0342
-0.0017
-0.0098
-0.0107
-0.0039
-0.0328
-0.0003
 0.0007
-0.0297
 0.0341
-0.0173
 0.0153
-0.0139
 0.0285
-0.0418
 0.0276
 0.0067
 0.0063
 0.0485
 0.0017
 0.0011
 0.0059
 0.0200
 0.0254
 0.0182
 0.0075
-0.0000
-0.0264
-0.0058
 0.0232
 0.0452
-0.0122
 0.0098
-0.0135
-0.0019
 0.0018
 0.0481
 0.0584
-0.0180
 0.0038
 0.0179
-0.0238
-0.0097
 0.0181
-0.0223
-0.0036
 0.0054
-0.0254
-0.0198
 0.0126
-0.0401
-0.0168
 0.0010
 0.0325
-0.0089
-0.0371
-0.0510
 0.0294
 0.0012
 0.0606
-0.0106
 0.0181
-0.0451
-0.0152
-0.0115
-0.0023
 0.0175
-0.0050
 0.0332
 0.0022
 0.0323
 0.0051
 0.0360
-0.0385
 0.0216
-0.0549
 0.0071
-0.0192
-0.0076
-0.0147
 0.0920
 0.0034
-0.0200
-0.0091
 0.0089
-0.0035
 0.0064
-0.0013
 0.0312
-0.0054
-0.0450
 0.0434
-0.0147
 0.0146
 0.0166
 0.0292
-0.0059
 0.0286
-0.0150
 0.0331
 0.0559
 0.0317
 0.0087
-0.0154
 0.0077
-0.0055
 0.0361
 0.0348
 0.0388
-0.0323
-0.0014
 0.0228
 0.0180
 0.0237
-0.0030
 0.0024
-0.0109
 0.0628
 0.0027
-0.0393
-0.0021
-0.0284
 0.0124
 0.0225
 0.0061
 0.0182
 0.0295
 0.0029
 0.0239
 0.0073
 0.0039
 0.0395
 0.0417
 0.0056
-0.0166
-0.0279
 0.0615
 0.0177
-0.0478
-0.0122
 0.0040
-0.0419
 0.0088
-0.0173
-0.0178
 0.0174
 0.0431
 0.0130
-0.0293
-0.0364
-0.0237
 0.0038
-0.0087
-0.0291
 0.0226
 0.0205
-0.0001
 0.0055
-0.0560
-0.0172
 0.0054
 0.0355
-0.0116
-0.0077
 0.0125
-0.0093
-0.0146
 0.0162
 0.0099
-0.0027
 0.0097
-0.0079
 0.0254
 0.0148
-0.0142
-0.0166
-0.0103
 0.0061
-0.0421
 0.0085
 0.0130
-0.0060
 0.0028
 0.0135
 0.0607
 0.0199
 0.0081
 0.0127
-0.0662
 0.0531
 0.0084
-0.0335
-0.0838
-0.0232
 0.0451
-0.0224
-0.0085
 0.0022
 0.0105
-0.0030
-0.0202
-0.0415
-0.0174
-0.0618
-0.0150
 0.0032
-0.0175
 0.0017
-0.0063
 0.0358
 0.0235
-0.0117
-0.0327
 0.0199
-0.0396
 0.0128
 0.0037
-0.0284
-0.0050
 0.0085
 0.0436
-0.0363
-0.0264
-0.0035
 0.0264
-0.0036
-0.0060
-0.0244
 0.0339
-0.0266
-0.0147
 0.0299
-0.0232
-0.0064
 0.0435
-0.0279
 0.0193
 0.0010
-0.0187
-0.0244
-0.0024
 0.0249
-0.0098
 0.0063
-0.0244
-0.0185
-0.0407
-0.0460
-0.0190
-0.0221
 0.0738
 0.0204
-0.0069
-0.0152
 0.0403
-0.0189
 0.0303
-0.0114
 0.0094
 0.0009
 0.0097
 0.0119
-0.0173
-0.0391
-0.0080
 0.0394
 0.0027
-0.0061
 0.0174
-0.0165
-0.0271
-0.0228
-0.0189
 0.0059
 0.0349
 0.0087
-0.0142
 0.0342
-0.0023
-0.0145
-0.0000
-0.0118
 0.0073
 0.0118
-0.0236
-0.0062
-0.0292
-0.0139
-0.0164
 0.0181
-0.0072
 0.0078
 0.0685
-0.0137
 0.0898
-0.0109
-0.0220
 0.0388
-0.0088
 0.0118
 0.0251
-0.0107
-0.0341
-0.0035
-0.0230
-0.0259
 0.0142
-0.0381
 0.0139
 0.0308
 0.0144
 0.0267
 0.0175
 0.0321
 0.0284
-0.0224
 0.0011
 0.0008
 0.0380
-0.0116
 0.0287
 0.0057
-0.0289
 0.0006
-0.0351
-0.0020
 0.0378
-0.0197
-0.0273
-0.0026
-0.0353
 0.0039
-0.0006
-0.0075
-0.0116
 0.0724
-0.0269
 0.0139
-0.0711
 0.0293
-0.0548
-0.0184
-0.0103
 0.0147
 0.0066
 0.0137
 0.0401
 0.0391
 0.0416
-0.0065
-0.0206
 0.0901
 0.0034
-0.0096
-0.0270
-0.0463
-0.0030
 0.0084
 0.0110
-0.0507
-0.0393
 0.0506
-0.0014
-0.0022
-0.0057
 0.0093
-0.0457
-0.0327
-0.0045
 0.0224
 0.0110
-0.0090
-0.0730
-0.0146
 0.0596
 0.0411
 0.0026
 0.0193
-0.0223
-0.0237
 0.0117
 0.0437
 0.0001
-0.0377
-0.0014
-0.0168
 0.0052
-0.0551
-0.0223
-0.0012
 0.0104
 0.0029
-0.0273
 0.1040
 0.0007
-0.0958
 0.0340
-0.0130
-0.0458
 0.0050
-0.0176
 0.0021
 0.0364
 0.0025
-0.0058
-0.0380
-0.0581
 0.0107
 0.0257
-0.0329
 0.0144
-0.0092
-0.0010
-0.0218
-0.0387
 0.0029
 0.0017
-0.0254
-0.0211
-0.0798
-0.0201
-0.0460
 0.0060
-0.0322
-0.0244
 0.0429
-0.0036
 0.0171
 0.0642
 0.0274
-0.0107
 0.0211
 0.0295
-0.0007
-0.0398
-0.0018
-0.0292
-0.0335
-0.0367
-0.0258
 0.0570
-0.0005
 0.0043
-0.0234
 0.0032
-0.0023
-0.0444
 0.0126
-0.0670
-0.0403
-0.0100
-0.0091
 0.0330
 0.0066
 0.0452
[torch.FloatTensor of size 512]
), ('layer2.2.bn3.running_var', 
1.00000e-03 *
  0.5843
  0.9373
  0.4874
  2.9589
  2.2107
  1.6918
  0.2487
  1.7293
  0.7951
  1.7783
  0.8456
  0.7854
  0.8096
  1.4888
  2.5226
  0.9060
  2.1978
  1.7684
  1.1984
  2.9734
  0.3480
  1.0718
  0.4426
  2.1073
  0.9145
  3.2855
  1.4708
  3.1134
  2.2162
  3.0053
  0.4139
  4.4856
  0.7058
  0.9016
  0.8095
  0.4208
  0.5868
  1.3208
  3.7085
  0.5852
  1.0288
  1.2530
  4.0427
  0.5780
  2.4196
  1.3960
  0.6189
  0.9784
  0.9523
  1.2274
  0.9899
  1.7376
  7.4960
  1.3555
  0.8968
  0.7983
  1.7512
  1.2802
  0.3303
  2.6455
  0.5689
  0.4815
  2.8791
  0.8216
  0.8926
  1.7355
  1.1120
  1.1984
  0.7282
  0.4870
  0.9705
  1.7831
  0.5807
  2.8578
  0.2696
  1.3277
  0.3290
  0.2759
  0.8411
  0.5846
  0.3383
  0.6368
  0.4262
  1.6706
  1.8660
  1.0707
  1.1143
  1.2707
  0.6804
  3.7445
  1.3601
  0.4379
  0.3735
  0.8791
  3.0064
  1.3861
  0.7261
  0.5739
  0.5613
  0.7802
  1.4366
  0.4691
  0.1702
  0.2274
  4.6040
  1.2354
  2.0605
  0.9557
  0.8116
  1.3599
  0.9577
  4.4228
  5.1137
  1.4425
  1.2537
  0.3042
  0.6198
  0.2541
  0.6952
  1.5777
  2.3847
  1.8766
  0.8554
  1.5168
  5.3460
  2.6460
  0.2157
  0.4227
  1.3849
  1.9510
  1.9550
  1.2864
  1.3953
  0.7576
  0.2881
  5.8690
  1.1545
  1.4991
  1.8554
  1.2793
  0.6444
  3.1211
  1.0752
  2.7015
  0.7862
  1.0116
  0.6542
  0.6594
  1.0039
  1.4368
  0.4621
  2.1449
  2.3537
  2.9572
  1.9934
  0.2779
  1.4101
  1.2074
  1.4188
  1.3181
  2.2918
  1.5263
  0.2549
  0.8877
  0.3736
  3.2119
  0.3283
  4.0225
  0.5960
  1.3085
  1.8351
  0.4847
  0.5009
  0.5542
  2.2913
  0.7851
  1.5088
  0.4732
  1.4538
  0.9364
  0.6648
  1.4381
  0.4152
  0.9423
  1.1469
  1.0752
  1.7343
  0.4154
  0.8595
  2.9852
  0.4485
  1.1586
  2.8608
  1.7709
  2.5260
  0.7149
  1.0982
  0.9631
  2.8423
  0.6349
  6.7430
  2.4898
  1.1905
  3.2284
  1.2770
  1.1410
  1.3540
  0.7679
  0.6220
  8.6511
  1.2764
  0.5955
  0.7665
  1.8761
  1.5866
  0.7584
  4.0646
  0.5748
  1.0897
  0.9193
  1.0276
  3.1761
  0.7671
  1.2628
  0.1891
  3.3090
  0.8371
  1.8970
  1.0096
  0.6024
  1.3473
  1.7995
  2.5057
  3.0864
  2.0340
  1.5869
  0.5968
  0.2265
  0.2641
  1.7253
  0.8385
  2.7264
  0.5771
  0.5722
  1.8144
  0.7692
  0.3208
  0.3302
  0.8134
  1.4817
  3.1488
  2.7127
  1.2184
  0.7698
  1.5984
  1.2458
  0.1097
  0.7127
  1.7132
  0.3630
  0.9638
  0.4245
  0.5048
  0.1795
  2.5195
  0.3590
  8.3632
  0.5602
  0.2243
  0.6489
  1.8241
  2.1224
  2.7097
  1.5871
  2.3847
  1.0683
  1.3412
  1.2864
  1.0523
  2.0026
  0.6682
  2.5003
  1.0058
  1.9062
  0.9317
  0.9125
  1.7779
  1.0426
  0.9083
  1.0376
  0.6814
  3.8974
  0.8443
  1.0372
  0.5721
  0.2880
  1.6274
  1.7130
  1.0363
  1.7368
  1.8726
  0.3466
  0.9777
  1.8695
  0.5603
  1.2121
  3.4033
  2.2467
  2.0949
  0.4763
  0.7441
  0.8313
  1.2251
  1.8647
  1.3307
  0.5516
  2.1364
  2.9172
  1.0822
  1.9612
  1.3120
  1.0677
  3.6700
  3.4397
  0.2670
  1.6340
  1.1705
  2.1020
  1.1906
  2.0110
  0.3117
  0.4913
  1.1792
  0.9169
  1.1987
  1.5058
  0.6583
  0.4410
  0.4622
  2.1254
  1.8478
  4.0851
  1.5389
  1.3750
  1.3290
  0.6579
  1.6681
  0.9277
  2.6661
  0.6873
  1.1769
  3.2503
  0.9040
  0.7933
  1.9569
  1.2828
  1.0587
  1.5793
  1.0441
  1.8010
  1.0372
  0.7813
  2.1313
  0.5979
  3.2148
  1.0612
  2.9333
  0.2346
  0.8863
  0.4496
  1.0675
  0.8198
  1.9932
  0.3858
  0.3698
  0.2551
  1.3476
  2.0129
  2.4359
  0.6633
  0.2185
  0.5499
  0.4905
  2.5817
  0.8133
  2.0251
  1.6442
  0.9153
  1.4446
  1.1427
  1.0199
  0.7318
  1.3089
  0.6746
  0.3781
  2.6381
  1.3063
  0.9502
  1.1837
  1.6074
  1.4793
  0.3837
  0.3870
  1.0295
  2.0462
  1.0944
  0.5965
  1.7018
  1.7269
  4.2060
  0.1561
  1.0436
  3.5877
  4.2037
  0.6033
  0.5093
  2.5667
  0.7876
  0.8862
  2.5780
  1.3223
  0.3189
  0.4764
  0.5523
  1.4085
  2.6858
  0.7300
  0.7195
  1.0055
  1.0101
  0.8468
  1.8631
  0.9948
  0.3314
  1.1789
  0.7781
  1.0900
  0.6757
  1.8492
  2.2741
  2.0714
  0.6630
  0.6730
  2.7331
  2.9091
  1.6293
  1.8619
  1.8298
  0.3217
  2.2891
  1.5103
  3.8859
  1.8400
  0.6385
  1.6208
  1.4050
  0.9662
  0.4692
  1.5953
  2.2268
  0.4681
  0.3878
  2.4426
  1.0671
  3.5377
  1.0341
  0.8439
  0.9253
  0.5698
  0.4544
  0.9224
  1.0378
  2.3009
  0.9021
  2.9539
  1.0866
  1.2270
  1.2028
  0.5425
  0.6350
  1.0583
  1.4383
  2.0878
  0.8866
  0.3490
  1.8204
  0.5690
  0.7098
  0.9714
  0.9519
  0.7497
  2.9380
  1.4909
  1.6604
  1.2280
  1.0398
  0.9842
  1.0767
  2.3152
  0.2081
  1.4053
  0.3933
  1.5386
  2.8683
  1.5340
  0.9202
  0.5134
  5.0844
  0.8439
  0.5572
  0.5938
  1.4848
[torch.FloatTensor of size 512]
), ('layer2.3.conv1.weight', 
( 0 , 0 ,.,.) = 
  3.8050e-03

( 0 , 1 ,.,.) = 
 -1.9299e-02

( 0 , 2 ,.,.) = 
  1.6487e-02
    ... 

( 0 ,509,.,.) = 
  5.6285e-03

( 0 ,510,.,.) = 
  2.5861e-02

( 0 ,511,.,.) = 
 -4.5902e-03
      ⋮  

( 1 , 0 ,.,.) = 
  2.2032e-02

( 1 , 1 ,.,.) = 
 -1.5341e-02

( 1 , 2 ,.,.) = 
 -1.6618e-03
    ... 

( 1 ,509,.,.) = 
  1.2841e-02

( 1 ,510,.,.) = 
  2.2107e-02

( 1 ,511,.,.) = 
  5.3425e-03
      ⋮  

( 2 , 0 ,.,.) = 
 -1.1393e-02

( 2 , 1 ,.,.) = 
 -5.1433e-04

( 2 , 2 ,.,.) = 
  1.1396e-02
    ... 

( 2 ,509,.,.) = 
  8.2032e-03

( 2 ,510,.,.) = 
  4.3621e-02

( 2 ,511,.,.) = 
  2.3429e-03
...     
      ⋮  

(125, 0 ,.,.) = 
  1.9219e-02

(125, 1 ,.,.) = 
 -2.8844e-02

(125, 2 ,.,.) = 
 -8.4328e-03
    ... 

(125,509,.,.) = 
 -1.7968e-02

(125,510,.,.) = 
  1.3575e-02

(125,511,.,.) = 
  8.0357e-03
      ⋮  

(126, 0 ,.,.) = 
  2.6217e-03

(126, 1 ,.,.) = 
 -6.2132e-03

(126, 2 ,.,.) = 
 -3.7966e-03
    ... 

(126,509,.,.) = 
 -2.9308e-03

(126,510,.,.) = 
 -7.1205e-03

(126,511,.,.) = 
  9.2810e-03
      ⋮  

(127, 0 ,.,.) = 
  8.9967e-03

(127, 1 ,.,.) = 
 -1.7275e-02

(127, 2 ,.,.) = 
 -6.6972e-03
    ... 

(127,509,.,.) = 
  2.3240e-03

(127,510,.,.) = 
 -1.4363e-03

(127,511,.,.) = 
 -1.8595e-02
[torch.FloatTensor of size 128x512x1x1]
), ('layer2.3.bn1.weight', 
 0.1933
 0.1745
 0.2015
 0.1434
 0.1982
 0.1646
 0.1300
 0.1550
 0.1509
 0.1886
 0.1708
 0.1808
 0.1298
 0.1940
 0.1376
 0.2050
 0.1648
 0.2263
 0.2375
 0.1856
 0.1696
 0.1560
 0.1636
 0.1629
 0.1736
 0.1691
 0.1799
 0.1843
 0.1190
 0.1901
 0.1574
 0.1613
 0.1489
 0.2024
 0.1434
 0.1643
 0.1377
 0.1842
 0.1626
 0.1889
 0.1453
 0.2046
 0.1646
 0.1712
 0.1745
 0.1644
 0.1686
 0.1949
 0.1360
 0.1764
 0.1926
 0.1554
 0.1822
 0.1575
 0.1424
 0.1599
 0.1876
 0.1632
 0.2079
 0.1231
 0.1688
 0.1290
 0.2197
 0.1295
 0.1555
 0.1975
 0.1702
 0.1712
 0.2074
 0.1815
 0.1759
 0.1368
 0.1868
 0.1720
 0.1879
 0.1352
 0.1620
 0.1334
 0.1505
 0.1787
 0.1253
 0.1357
 0.1478
 0.1604
 0.1495
 0.1609
 0.1859
 0.1653
 0.1574
 0.1922
 0.1444
 0.1965
 0.1663
 0.1840
 0.1670
 0.1751
 0.1812
 0.1994
 0.2035
 0.1679
 0.1741
 0.1648
 0.1720
 0.1863
 0.1407
 0.1734
 0.1615
 0.1712
 0.1470
 0.1726
 0.1928
 0.1893
 0.1618
 0.1682
 0.2273
 0.1751
 0.2389
 0.1601
 0.1422
 0.2083
 0.2071
 0.1580
 0.1803
 0.1380
 0.1505
 0.1481
 0.1653
 0.1743
[torch.FloatTensor of size 128]
), ('layer2.3.bn1.bias', 
-0.1208
-0.0083
-0.1050
 0.0442
-0.0881
-0.0430
 0.0200
 0.0252
 0.0549
-0.1148
 0.0246
 0.0192
 0.0552
-0.1162
 0.1111
-0.1053
-0.0561
-0.1035
-0.1617
-0.0660
 0.0272
 0.0192
-0.0761
-0.0998
-0.0142
-0.0216
-0.0481
-0.0464
 0.0950
-0.1267
-0.0593
-0.0478
 0.0002
-0.0991
 0.0335
 0.0238
-0.0098
-0.0871
 0.0215
-0.1557
 0.0375
-0.2180
-0.0507
 0.0625
-0.0563
-0.0467
 0.0418
-0.0344
-0.0192
-0.0732
-0.0354
-0.0145
-0.0933
 0.0265
-0.0063
-0.0113
-0.0038
 0.0514
-0.1111
 0.0763
-0.0559
 0.0167
-0.1314
 0.0433
 0.0502
-0.0397
-0.0432
 0.0063
-0.1221
-0.1017
-0.0252
 0.0020
-0.1009
 0.0219
-0.1026
-0.0127
 0.0382
 0.0702
-0.0331
-0.1167
 0.1101
 0.0528
 0.1014
 0.0395
 0.0331
 0.0388
-0.0369
-0.0211
 0.0288
 0.0176
 0.0309
-0.1166
-0.0035
-0.0111
 0.0483
-0.0758
-0.0648
-0.0809
-0.1003
-0.0029
-0.0275
-0.0038
-0.0795
-0.1249
 0.0298
-0.0722
 0.0292
-0.0006
 0.0562
-0.0073
 0.0240
-0.0383
-0.0890
 0.0006
-0.1483
-0.0649
-0.2346
 0.0130
 0.0390
-0.1281
-0.1263
-0.0202
-0.0838
 0.0609
 0.0648
 0.0477
 0.0003
-0.0583
[torch.FloatTensor of size 128]
), ('layer2.3.bn1.running_mean', 
-0.0793
-0.1010
 0.0467
-0.1661
-0.0114
 0.0804
 0.0195
-0.0662
-0.0537
 0.1003
 0.0233
 0.0151
-0.0747
-0.0647
 0.1548
-0.1208
 0.0896
-0.1079
-0.1557
 0.0597
-0.0445
-0.1505
-0.0691
 0.0246
-0.2771
-0.1641
-0.0643
 0.0038
 0.0272
 0.0951
-0.0201
 0.0343
-0.0710
-0.0547
-0.1085
-0.1500
 0.0997
-0.0170
-0.0769
-0.0379
-0.1906
-0.1519
 0.0239
-0.1301
 0.0650
 0.0131
-0.2698
-0.0779
 0.0473
-0.0501
-0.0305
-0.0423
 0.1331
-0.1527
-0.1188
-0.0524
-0.0560
-0.2254
-0.1134
 0.0672
 0.0165
-0.1374
-0.0765
-0.0958
-0.1687
-0.1591
 0.0803
-0.0235
-0.0562
 0.0269
-0.1045
 0.0854
-0.0366
-0.1868
 0.0549
-0.1035
-0.0722
-0.1379
 0.1137
-0.0188
 0.0164
-0.0980
-0.0645
-0.0055
-0.1114
-0.2107
 0.0360
-0.0635
-0.2317
-0.0465
-0.0748
 0.0393
-0.0818
-0.1568
-0.1422
 0.0444
-0.2566
-0.0422
 0.0923
-0.0784
-0.0428
-0.1718
 0.1246
-0.0589
-0.2273
 0.0109
-0.0031
-0.0863
-0.1057
-0.0381
-0.2502
 0.0391
 0.1161
-0.0756
-0.0834
-0.1092
-0.0676
-0.1526
 0.0545
-0.1914
-0.0943
-0.0577
 0.0865
-0.0559
-0.1692
-0.1211
-0.1528
-0.0735
[torch.FloatTensor of size 128]
), ('layer2.3.bn1.running_var', 
1.00000e-02 *
  2.0440
  3.1330
  2.2986
  2.1338
  1.8435
  1.7245
  1.6734
  2.9083
  3.4513
  2.0467
  3.2726
  2.9355
  1.7991
  2.1822
  3.0219
  2.4365
  1.8288
  2.5533
  2.2021
  2.8855
  2.8954
  2.6543
  2.0901
  1.3273
  2.2744
  2.3545
  2.5600
  3.3110
  1.9133
  1.6773
  2.0385
  2.2962
  1.8984
  2.6818
  1.9076
  2.3868
  1.3939
  1.9312
  2.9537
  1.6104
  2.7970
  0.9458
  2.2885
  3.9091
  2.5649
  1.7421
  2.7965
  3.4157
  0.9402
  1.7416
  2.3068
  2.5949
  2.0624
  2.4612
  1.8078
  1.9133
  3.9874
  4.0211
  1.9978
  1.3839
  2.1655
  2.0535
  2.2133
  1.9762
  2.8981
  3.3224
  2.7070
  3.4564
  2.2767
  1.9705
  2.5730
  1.9308
  2.0056
  3.3392
  1.6422
  2.0935
  3.5896
  2.2117
  2.2962
  1.3914
  2.1628
  2.2796
  2.6736
  2.6554
  2.6289
  2.8866
  3.2328
  2.3764
  3.5946
  3.0005
  1.8817
  2.0740
  1.7701
  3.3369
  3.2294
  1.9007
  2.0104
  2.6156
  3.0526
  2.8991
  2.3545
  2.3272
  1.7486
  1.6733
  2.2055
  1.6950
  2.5824
  3.2668
  3.2759
  3.0794
  4.2335
  2.3770
  1.6140
  2.9002
  2.7634
  1.8822
  1.3223
  1.9228
  2.1764
  2.1862
  1.5817
  3.0276
  2.1565
  2.2849
  3.4964
  2.5748
  2.4102
  2.3478
[torch.FloatTensor of size 128]
), ('layer2.3.conv2.weight', 
( 0 , 0 ,.,.) = 
  6.3425e-03  7.8066e-03 -5.9981e-03
 -3.4487e-03  4.7611e-03  3.8540e-03
  2.0846e-02  1.8216e-03 -6.1446e-04

( 0 , 1 ,.,.) = 
  1.6795e-03  3.0943e-03  1.6706e-02
  1.7158e-03  5.4718e-05  5.9817e-03
 -7.7672e-04 -5.2933e-03  8.9279e-03

( 0 , 2 ,.,.) = 
  2.2518e-02  3.8244e-02  1.8780e-02
  1.9846e-02  1.4106e-02  2.1835e-02
  1.9422e-02  1.2194e-02  1.0690e-03
    ... 

( 0 ,125,.,.) = 
  3.5645e-02  6.1268e-03 -1.5047e-02
  3.1574e-02 -1.9045e-02 -4.1200e-02
  2.8079e-02 -5.6861e-03 -3.0632e-02

( 0 ,126,.,.) = 
  1.9894e-02  1.8100e-02 -2.4647e-02
 -5.3777e-04  1.0986e-02 -1.7435e-03
  2.4066e-02  2.0868e-02 -2.3315e-02

( 0 ,127,.,.) = 
 -2.0758e-02  1.1063e-03  1.4957e-02
 -3.6696e-02 -1.4199e-02  1.0456e-02
 -2.8005e-02  5.4784e-03  3.3380e-02
      ⋮  

( 1 , 0 ,.,.) = 
  2.1024e-03  2.4975e-02  2.7121e-02
  4.1890e-03 -1.5631e-02  4.6780e-03
  1.5731e-02 -1.1325e-02 -3.7648e-02

( 1 , 1 ,.,.) = 
 -4.8578e-03 -1.5355e-02  2.4391e-02
  7.5786e-03 -3.4544e-02  3.7200e-02
  2.8405e-02  2.8893e-02  1.8144e-02

( 1 , 2 ,.,.) = 
 -1.7961e-02 -1.7295e-02 -3.5341e-02
  5.9240e-02 -2.0096e-02 -2.5839e-02
  2.0119e-02  9.4796e-02  4.8867e-02
    ... 

( 1 ,125,.,.) = 
  3.0112e-02  2.4018e-02 -1.0833e-02
  1.6535e-02  1.0890e-02  5.1681e-03
  6.1969e-03 -2.8730e-02 -2.5644e-02

( 1 ,126,.,.) = 
  1.0316e-02  1.1595e-02  9.7774e-03
  1.1251e-02 -2.3838e-02  5.2018e-03
  2.0050e-02 -1.5464e-02 -4.7344e-02

( 1 ,127,.,.) = 
 -2.3275e-02 -3.1601e-02 -3.3325e-02
 -7.1235e-03 -3.8596e-02  4.8014e-03
  2.6929e-02  2.7941e-02  1.2472e-02
      ⋮  

( 2 , 0 ,.,.) = 
 -2.6790e-03 -7.8069e-03  1.5697e-02
 -9.2817e-03 -3.1359e-02 -1.8011e-02
 -7.4429e-03 -9.8681e-03 -4.8270e-03

( 2 , 1 ,.,.) = 
  7.6149e-03  2.3068e-03 -3.1427e-03
 -2.6594e-02 -5.7359e-03  3.9425e-03
 -1.8057e-02 -1.6741e-02 -2.0627e-02

( 2 , 2 ,.,.) = 
  2.0367e-02 -1.4038e-02 -1.4850e-02
  7.0032e-03 -9.1246e-04 -6.7762e-03
  1.2642e-02  3.0074e-02  3.2019e-02
    ... 

( 2 ,125,.,.) = 
 -1.3792e-02  5.0076e-03  9.4594e-04
 -7.0901e-03 -3.7710e-03 -2.1320e-02
 -1.9195e-02  5.0308e-03  2.0809e-02

( 2 ,126,.,.) = 
 -1.3301e-02 -1.6120e-02 -2.1279e-02
  4.8702e-02  1.5276e-02 -3.6071e-02
  8.6012e-04  6.2694e-03  2.2975e-02

( 2 ,127,.,.) = 
 -3.5448e-03 -5.4041e-03 -2.2905e-02
 -1.7587e-02 -6.1906e-03  6.8526e-03
  1.9518e-02  2.4120e-02  2.8079e-02
...     
      ⋮  

(125, 0 ,.,.) = 
 -5.8243e-02 -1.9527e-03 -2.0026e-02
 -4.2831e-02 -5.2364e-03  2.0417e-02
 -8.8020e-03 -6.2901e-02 -1.4732e-02

(125, 1 ,.,.) = 
 -1.2308e-02 -1.0047e-02 -9.6017e-03
 -2.7270e-02  3.3299e-02 -1.5759e-02
 -1.0160e-02  1.7439e-02 -3.3807e-03

(125, 2 ,.,.) = 
  3.1335e-02 -2.1841e-03 -3.8891e-03
  2.1179e-03  4.4575e-02 -8.2318e-02
 -1.9756e-02  1.9435e-02 -4.2395e-02
    ... 

(125,125,.,.) = 
  1.8102e-02  1.3693e-02  6.4129e-03
  2.1682e-02 -5.6668e-04  2.9968e-02
 -5.9797e-03 -2.3583e-02  2.4803e-02

(125,126,.,.) = 
 -1.1293e-02 -1.1813e-02 -2.6792e-02
  4.6765e-03 -1.4267e-02 -1.4190e-02
 -1.2220e-02  1.2351e-02  8.6050e-03

(125,127,.,.) = 
  2.3794e-02 -3.8109e-02  3.3384e-02
  4.5388e-02 -1.8319e-03 -4.8194e-02
  1.8151e-02 -1.4792e-02 -4.6150e-02
      ⋮  

(126, 0 ,.,.) = 
 -7.3098e-03  1.2843e-02 -2.0237e-02
  1.2503e-03 -6.0009e-03 -8.6790e-03
  1.3704e-02  1.5641e-02  3.3672e-03

(126, 1 ,.,.) = 
  5.9067e-03  2.6577e-02 -1.8495e-02
 -3.7595e-02 -1.3465e-02  2.9414e-02
 -9.6436e-04 -4.6155e-03 -5.7871e-03

(126, 2 ,.,.) = 
  6.6646e-02 -6.5920e-03 -3.2667e-02
  4.1016e-04  2.0832e-02  6.2238e-02
 -1.2086e-02 -1.4311e-02 -1.2527e-02
    ... 

(126,125,.,.) = 
 -2.2447e-02  7.2431e-03 -3.7030e-06
 -3.0055e-02 -1.7382e-02  2.8570e-03
  3.5450e-03 -1.1372e-02 -3.7529e-02

(126,126,.,.) = 
  2.5008e-02  1.0940e-02  1.3994e-02
  2.0757e-02  1.1284e-02 -4.4114e-02
 -3.7229e-03 -5.7655e-03 -6.9855e-03

(126,127,.,.) = 
  1.2280e-03  7.1947e-03 -2.1987e-03
 -1.3080e-02 -8.5207e-03 -1.9941e-03
 -1.2186e-03  5.4275e-03  6.8117e-05
      ⋮  

(127, 0 ,.,.) = 
  1.0657e-02 -6.6865e-03 -1.1085e-02
  3.7792e-02  4.7979e-03 -2.5554e-02
 -1.0018e-02 -9.7269e-03 -3.1181e-02

(127, 1 ,.,.) = 
 -1.2208e-02 -1.8808e-02 -3.8413e-02
 -4.0185e-02 -4.9522e-03  1.8202e-02
 -4.4533e-02 -1.2457e-02  2.1485e-03

(127, 2 ,.,.) = 
  6.8635e-03 -1.0885e-02  3.5082e-03
  2.6554e-02  8.0605e-02  3.8870e-02
  1.0307e-02  6.1691e-02  4.6724e-03
    ... 

(127,125,.,.) = 
 -2.8504e-04 -3.1935e-02 -3.5198e-02
  8.4391e-03 -2.8120e-02  1.4435e-02
 -1.0772e-02 -1.5768e-02  1.6674e-02

(127,126,.,.) = 
 -1.2566e-02 -4.5324e-03 -9.2577e-03
  4.5811e-04 -9.4321e-03  8.5911e-03
 -9.6112e-03 -9.8607e-04 -1.4677e-02

(127,127,.,.) = 
  1.1885e-02 -1.1211e-04  2.4021e-03
 -3.1185e-02 -2.3945e-02  2.1897e-02
  3.7961e-03 -2.4228e-02 -1.7371e-02
[torch.FloatTensor of size 128x128x3x3]
), ('layer2.3.bn2.weight', 
 0.2074
 0.2474
 0.1677
 0.2425
 0.2163
 0.1955
 0.2321
 0.2058
 0.2101
 0.2308
 0.2240
 0.1516
 0.1945
 0.2006
 0.2096
 0.2184
 0.1866
 0.1734
 0.1951
 0.2249
 0.2122
 0.1948
 0.2335
 0.2273
 0.2185
 0.1714
 0.1980
 0.2037
 0.1971
 0.2373
 0.1454
 0.1916
 0.2003
 0.1877
 0.2570
 0.1964
 0.1639
 0.1766
 0.1626
 0.2256
 0.2051
 0.1815
 0.2470
 0.2398
 0.2179
 0.2355
 0.2473
 0.1630
 0.1952
 0.2093
 0.1856
 0.2490
 0.2296
 0.2589
 0.2215
 0.2276
 0.1625
 0.1952
 0.1762
 0.1658
 0.2189
 0.2460
 0.1902
 0.2486
 0.2318
 0.1867
 0.2165
 0.2205
 0.2127
 0.1793
 0.1959
 0.1964
 0.1633
 0.2262
 0.2057
 0.1440
 0.2084
 0.1799
 0.1784
 0.1881
 0.1870
 0.1212
 0.1888
 0.1679
 0.1979
 0.2345
 0.1304
 0.1329
 0.2103
 0.2453
 0.1983
 0.2224
 0.1246
 0.2273
 0.1703
 0.2264
 0.1801
 0.1716
 0.2181
 0.1845
 0.1691
 0.1713
 0.1945
 0.1828
 0.2058
 0.2348
 0.2278
 0.2363
 0.2057
 0.2483
 0.2212
 0.2356
 0.2158
 0.1990
 0.2075
 0.2485
 0.2038
 0.1909
 0.1976
 0.2183
 0.1872
 0.1936
 0.1987
 0.1839
 0.2238
 0.2061
 0.1845
 0.2437
[torch.FloatTensor of size 128]
), ('layer2.3.bn2.bias', 
-0.0622
-0.1638
-0.0352
-0.1200
-0.0676
-0.0391
-0.1209
-0.1094
-0.1203
-0.1041
-0.0943
 0.0809
-0.0195
-0.1124
-0.1269
-0.0329
-0.0352
-0.0932
-0.0577
-0.0920
-0.1281
-0.0832
-0.1282
-0.0824
-0.0811
-0.0376
-0.0664
-0.0705
-0.0845
-0.1240
 0.0886
-0.0434
-0.0452
-0.0660
-0.1429
-0.0634
-0.0443
-0.0639
 0.0263
-0.0819
-0.0729
-0.0702
-0.1614
-0.1583
-0.1276
-0.1568
-0.1578
 0.0259
-0.0329
-0.0637
-0.0773
-0.1483
-0.1030
-0.1641
-0.0940
-0.0873
 0.0424
-0.0460
-0.0348
 0.0079
-0.0671
-0.1516
-0.0784
-0.1361
-0.0619
-0.0370
-0.0766
-0.2178
-0.0514
-0.0255
-0.0466
-0.0802
 0.0029
-0.1203
-0.0548
 0.0834
-0.1030
-0.0586
 0.0534
-0.0506
-0.0351
 0.1891
-0.0610
-0.0689
-0.0343
-0.0948
 0.2578
 0.1662
-0.0801
-0.0803
-0.0505
-0.0875
 0.2776
-0.0929
 0.0497
-0.1199
 0.0051
-0.0111
-0.0731
-0.0604
-0.0234
-0.0865
-0.0067
 0.1228
-0.0501
-0.1657
-0.1405
-0.1211
-0.0772
-0.1539
-0.1529
-0.1700
-0.1135
-0.0868
-0.0574
-0.1691
-0.0490
-0.0817
-0.0362
-0.0600
-0.0508
-0.0279
-0.0495
-0.0010
-0.1255
-0.0026
-0.0604
-0.1522
[torch.FloatTensor of size 128]
), ('layer2.3.bn2.running_mean', 
-0.1526
-0.0054
-0.0595
-0.0455
 0.0092
 0.0067
-0.0438
-0.1263
-0.0385
-0.0749
-0.0851
 0.0703
-0.0498
-0.0875
-0.0615
-0.0660
-0.0886
-0.1515
 0.4644
-0.0771
-0.0478
-0.0400
-0.0586
-0.0804
 0.0146
-0.0733
-0.0220
-0.0650
-0.0429
 0.0279
-0.0922
-0.0545
-0.1161
-0.0218
-0.0386
-0.0971
-0.1419
-0.0601
-0.0189
-0.0605
-0.1221
-0.0366
-0.0787
-0.0302
 0.0950
 0.0133
-0.0959
-0.1253
-0.0580
-0.0893
-0.1129
 0.0094
-0.0741
-0.0398
-0.0322
-0.0816
 0.0225
-0.0629
-0.0346
-0.0145
-0.0341
-0.0915
 0.0208
-0.0423
-0.0392
-0.0606
-0.0434
 0.1506
-0.0112
-0.0441
-0.0104
-0.0748
-0.0274
-0.0116
-0.0536
-0.0416
-0.0239
-0.0548
 0.1139
-0.0186
-0.0386
-0.0825
 0.0002
 0.0090
-0.0386
-0.0665
-0.0984
-0.0259
-0.0632
-0.1280
-0.0072
-0.0340
-0.2022
-0.0393
 0.0700
-0.0132
-0.0303
 0.0072
-0.0578
-0.0259
-0.0809
-0.0987
-0.1101
-0.0932
-0.0706
-0.0963
-0.1588
-0.0987
-0.0978
-0.0407
-0.1281
-0.0924
-0.0644
 0.0258
-0.0445
-0.0538
-0.0563
-0.0910
-0.0673
-0.0390
-0.0363
 0.0928
-0.0663
-0.0519
-0.0483
-0.1081
-0.0939
 0.1629
[torch.FloatTensor of size 128]
), ('layer2.3.bn2.running_var', 
1.00000e-02 *
  1.9742
  1.3495
  1.0904
  1.8249
  1.6168
  1.3784
  1.9610
  1.2637
  1.5643
  1.7569
  1.8663
  1.8552
  1.3895
  0.9572
  1.0032
  1.7974
  1.2118
  0.8028
  2.0969
  1.5549
  1.4468
  1.4091
  1.6317
  1.6699
  1.5979
  1.3131
  1.1546
  1.3981
  1.1524
  1.2490
  1.0201
  1.9099
  1.2936
  1.1204
  1.8659
  1.1543
  1.6661
  0.9645
  0.9461
  1.4442
  1.7068
  1.2729
  1.6821
  1.4800
  1.5089
  1.4111
  1.6788
  1.3613
  1.6005
  2.4430
  1.0798
  1.3539
  1.3312
  1.7005
  1.7887
  1.7743
  1.9517
  1.6318
  1.1894
  1.1399
  1.7172
  1.5075
  1.3327
  1.4779
  2.0304
  1.6395
  1.2316
  1.1784
  2.1996
  1.2184
  1.3704
  1.1642
  1.0949
  1.9532
  1.4164
  1.7190
  1.4150
  1.0836
  1.3854
  1.1833
  1.0410
  1.0888
  1.6013
  0.8775
  1.5383
  1.7572
  1.3860
  1.7971
  1.7305
  1.9692
  1.7394
  1.9833
  1.6943
  2.2705
  2.2844
  1.2680
  1.1306
  1.2425
  1.4117
  1.1494
  1.3688
  0.8327
  1.6042
  4.4853
  1.8879
  1.2332
  1.5338
  1.5607
  1.1177
  1.4042
  1.1803
  1.4318
  1.1429
  2.1012
  2.2415
  1.8750
  2.0938
  1.1626
  1.8814
  1.2721
  1.3026
  1.7267
  1.7054
  1.6400
  1.7655
  1.7369
  1.0813
  3.1192
[torch.FloatTensor of size 128]
), ('layer2.3.conv3.weight', 
( 0 , 0 ,.,.) = 
 -1.8562e-03

( 0 , 1 ,.,.) = 
 -8.4088e-03

( 0 , 2 ,.,.) = 
 -2.1608e-02
    ... 

( 0 ,125,.,.) = 
 -1.7563e-02

( 0 ,126,.,.) = 
 -4.3702e-03

( 0 ,127,.,.) = 
  1.2788e-04
      ⋮  

( 1 , 0 ,.,.) = 
 -4.0999e-03

( 1 , 1 ,.,.) = 
  2.9778e-02

( 1 , 2 ,.,.) = 
  3.3029e-02
    ... 

( 1 ,125,.,.) = 
 -2.0626e-02

( 1 ,126,.,.) = 
  4.4738e-02

( 1 ,127,.,.) = 
 -3.7762e-02
      ⋮  

( 2 , 0 ,.,.) = 
 -5.5816e-03

( 2 , 1 ,.,.) = 
  6.2830e-03

( 2 , 2 ,.,.) = 
  4.7652e-03
    ... 

( 2 ,125,.,.) = 
 -3.3680e-03

( 2 ,126,.,.) = 
 -1.7510e-02

( 2 ,127,.,.) = 
  6.6703e-03
...     
      ⋮  

(509, 0 ,.,.) = 
 -1.1869e-02

(509, 1 ,.,.) = 
  5.6422e-03

(509, 2 ,.,.) = 
  4.8070e-03
    ... 

(509,125,.,.) = 
  2.1484e-03

(509,126,.,.) = 
 -1.5715e-02

(509,127,.,.) = 
  2.6373e-03
      ⋮  

(510, 0 ,.,.) = 
  1.2345e-02

(510, 1 ,.,.) = 
  6.1785e-03

(510, 2 ,.,.) = 
 -4.3113e-02
    ... 

(510,125,.,.) = 
  1.0104e-02

(510,126,.,.) = 
  2.9068e-02

(510,127,.,.) = 
 -8.6110e-04
      ⋮  

(511, 0 ,.,.) = 
 -1.4418e-03

(511, 1 ,.,.) = 
 -1.1389e-02

(511, 2 ,.,.) = 
 -1.4606e-03
    ... 

(511,125,.,.) = 
 -2.4021e-02

(511,126,.,.) = 
  1.2580e-02

(511,127,.,.) = 
  2.8908e-02
[torch.FloatTensor of size 512x128x1x1]
), ('layer2.3.bn3.weight', 
 0.0119
 0.1173
 0.0142
 0.0979
 0.0618
 0.0040
 0.0135
 0.0443
 0.1939
 0.1483
 0.1087
 0.0468
 0.0522
 0.0127
 0.0733
 0.2005
 0.0481
 0.1391
 0.1191
 0.0856
 0.1922
 0.0388
 0.0505
 0.0311
 0.1017
 0.0955
 0.1766
 0.1432
 0.0249
 0.1612
 0.0834
 0.0021
 0.0464
 0.0812
 0.0508
 0.0251
 0.0131
 0.0965
 0.0453
 0.0667
 0.1400
 0.1486
 0.1518
-0.0275
 0.0261
 0.2047
 0.0321
 0.1434
 0.1926
 0.1111
 0.0883
 0.2424
 0.1063
 0.0634
 0.0457
 0.1309
 0.1432
 0.2118
-0.0009
 0.0825
 0.0146
 0.1900
 0.1709
 0.0311
 0.2123
 0.1097
-0.0056
 0.1069
 0.2154
 0.1471
 0.0809
 0.1879
 0.1282
 0.1203
 0.0892
 0.0428
 0.0847
 0.0816
 0.1266
 0.1978
 0.0343
 0.1931
 0.0117
 0.0968
 0.1422
 0.0946
 0.1074
-0.0218
 0.0203
 0.0534
 0.0503
 0.1792
 0.1640
 0.0504
 0.0218
 0.1769
 0.0770
 0.0040
 0.0775
-0.0137
 0.0938
 0.1874
 0.0025
 0.0915
 0.0336
 0.0001
 0.1412
 0.1888
 0.0606
 0.0767
 0.0398
 0.1554
 0.0797
 0.1503
-0.0069
 0.0032
 0.1461
 0.0432
 0.0846
 0.1122
 0.0657
 0.1538
 0.0561
 0.1017
 0.0766
 0.0302
 0.0425
 0.0103
 0.1762
 0.1554
 0.1355
 0.1316
 0.1422
-0.0002
-0.0108
 0.0273
 0.1339
 0.1425
 0.0030
 0.1100
 0.1226
 0.0744
 0.1430
 0.0569
 0.0047
 0.0774
 0.0273
 0.0945
 0.1138
 0.0591
 0.0194
 0.0361
 0.1027
 0.0124
 0.0402
 0.0051
-0.0505
 0.0306
 0.0633
 0.1056
 0.1538
 0.0071
 0.0481
-0.0294
 0.0695
 0.0070
 0.0145
 0.1263
 0.1907
 0.1059
 0.0068
 0.0592
 0.0084
-0.0011
 0.0722
 0.1757
 0.0238
 0.1174
 0.0619
 0.1449
 0.2072
 0.1723
 0.0430
 0.0194
 0.0748
 0.0358
 0.0034
 0.2697
-0.0149
 0.0609
 0.0694
 0.1492
 0.0606
 0.1092
 0.0030
 0.0927
 0.0329
 0.2003
 0.1199
 0.0014
 0.0682
 0.1515
 0.0466
 0.0442
 0.0235
 0.2072
 0.1263
 0.0426
 0.0026
 0.0922
 0.0466
 0.0758
 0.0258
 0.0309
 0.1090
 0.1266
 0.0703
-0.0065
 0.0126
 0.0970
 0.0989
 0.1170
-0.0206
 0.2532
 0.0642
 0.1580
 0.1944
 0.0082
 0.1174
 0.0653
 0.1979
 0.0979
 0.1091
 0.0922
-0.0027
 0.1173
 0.0394
 0.0329
 0.1371
 0.0505
 0.0400
 0.0364
 0.1217
 0.0053
 0.2201
 0.1495
 0.0383
 0.0428
 0.1527
 0.0173
 0.1342
 0.0027
 0.1904
 0.1486
 0.2225
 0.1141
 0.1266
 0.1079
 0.1167
 0.0085
 0.2475
 0.0887
 0.1340
 0.0094
 0.1558
-0.0047
 0.0711
 0.0353
-0.0062
 0.0360
-0.0011
 0.1202
 0.0296
 0.0763
 0.1654
 0.0068
 0.1598
 0.1536
 0.1467
 0.0598
 0.0808
 0.0644
 0.0946
 0.0838
 0.0391
 0.0069
 0.1463
 0.1057
 0.0476
 0.2007
-0.0006
 0.0135
 0.1956
 0.1521
 0.2308
 0.0955
 0.1244
 0.0316
 0.0570
 0.2520
 0.1136
 0.0564
 0.2398
 0.1165
 0.1543
 0.1807
 0.1293
 0.1368
 0.1982
 0.0245
 0.1866
-0.0035
-0.0019
 0.0323
 0.0289
 0.0584
 0.1549
 0.0135
-0.0151
 0.0173
 0.1220
 0.1056
 0.0508
 0.0861
-0.0005
 0.0203
 0.1603
 0.1600
 0.0504
 0.0569
 0.0184
 0.0564
-0.0047
 0.0385
 0.1841
 0.0637
 0.2195
 0.0291
 0.1043
 0.0846
 0.0369
 0.0862
 0.0387
 0.1482
 0.0241
 0.0735
 0.0176
 0.0035
 0.1779
 0.0066
 0.0828
 0.0130
 0.0659
 0.0690
 0.0301
 0.0980
 0.1494
-0.0116
 0.0948
 0.2130
 0.0883
 0.0639
 0.1180
-0.0240
 0.0549
 0.0446
 0.0556
 0.1846
 0.0257
 0.0589
 0.0477
-0.0048
 0.0848
 0.0631
 0.0148
 0.1873
 0.1089
 0.1353
 0.1487
 0.0554
 0.1436
 0.0604
 0.0054
 0.1248
 0.0826
 0.1573
 0.0137
 0.0192
 0.0069
 0.1349
 0.1167
 0.1494
 0.0097
 0.2601
 0.0933
-0.0033
 0.0303
 0.1774
 0.1147
 0.0954
 0.0241
 0.0023
 0.1656
 0.0656
 0.0864
-0.0004
-0.0099
 0.1035
 0.0851
 0.0574
 0.1939
 0.1244
 0.0372
 0.0412
 0.0743
 0.2109
 0.0959
 0.1913
 0.0023
 0.0807
 0.0663
 0.0390
 0.2655
 0.1125
 0.1016
 0.1639
 0.0137
 0.0239
 0.0977
 0.1802
 0.0341
 0.0643
 0.1027
 0.0243
 0.0888
 0.2672
 0.1857
 0.1321
 0.0720
 0.1274
 0.1074
-0.0092
 0.0514
-0.0142
 0.0633
 0.1901
 0.0298
 0.1709
 0.0434
 0.2026
 0.1118
 0.1421
 0.0628
 0.0857
 0.0562
 0.0543
 0.1512
 0.1599
 0.0042
 0.0577
 0.3164
 0.1101
 0.0322
 0.0938
 0.0195
 0.0884
 0.2268
 0.0440
-0.0039
 0.1977
 0.1914
 0.0095
 0.0085
 0.0232
 0.0583
 0.1516
 0.0114
 0.1619
 0.1051
 0.1312
 0.2300
 0.1101
-0.0027
 0.1132
 0.1442
 0.1134
 0.0182
 0.0735
 0.0554
 0.0136
 0.1872
 0.0623
 0.0744
 0.0651
 0.0083
 0.0620
 0.1309
 0.0681
 0.0032
 0.0466
 0.0518
 0.0049
 0.1806
 0.0877
 0.1353
 0.0476
 0.0271
 0.0497
 0.1111
-0.0058
 0.0990
 0.0568
[torch.FloatTensor of size 512]
), ('layer2.3.bn3.bias', 
-1.0753e-02
 1.2296e-01
 1.8016e-02
-1.4745e-01
-1.0184e-01
 1.4736e-02
-2.6944e-03
-9.4832e-02
-7.0655e-02
-1.7460e-01
-8.9392e-02
-6.2397e-02
 5.8483e-02
 8.1586e-03
-6.0236e-02
-8.8126e-02
 4.0486e-02
-9.0144e-02
-6.8058e-02
-1.3243e-01
 4.6727e-03
-2.2941e-02
-9.3018e-02
 4.8685e-02
-1.6099e-01
-1.1730e-01
-1.8592e-01
-5.0664e-02
 5.8731e-02
-4.4601e-02
-1.0900e-01
 1.1684e-02
-6.4666e-02
-1.0929e-01
-1.0403e-01
 9.2314e-03
 1.1211e-02
-1.1996e-01
 5.7720e-02
-3.6448e-02
-1.3682e-01
-9.1506e-02
-9.8696e-02
-1.3878e-02
-5.9053e-04
-1.3165e-01
-2.1491e-02
-1.0817e-01
-1.4217e-01
-6.5535e-02
-1.3517e-01
 5.4794e-02
-1.3225e-01
-7.9678e-02
-8.1386e-02
 3.1635e-03
-1.1036e-02
 7.3200e-03
 1.5462e-02
-9.3652e-02
-2.3393e-02
 2.1940e-02
-9.7693e-02
-6.2564e-02
-7.5043e-02
-1.2641e-01
-7.2580e-03
-9.3723e-02
 9.8195e-02
-1.1159e-01
-1.2596e-01
-2.0165e-01
-1.3123e-01
-9.4722e-02
-6.6261e-02
-7.9655e-02
-8.2974e-02
-1.2387e-01
 5.7409e-02
 8.0741e-02
-9.9055e-03
-8.8603e-02
-2.6275e-02
-6.2083e-02
-9.4109e-02
-1.4126e-01
-1.1017e-01
-4.5961e-02
-3.6952e-02
 5.6733e-02
 9.2809e-02
-3.3842e-02
 4.9556e-02
 2.5482e-02
 2.0694e-02
 1.0866e-02
-1.0336e-01
-2.4721e-02
-9.7841e-02
 1.8015e-02
-1.1074e-01
 1.8100e-02
 3.4115e-03
-9.0010e-03
-6.0877e-02
 7.1027e-03
-1.7420e-01
-8.6625e-02
-1.4987e-02
-2.3115e-02
-7.2371e-02
-1.7420e-01
-1.0314e-01
 5.7210e-02
 8.3013e-03
 7.0849e-06
-7.1349e-02
-6.6974e-02
-9.2546e-02
-2.9622e-02
-4.0594e-02
 6.1131e-02
-1.0239e-01
-1.8121e-01
-1.9086e-02
-8.2694e-02
 3.3261e-02
 2.5272e-02
-5.8061e-02
 3.7129e-02
-9.3135e-02
-1.4024e-01
-1.6741e-01
 4.0829e-03
 1.3346e-02
 1.1912e-02
-1.5067e-01
 8.2857e-02
-7.4040e-03
-7.5757e-03
-9.1867e-02
-1.4280e-01
-1.5459e-01
-7.5941e-02
 8.5401e-03
-1.3061e-01
-5.0029e-02
-1.1627e-01
-1.1917e-01
-1.1245e-01
-1.3704e-02
-6.2715e-02
-1.5910e-01
-5.0106e-02
-1.1517e-01
 2.7630e-04
-1.0772e-01
 4.6743e-02
-8.4230e-02
-4.3368e-02
-1.6859e-01
 1.0209e-02
 3.9963e-03
-3.4581e-02
-3.6768e-02
 4.9823e-03
 1.1886e-02
-1.0845e-01
 1.1292e-03
-8.8278e-02
 1.3322e-03
-2.2974e-02
-8.4240e-03
 1.6296e-02
-1.0848e-02
-2.6196e-02
-2.5264e-02
-1.4026e-01
-6.9171e-02
-4.4666e-02
-1.2755e-01
-1.1186e-01
-5.6086e-02
-2.9973e-03
-1.3929e-01
-7.8196e-02
 1.1020e-03
-1.5131e-01
 1.7389e-02
 4.5829e-02
-1.2167e-01
-9.3535e-02
-1.3126e-01
-2.1264e-01
-1.7958e-03
-3.6409e-02
 4.1059e-02
-3.7527e-02
-5.7838e-02
 5.7738e-03
-7.0301e-02
-6.1492e-02
-6.4794e-02
-4.4071e-02
-2.8237e-02
-4.0050e-02
-2.9837e-02
-2.6018e-02
-2.0245e-03
-8.6654e-02
 9.1139e-02
-8.3759e-02
 3.9878e-02
-4.8310e-02
-3.0793e-02
-6.5844e-02
 6.1619e-02
-9.2482e-04
-8.1795e-03
-9.5234e-02
-1.3183e-01
 7.2410e-02
-3.2863e-02
-3.7035e-02
-1.9199e-01
-1.5521e-01
-9.5280e-02
-2.5488e-02
-1.3084e-01
-8.5780e-02
-1.7694e-01
-1.2969e-01
-1.5030e-01
-1.0276e-01
 6.7782e-03
-1.7357e-01
 2.8345e-02
-1.3535e-02
-1.6473e-01
-8.6814e-02
 3.3441e-02
-7.3201e-02
-1.1077e-01
-2.1831e-03
-1.5356e-01
-3.6794e-02
-5.3170e-02
-7.1816e-02
 2.9639e-02
-6.8593e-02
-1.8413e-01
 2.0662e-02
-2.2187e-02
-1.3400e-01
-1.9961e-01
-1.1171e-01
-2.0527e-02
-1.0173e-01
-1.8351e-01
 9.0249e-03
-1.3788e-01
-7.3549e-02
-6.4792e-02
 8.1229e-03
-1.4727e-01
 6.2829e-03
-8.3501e-02
-5.0290e-02
 2.6632e-02
-5.1704e-02
 2.0771e-04
-7.9790e-02
-5.8899e-02
-1.2353e-01
-1.2764e-01
 9.6215e-03
-2.3874e-02
-6.9386e-02
-1.0972e-01
-7.7112e-02
-1.1081e-01
-8.2271e-02
-2.0116e-02
-6.6208e-02
-3.8037e-02
 3.7714e-03
-9.3259e-02
-9.8476e-02
 2.9768e-02
-3.2294e-02
 1.9760e-03
-2.1011e-02
-1.0929e-01
-1.1785e-01
-2.3031e-02
-1.1310e-01
-1.8987e-01
 7.3373e-02
-8.3597e-02
-7.7471e-02
 4.5497e-02
-1.5749e-02
-1.2919e-01
-1.2601e-01
-3.4712e-02
-2.2256e-02
-2.4382e-02
-7.9151e-02
-5.8132e-02
-6.3466e-02
-6.6077e-02
 8.8458e-03
-2.0902e-02
-3.4018e-02
-5.7782e-02
 9.4693e-02
-1.5027e-01
-1.0736e-01
-9.1824e-03
-1.3116e-02
-1.1104e-01
-1.7433e-01
 4.9677e-02
-6.9699e-02
 6.9792e-03
-2.8829e-02
-1.6447e-01
-1.7309e-01
-1.4145e-01
 6.4827e-02
-1.6273e-02
-7.0341e-02
-1.5254e-02
-6.1869e-02
 3.5556e-02
-7.5051e-02
-1.7145e-01
-3.5523e-02
-3.7991e-02
-1.1560e-01
-6.3393e-02
-1.4697e-01
-9.1959e-02
 2.9727e-02
-1.8045e-02
-1.5468e-01
-1.1406e-02
-1.8444e-03
-7.1585e-02
 3.4634e-03
-1.1167e-01
 2.4004e-02
-1.0770e-01
-9.2750e-02
-7.9636e-02
-1.3726e-01
-1.2879e-01
 9.6147e-03
-2.8220e-02
-1.3269e-01
-1.3389e-01
-5.7664e-02
-1.2113e-01
-4.0037e-02
-6.5323e-02
-7.5729e-02
-6.8245e-03
-1.4686e-01
-5.6973e-02
-6.1606e-02
 7.2811e-02
 1.0838e-02
-1.3790e-01
-7.6565e-02
-1.8736e-02
-1.1127e-01
-5.4532e-02
-1.5063e-01
-1.0753e-01
-7.4601e-02
-6.4743e-02
-6.4706e-02
-1.0954e-02
-5.8167e-02
-4.2078e-02
-1.3516e-01
-9.4269e-03
-2.3380e-02
 5.0527e-04
-1.9994e-02
-2.4426e-02
-8.6712e-02
 1.0163e-02
-6.4395e-02
 5.7263e-02
 4.7844e-03
-5.1262e-02
-7.5730e-03
-1.2574e-01
-7.4685e-02
 4.9026e-02
 2.5039e-03
-8.6998e-02
-8.5562e-02
-1.8630e-01
 1.1889e-02
-5.6308e-03
 6.0317e-02
-7.1492e-02
 2.0359e-02
-1.1749e-01
-1.5082e-01
-6.2635e-02
 4.7078e-02
 7.6936e-02
-1.2096e-01
 1.4469e-02
-1.1042e-01
-2.3019e-02
 6.8692e-02
 3.3290e-02
 4.4978e-02
-4.7317e-02
 1.6809e-03
-1.8336e-01
-4.0488e-02
 1.5075e-02
 3.0089e-02
-1.3889e-01
-3.2943e-02
 2.4125e-02
-4.8884e-02
-8.4686e-02
 4.8062e-02
 2.9681e-02
-1.7556e-01
 2.5638e-02
-3.4004e-02
-7.7240e-02
-1.7466e-01
 2.6967e-02
 1.7453e-02
 1.4212e-02
 5.6448e-04
-4.1859e-02
-6.3392e-02
-3.0698e-02
-1.8603e-01
-9.7148e-02
-1.1214e-01
-1.8083e-01
-1.1350e-01
-1.2246e-01
-8.4534e-02
-7.4321e-02
-1.8591e-01
-1.8206e-02
-8.8515e-03
-1.3595e-03
-1.1627e-01
-1.8251e-01
-5.8070e-02
-3.9273e-03
-1.1738e-01
-4.6192e-02
-1.2331e-01
 3.5421e-02
 7.9290e-02
-9.7586e-03
-1.4005e-01
 9.2140e-02
-1.0471e-02
 7.3859e-03
-4.6659e-02
-8.1156e-02
-1.3107e-01
 1.7838e-02
-6.3042e-02
-1.4772e-01
-3.4397e-02
 1.1343e-01
-1.3986e-01
 3.8241e-03
-5.0460e-02
-3.3018e-03
-1.4448e-01
 2.4820e-02
-1.2669e-01
-7.1906e-02
 8.8935e-03
-1.0229e-02
 5.7365e-02
-1.5502e-01
-6.4526e-02
 1.2748e-02
-7.4664e-02
-1.0345e-01
-3.0873e-02
 7.8435e-04
-2.7945e-02
-9.3781e-02
 9.8994e-03
-8.7845e-02
-8.9566e-02
-1.3074e-01
-5.2352e-02
-2.7542e-02
 5.4210e-02
-1.1718e-02
 5.8357e-03
-1.1990e-01
-7.6472e-02
[torch.FloatTensor of size 512]
), ('layer2.3.bn3.running_mean', 
-0.0091
 0.0059
 0.0024
-0.0307
-0.0124
 0.0014
-0.0022
-0.0107
-0.0244
-0.0216
-0.0485
-0.0029
 0.0336
 0.0054
 0.0117
-0.0023
 0.0347
-0.0158
-0.0350
-0.0052
-0.0089
 0.0015
 0.0188
 0.0105
 0.0257
 0.0084
 0.0048
-0.0685
-0.0081
 0.0001
 0.0459
 0.0044
-0.0233
 0.0108
 0.0248
-0.0103
-0.0059
 0.0641
-0.0482
-0.0009
-0.0127
-0.0146
-0.0216
-0.0057
-0.0160
-0.0060
-0.0044
 0.0021
-0.0408
 0.0050
-0.0266
-0.0059
-0.0258
 0.0360
 0.0209
-0.0276
 0.0342
-0.0375
-0.0068
-0.0177
-0.0203
-0.0644
 0.0167
-0.0016
-0.1023
 0.0185
-0.0067
 0.0187
-0.0029
-0.0109
 0.0087
-0.0151
-0.0080
 0.0123
-0.0083
 0.0251
-0.0228
 0.0116
 0.0179
-0.0245
-0.0060
 0.0227
-0.0106
-0.0024
 0.0105
 0.0165
 0.0253
 0.0024
-0.0023
-0.0306
 0.0158
-0.0303
-0.0141
-0.0432
-0.0104
 0.0063
 0.0274
 0.0068
 0.0081
 0.0059
 0.0517
-0.0137
 0.0003
 0.0016
-0.0179
 0.0047
-0.0036
-0.0299
-0.0084
-0.0025
 0.0438
-0.0220
-0.0036
-0.0682
 0.0077
 0.0034
-0.0187
-0.0036
-0.0032
-0.0013
 0.0115
-0.0018
 0.0109
-0.0026
-0.0123
 0.0045
-0.0177
 0.0089
-0.0262
-0.0186
-0.0016
 0.0055
-0.0004
-0.0020
 0.0009
-0.0053
-0.0022
 0.0511
-0.0027
 0.0146
 0.0316
-0.0341
 0.0055
-0.0092
-0.0009
 0.0124
 0.0101
-0.0191
 0.0369
-0.0078
 0.0022
-0.0004
-0.0027
-0.0007
-0.0099
 0.0036
-0.0054
-0.0283
 0.0499
 0.0001
 0.0065
 0.0016
-0.0200
-0.0044
 0.0069
-0.0064
-0.0119
-0.0103
-0.0498
-0.0233
-0.0022
-0.0018
 0.0001
 0.0051
-0.0114
-0.0352
 0.0073
 0.0048
 0.0135
 0.0787
 0.0460
 0.0194
 0.0179
 0.0085
 0.0112
 0.0497
-0.0038
 0.0002
 0.0020
 0.0038
 0.0063
 0.0299
 0.0154
 0.0860
-0.0060
 0.0084
 0.0096
-0.0012
-0.0270
-0.0034
-0.0052
 0.0299
-0.0028
-0.0202
-0.0045
 0.0008
 0.0142
-0.0118
-0.0032
-0.0290
-0.0030
 0.0654
 0.0055
-0.0248
-0.0155
-0.0066
 0.0092
 0.0076
-0.0018
 0.0173
 0.0193
-0.0168
 0.0093
-0.0421
 0.0099
-0.0270
-0.0230
-0.0030
-0.0215
 0.0290
 0.0378
 0.0368
-0.0079
 0.0072
-0.0071
-0.0026
-0.0002
-0.0132
-0.0263
 0.0185
-0.0041
-0.0186
-0.0103
-0.0015
 0.0299
-0.0010
-0.0148
 0.0289
-0.0715
-0.0119
 0.0200
-0.0049
-0.0363
-0.0191
-0.0005
-0.0065
-0.0256
-0.0023
 0.0157
 0.0015
-0.0154
-0.0040
-0.0272
 0.0078
-0.0072
 0.0033
-0.0220
 0.0045
 0.0023
 0.0052
 0.0033
-0.0166
 0.0065
 0.0026
-0.0033
 0.0020
 0.0029
-0.0249
-0.0134
-0.0075
 0.0214
-0.0197
-0.0462
 0.0031
 0.0118
-0.0055
-0.0078
 0.0064
-0.0292
-0.0690
-0.0045
-0.0134
-0.0060
-0.0323
-0.1420
-0.0199
-0.0275
-0.0047
-0.0154
-0.0808
 0.0536
-0.0415
-0.0762
 0.0105
-0.0640
 0.0252
 0.0529
 0.0846
-0.1143
-0.0011
-0.0231
-0.0054
 0.0097
 0.0012
-0.0112
-0.0043
 0.0059
-0.0108
 0.0035
 0.0074
-0.0332
-0.0048
-0.0069
-0.0212
-0.0122
-0.0094
-0.0024
-0.0063
-0.0111
 0.0034
 0.0118
-0.0097
 0.0088
-0.0176
-0.0689
-0.0086
-0.0826
-0.0050
-0.0109
-0.0098
 0.0068
 0.0092
-0.0393
 0.0021
-0.0091
 0.0204
-0.0068
 0.0042
-0.0853
-0.0086
 0.0104
-0.0102
 0.0178
 0.0001
-0.0097
 0.0299
 0.0181
 0.0010
 0.0371
 0.0051
-0.0266
-0.0101
-0.0091
 0.0015
-0.0287
 0.0132
-0.0124
-0.0391
 0.0464
 0.0015
-0.0086
-0.0036
-0.0072
 0.0242
 0.0076
-0.0605
 0.0125
 0.0116
-0.0049
 0.0027
 0.0265
-0.0013
 0.0063
 0.0292
-0.0109
-0.0222
-0.0005
-0.0073
 0.0133
 0.0097
-0.0141
 0.0169
-0.0082
-0.0591
 0.0068
-0.0009
 0.0034
 0.0391
 0.0318
-0.0125
 0.0003
-0.0018
-0.0212
 0.0073
 0.0394
-0.0018
 0.0149
-0.0372
 0.0155
-0.0149
-0.0792
 0.0115
 0.0174
 0.0068
 0.0040
 0.0128
-0.0244
-0.0072
-0.0051
-0.0216
 0.0231
 0.0015
-0.1104
-0.0047
 0.0512
 0.0221
 0.0026
 0.0007
 0.0165
 0.0054
-0.0354
 0.0287
-0.0006
 0.0001
-0.0296
-0.0948
-0.0104
-0.0464
 0.0467
-0.0189
-0.0129
 0.0051
 0.0208
-0.0009
 0.0338
-0.0203
-0.0023
-0.0130
-0.0129
-0.0105
 0.0467
-0.0047
-0.0179
-0.0026
-0.0113
 0.0209
-0.0006
-0.0629
-0.0006
 0.0742
-0.0984
-0.0418
 0.0048
-0.0036
 0.0062
-0.0263
-0.0162
-0.0004
 0.0033
-0.0308
-0.0179
-0.0023
-0.0019
 0.0048
-0.0225
-0.0197
 0.0164
-0.0067
-0.0198
-0.0224
-0.0850
 0.0046
 0.0051
 0.0057
-0.0492
 0.0059
 0.0025
 0.0094
-0.0147
-0.0044
-0.0277
-0.0132
 0.0021
 0.0016
 0.0010
-0.0293
 0.0241
 0.0139
 0.0006
-0.0212
-0.0014
-0.0014
-0.0182
-0.0066
 0.0494
-0.0258
-0.0023
 0.0021
-0.0064
 0.0028
 0.0501
-0.0082
[torch.FloatTensor of size 512]
), ('layer2.3.bn3.running_var', 
1.00000e-03 *
  0.1313
  1.7065
  0.2585
  0.6283
  0.4924
  0.1418
  0.1797
  0.4537
  1.6856
  1.4568
  0.6017
  0.3071
  0.9005
  0.2232
  0.4562
  2.2326
  1.3579
  1.3680
  1.0484
  0.8837
  1.9032
  0.4415
  0.6427
  0.6024
  1.6501
  0.7192
  1.4498
  1.7738
  0.3735
  2.3479
  0.7463
  0.2261
  0.4325
  0.4912
  0.4254
  0.4205
  0.6460
  1.6628
  0.8267
  1.2944
  1.5123
  1.6315
  1.5859
  0.2287
  0.3775
  2.1408
  0.2253
  1.2234
  2.2559
  0.9681
  0.5056
  4.7754
  1.1626
  0.7583
  0.4956
  1.8410
  1.4308
  2.9008
  0.2375
  0.7053
  0.1264
  1.5653
  1.7664
  0.1466
  1.7282
  0.8465
  0.0565
  1.4412
  3.0879
  1.3167
  0.8854
  1.8617
  1.0656
  1.1968
  1.4940
  0.5920
  0.8882
  1.2869
  1.8873
  2.6192
  0.4058
  1.6167
  0.0874
  1.0327
  1.2150
  1.1681
  1.5272
  0.2122
  0.2415
  0.8166
  0.8492
  2.1686
  1.8095
  0.4949
  0.2971
  2.1354
  0.9299
  0.1061
  0.5895
  0.1609
  0.7588
  1.8964
  0.0883
  1.0351
  0.3129
  0.2245
  1.5181
  1.8852
  1.0804
  0.9180
  0.7966
  2.5787
  0.6076
  1.8121
  0.1175
  0.3286
  1.4583
  0.3898
  1.3628
  1.0672
  0.8386
  1.2647
  0.4076
  0.7240
  0.9057
  0.3713
  0.8874
  0.2639
  1.4843
  1.7664
  1.7127
  1.1137
  0.9042
  0.1305
  0.2781
  0.3721
  1.2938
  3.2927
  0.1666
  1.3393
  1.4086
  0.6954
  1.1530
  0.4504
  0.1411
  0.6896
  0.2268
  0.8790
  1.7933
  0.5901
  0.2607
  0.3445
  1.3372
  0.1613
  0.3638
  0.2446
  0.2696
  0.5555
  0.4373
  1.1572
  1.3029
  0.3165
  0.7219
  0.1832
  1.1348
  0.1739
  0.3442
  1.3052
  2.6410
  0.5916
  0.1520
  1.3569
  0.4063
  0.2973
  0.7197
  2.6418
  0.1709
  1.4525
  0.2653
  1.8211
  1.8590
  1.9691
  0.4903
  0.1990
  0.8082
  0.6046
  0.1712
  2.1280
  0.1915
  1.0624
  0.5949
  1.5143
  0.7100
  1.7745
  0.1592
  1.3323
  0.6139
  2.5648
  1.2801
  0.1408
  0.8044
  2.4931
  0.3529
  0.4196
  0.2573
  2.8135
  1.1205
  0.3533
  0.3464
  1.0628
  1.1299
  1.3628
  1.1770
  0.2885
  1.2552
  1.2476
  1.1055
  0.3687
  0.1256
  1.1493
  0.6278
  1.2356
  0.0861
  2.9526
  0.3278
  1.7620
  2.1957
  0.1338
  1.1846
  1.2825
  2.1730
  0.9484
  1.4411
  0.7253
  0.1725
  1.0101
  0.5127
  0.5767
  2.4904
  0.5026
  0.6727
  0.4226
  1.0408
  0.1823
  1.7064
  1.7149
  0.4345
  0.7410
  1.7766
  0.3115
  1.2981
  0.2986
  2.2698
  1.5898
  1.7949
  0.9226
  1.2885
  0.8215
  1.1777
  0.2152
  2.0242
  1.0949
  1.1785
  0.2104
  1.6328
  0.2597
  0.7001
  0.1797
  0.2299
  0.2123
  0.1355
  1.1615
  0.1185
  0.7294
  1.9756
  0.1148
  2.0964
  1.3184
  1.0613
  0.6120
  0.9487
  0.5837
  1.2208
  0.8292
  0.2017
  0.1672
  1.4779
  0.9549
  0.7455
  2.4351
  0.1258
  0.1723
  1.8161
  1.6432
  2.3635
  1.2882
  1.3627
  0.6797
  0.3267
  3.8899
  1.1674
  0.5404
  2.0301
  0.7893
  1.2788
  2.1921
  1.3441
  1.8724
  1.9863
  0.1084
  1.7294
  0.1293
  0.1426
  0.2585
  0.2432
  1.0838
  1.4685
  0.1165
  0.1988
  0.1841
  1.2247
  0.7750
  0.6269
  0.7203
  0.2036
  0.1702
  1.1897
  1.1639
  0.4512
  1.1827
  0.4212
  0.7388
  0.0909
  0.2080
  1.7373
  0.5925
  2.2774
  0.2129
  0.8784
  0.6784
  0.3439
  0.8527
  0.3905
  1.3494
  0.1888
  0.9260
  0.1559
  0.1805
  2.0491
  0.3637
  0.9395
  0.2673
  0.9069
  0.5948
  0.2745
  0.9681
  0.8391
  0.2137
  1.4047
  2.2200
  0.9049
  1.1974
  0.8978
  0.1040
  0.5484
  0.6779
  0.6228
  1.7487
  0.5887
  0.5598
  1.0881
  0.1708
  1.1287
  0.7833
  0.2241
  2.2390
  0.9673
  1.6105
  1.1364
  0.3987
  2.8520
  0.8219
  0.0906
  1.2544
  1.1122
  1.3215
  0.1209
  0.2879
  0.3109
  2.2224
  1.5687
  1.6751
  0.1608
  2.0793
  1.9357
  0.1589
  0.4211
  1.9855
  0.9971
  1.0361
  0.5423
  0.1684
  2.9251
  0.7035
  0.5510
  0.1773
  0.2218
  1.1609
  0.6774
  0.8586
  2.1789
  1.1905
  0.3305
  0.7211
  1.7963
  2.3184
  1.0590
  2.8114
  0.0979
  1.6573
  0.8476
  0.7810
  3.1048
  1.1308
  0.8032
  1.3591
  0.3325
  0.6035
  0.5812
  1.8747
  0.6221
  0.8162
  1.9484
  0.4580
  1.5258
  2.8069
  2.2595
  1.6853
  0.8691
  1.0167
  1.3265
  0.1691
  0.4488
  0.1609
  0.8094
  2.8411
  0.1639
  2.2321
  0.6006
  1.7667
  0.7543
  1.3772
  0.5017
  1.0784
  0.3302
  0.3614
  1.8122
  1.8735
  0.1383
  1.2508
  3.0919
  1.4842
  0.2428
  0.5934
  0.1011
  0.9765
  2.2446
  0.6490
  0.1033
  2.1353
  2.1833
  0.1704
  0.2557
  0.1290
  0.5281
  1.1791
  0.1803
  1.4401
  1.1704
  1.8558
  3.1624
  1.1735
  0.1313
  2.0502
  2.1341
  1.2207
  0.3575
  0.7770
  0.3601
  0.2764
  2.2550
  0.6919
  0.5195
  0.9581
  0.2244
  0.4472
  1.4192
  0.6586
  0.1244
  0.6226
  0.4275
  0.1976
  2.2869
  0.7376
  1.3290
  0.3678
  0.2487
  0.7382
  1.0656
  0.2179
  1.9173
  0.6327
[torch.FloatTensor of size 512]
), ('layer3.0.conv1.weight', 
( 0 , 0 ,.,.) = 
  1.4149e-02

( 0 , 1 ,.,.) = 
  8.5007e-03

( 0 , 2 ,.,.) = 
  1.9201e-02
    ... 

( 0 ,509,.,.) = 
 -4.0301e-04

( 0 ,510,.,.) = 
 -1.6396e-02

( 0 ,511,.,.) = 
  6.3780e-03
      ⋮  

( 1 , 0 ,.,.) = 
 -7.8159e-03

( 1 , 1 ,.,.) = 
 -1.5656e-04

( 1 , 2 ,.,.) = 
  1.0262e-02
    ... 

( 1 ,509,.,.) = 
  5.9090e-03

( 1 ,510,.,.) = 
  8.4134e-02

( 1 ,511,.,.) = 
 -2.8860e-03
      ⋮  

( 2 , 0 ,.,.) = 
  2.2822e-02

( 2 , 1 ,.,.) = 
 -4.3230e-02

( 2 , 2 ,.,.) = 
  7.8770e-03
    ... 

( 2 ,509,.,.) = 
  1.4022e-02

( 2 ,510,.,.) = 
  6.7142e-03

( 2 ,511,.,.) = 
 -3.1446e-02
...     
      ⋮  

(253, 0 ,.,.) = 
 -8.5522e-05

(253, 1 ,.,.) = 
  8.2215e-03

(253, 2 ,.,.) = 
 -2.0979e-02
    ... 

(253,509,.,.) = 
 -1.9268e-02

(253,510,.,.) = 
 -1.8546e-02

(253,511,.,.) = 
  3.3627e-02
      ⋮  

(254, 0 ,.,.) = 
 -1.2550e-02

(254, 1 ,.,.) = 
 -5.8194e-02

(254, 2 ,.,.) = 
  5.3627e-03
    ... 

(254,509,.,.) = 
  7.3295e-03

(254,510,.,.) = 
  2.6446e-02

(254,511,.,.) = 
  3.0675e-02
      ⋮  

(255, 0 ,.,.) = 
 -3.8911e-03

(255, 1 ,.,.) = 
 -2.7051e-02

(255, 2 ,.,.) = 
 -2.2847e-03
    ... 

(255,509,.,.) = 
  7.5297e-03

(255,510,.,.) = 
 -5.2904e-02

(255,511,.,.) = 
  6.0825e-03
[torch.FloatTensor of size 256x512x1x1]
), ('layer3.0.bn1.weight', 
 0.1946
 0.2601
 0.2029
 0.1747
 0.2215
 0.1995
 0.2640
 0.2500
 0.2342
 0.2072
 0.2172
 0.2479
 0.2861
 0.2639
 0.2448
 0.2768
 0.1862
 0.2633
 0.2423
 0.1850
 0.2633
 0.2455
 0.2833
 0.2693
 0.2729
 0.2791
 0.2744
 0.2201
 0.2304
 0.2119
 0.2682
 0.2393
 0.2065
 0.2103
 0.1802
 0.2136
 0.2190
 0.2243
 0.2371
 0.2139
 0.2046
 0.2209
 0.1994
 0.2091
 0.2018
 0.2023
 0.2911
 0.2256
 0.2194
 0.1831
 0.2852
 0.2344
 0.3128
 0.2464
 0.2380
 0.2307
 0.1979
 0.2267
 0.2337
 0.2341
 0.2802
 0.1647
 0.2803
 0.2407
 0.2042
 0.3024
 0.2114
 0.2267
 0.1883
 0.2827
 0.2215
 0.2315
 0.2330
 0.2287
 0.2992
 0.2437
 0.2398
 0.2680
 0.2352
 0.2196
 0.2743
 0.2032
 0.2107
 0.2373
 0.2054
 0.2627
 0.2570
 0.2679
 0.2402
 0.2196
 0.2359
 0.2219
 0.2065
 0.2145
 0.1937
 0.2652
 0.1711
 0.2656
 0.2181
 0.1799
 0.2528
 0.2552
 0.1657
 0.2553
 0.2602
 0.2552
 0.1868
 0.2030
 0.1685
 0.2602
 0.2258
 0.2057
 0.2460
 0.1817
 0.2335
 0.2478
 0.2260
 0.2288
 0.2435
 0.2234
 0.2226
 0.2391
 0.1664
 0.2759
 0.2846
 0.3010
 0.2254
 0.1892
 0.2445
 0.2194
 0.2523
 0.2271
 0.2227
 0.1839
 0.3044
 0.2194
 0.2243
 0.2795
 0.2475
 0.2451
 0.2267
 0.2362
 0.1989
 0.2099
 0.2307
 0.1967
 0.2383
 0.2767
 0.2448
 0.2679
 0.2379
 0.2293
 0.2976
 0.2862
 0.3137
 0.2089
 0.2370
 0.2605
 0.2073
 0.2613
 0.2536
 0.1897
 0.2381
 0.1994
 0.2133
 0.2054
 0.2239
 0.2939
 0.2420
 0.1751
 0.2585
 0.2714
 0.2666
 0.2222
 0.2338
 0.1448
 0.2367
 0.2191
 0.2261
 0.2479
 0.1759
 0.2142
 0.1761
 0.1973
 0.2791
 0.2643
 0.2255
 0.2505
 0.2510
 0.2321
 0.2263
 0.2527
 0.3157
 0.2557
 0.2281
 0.2543
 0.2325
 0.2176
 0.1627
 0.2374
 0.2041
 0.2547
 0.2005
 0.1941
 0.2861
 0.2709
 0.1976
 0.2113
 0.2182
 0.2614
 0.2776
 0.2540
 0.2102
 0.2363
 0.2581
 0.2749
 0.3118
 0.2595
 0.1762
 0.2610
 0.2945
 0.1842
 0.2027
 0.2073
 0.2360
 0.2402
 0.1846
 0.2015
 0.2306
 0.2679
 0.2340
 0.2846
 0.2296
 0.2181
 0.2513
 0.2514
 0.1995
 0.2275
 0.2484
 0.2218
 0.2879
 0.2341
 0.1724
 0.2399
 0.2207
 0.2462
 0.2465
 0.2051
 0.1957
 0.2108
 0.2571
 0.1862
 0.2053
 0.2410
 0.2301
 0.2457
[torch.FloatTensor of size 256]
), ('layer3.0.bn1.bias', 
-0.0716
-0.2118
-0.0776
 0.0197
-0.1609
-0.0218
-0.2534
-0.1076
-0.1576
-0.0182
-0.1506
-0.2263
-0.1674
-0.1875
-0.1416
-0.2482
 0.0012
-0.1473
-0.1854
-0.0933
-0.1392
-0.1027
-0.2084
-0.1890
-0.2250
-0.2308
-0.1630
-0.1422
-0.0097
-0.0764
-0.1343
-0.0920
-0.0534
-0.1273
 0.0199
-0.0858
-0.1193
-0.1170
-0.1612
-0.0805
-0.0423
-0.0578
-0.1412
-0.0036
-0.0892
-0.0374
-0.1476
-0.0248
-0.1621
-0.0887
-0.2156
-0.0749
-0.1690
-0.0949
-0.1377
-0.1694
-0.0707
-0.0870
-0.1093
-0.0439
-0.2401
 0.0838
-0.2995
-0.1609
-0.0226
-0.3787
-0.1112
-0.0890
-0.0177
-0.2452
-0.1449
-0.1050
-0.2006
-0.0787
-0.3330
-0.0827
-0.1361
-0.2433
-0.0031
-0.1533
-0.3331
-0.0573
-0.1027
-0.1138
-0.0437
-0.1728
-0.2564
-0.1506
-0.1109
-0.1361
-0.1181
-0.1117
-0.1008
-0.1590
 0.0091
-0.2118
-0.0568
-0.2091
-0.0704
-0.0842
-0.0906
-0.2344
 0.0700
-0.1869
-0.1733
-0.2587
-0.0661
-0.0717
 0.1135
-0.1573
-0.1957
-0.0804
-0.1211
-0.0696
-0.1198
-0.1329
-0.0089
-0.1254
-0.0360
-0.1139
-0.0515
-0.0822
 0.0103
-0.1617
-0.1541
-0.3764
-0.1627
 0.0025
-0.1052
-0.1416
-0.1432
-0.1132
-0.1150
-0.0381
-0.3602
-0.1302
-0.0952
-0.1754
-0.2268
-0.1265
-0.0841
-0.1244
-0.0576
-0.0665
-0.1428
-0.1169
-0.1089
-0.3248
-0.1756
-0.2729
-0.1996
-0.1322
-0.3657
-0.2219
-0.3054
-0.0775
-0.2190
-0.1458
-0.1152
-0.1375
-0.1436
-0.0792
-0.1508
-0.0657
-0.0540
-0.1350
-0.1441
-0.1499
-0.0971
-0.0420
-0.1492
-0.2273
-0.2078
-0.1288
-0.1164
 0.1052
-0.0969
 0.0094
-0.1471
-0.1288
 0.0119
-0.0149
 0.0104
-0.0856
-0.1134
-0.0732
-0.0971
-0.1658
-0.1940
-0.1231
-0.1383
-0.1640
-0.2866
-0.1967
-0.2092
-0.1384
-0.1586
-0.1384
 0.0311
-0.0995
-0.0646
-0.1402
-0.0542
-0.0334
-0.1148
-0.2483
-0.1025
-0.1563
-0.0372
-0.1717
-0.1893
-0.2048
-0.1561
-0.1682
-0.1151
-0.0831
-0.1567
-0.2252
-0.0313
-0.1226
-0.2396
 0.0042
-0.1189
-0.0200
-0.1777
-0.1839
-0.0052
-0.0108
-0.1689
-0.3388
-0.1544
-0.2196
-0.1544
-0.0588
-0.1284
-0.3399
-0.1008
-0.1639
-0.1698
-0.1328
-0.2455
-0.1520
-0.0480
-0.1432
-0.1242
-0.1071
-0.1742
-0.0980
-0.0320
-0.0709
-0.1540
-0.0410
-0.0948
-0.1573
-0.2218
-0.1688
[torch.FloatTensor of size 256]
), ('layer3.0.bn1.running_mean', 
-0.0419
-0.1015
-0.0942
 0.0065
-0.1646
 0.0114
-0.0692
-0.0515
-0.1672
-0.0029
-0.1664
-0.1530
-0.2476
-0.3053
-0.0656
-0.1274
 0.0505
-0.1122
-0.1161
-0.0796
-0.3422
-0.1277
-0.1871
-0.1462
-0.0622
-0.1144
 0.0628
-0.0458
-0.1727
-0.0263
-0.1188
-0.1566
-0.1156
 0.0114
-0.1218
 0.0848
-0.0503
 0.0217
-0.1136
-0.0977
-0.0042
-0.1171
-0.0324
-0.0673
-0.0669
-0.3338
-0.1159
 0.0155
-0.1296
-0.0366
-0.0981
-0.1451
-0.0967
-0.0510
-0.0999
-0.0478
-0.2744
-0.2702
-0.1340
-0.1511
-0.0616
-0.1345
-0.1600
-0.0533
-0.0400
-0.0631
-0.0958
-0.1552
-0.1844
-0.0321
-0.0231
-0.2189
-0.2585
-0.1241
 0.0960
-0.2309
-0.1039
-0.1142
-0.2458
-0.1877
 0.0018
-0.0897
-0.1022
-0.0363
-0.1720
 0.0395
-0.0665
 0.0865
 0.0986
-0.0816
-0.0573
-0.0583
 0.0959
-0.1525
-0.0494
 0.0424
-0.0843
-0.1683
-0.1015
 0.0179
-0.1704
-0.0283
-0.1404
 0.1149
-0.2309
 0.2500
-0.1309
-0.0557
 0.0954
-0.0377
-0.0299
-0.2001
-0.0070
-0.0943
-0.2038
 0.0912
-0.1011
-0.1385
-0.2821
 0.0809
-0.1405
-0.0047
-0.0846
-0.0362
-0.0493
-0.2308
-0.0141
-0.0233
-0.0544
 0.0139
-0.0115
-0.0216
-0.0604
-0.1310
-0.0644
 0.0286
-0.0771
-0.1293
-0.0935
-0.1747
-0.0251
 0.0636
-0.2550
-0.0261
 0.1714
-0.0357
-0.2135
 0.2775
-0.1306
 0.0960
-0.0860
-0.1041
-0.1990
-0.0286
-0.2320
-0.1557
-0.0532
-0.0807
-0.0389
-0.1499
-0.1665
-0.1177
-0.1134
-0.0989
-0.0844
-0.0468
-0.1094
-0.1133
-0.1156
 0.0501
-0.0426
-0.0180
-0.1002
-0.0493
-0.3908
-0.0380
-0.0074
-0.1763
-0.0639
 0.0042
-0.1103
-0.3270
 0.0090
-0.0606
-0.1782
-0.1253
 0.0192
-0.0948
-0.0880
-0.2267
-0.1621
-0.0824
-0.0994
-0.1575
 0.0188
-0.1649
-0.2872
 0.0315
 0.0208
-0.0537
 0.0060
-0.1550
-0.0745
-0.2644
-0.0399
-0.0049
-0.0354
-0.0924
 0.1252
 0.0845
-0.0492
-0.1914
 0.0497
-0.1747
-0.1546
-0.0871
-0.0394
-0.0704
 0.0142
-0.0100
-0.0507
-0.0699
-0.0595
-0.0593
-0.0100
-0.0339
-0.0032
-0.2677
-0.1231
-0.1389
-0.0248
-0.0972
 0.0650
-0.0997
-0.2287
-0.1196
-0.1149
 0.0506
-0.1991
-0.1764
-0.1478
-0.1483
-0.0592
-0.0297
-0.1342
-0.1241
-0.0368
-0.1767
-0.0798
-0.0701
-0.0886
-0.1410
-0.1364
-0.0421
-0.0351
-0.1564
[torch.FloatTensor of size 256]
), ('layer3.0.bn1.running_var', 
1.00000e-02 *
  2.4537
  2.2511
  3.3157
  2.6276
  1.7009
  3.2649
  2.6011
  3.7712
  2.2274
  3.5459
  2.3657
  2.2317
  3.2627
  2.8098
  2.5394
  2.4375
  3.3705
  3.4068
  2.7727
  1.7610
  3.4475
  3.8975
  2.7456
  3.5657
  2.7080
  3.0891
  3.6792
  2.1401
  4.0967
  2.0559
  4.5567
  3.9001
  2.9982
  2.0417
  4.4934
  3.4530
  2.0984
  2.1837
  2.8403
  2.7246
  4.0940
  5.1694
  1.5724
  6.1002
  2.1650
  3.0637
  3.9347
  4.3311
  2.4923
  1.9192
  2.5726
  4.2750
  2.5242
  3.2734
  2.5981
  2.1895
  3.3461
  3.1594
  2.8671
  3.9680
  2.9389
  3.2888
  2.3973
  2.6939
  3.2996
  1.7348
  2.2489
  2.6411
  2.8600
  2.8791
  3.2704
  3.4131
  2.0725
  3.8279
  2.5373
  3.8777
  2.5908
  2.0568
  5.8787
  2.3636
  1.7899
  2.2301
  1.7030
  2.4351
  2.4252
  2.9643
  2.0862
  3.3106
  3.4330
  2.1938
  3.0054
  3.3513
  2.7135
  2.5349
  4.3934
  2.3555
  1.7484
  2.9807
  3.4705
  1.8126
  3.3597
  1.6891
  5.4114
  2.8778
  2.3872
  2.0688
  2.0917
  2.6153
  5.0769
  3.6061
  2.6197
  2.6886
  3.4122
  2.2416
  2.4312
  2.7638
  7.1413
  2.4956
  5.8994
  2.6464
  4.1509
  4.5832
  2.2741
  4.2599
  4.1739
  2.1769
  2.0455
  3.5148
  3.5392
  2.1570
  4.0460
  3.6944
  1.9444
  3.6710
  2.3763
  1.9102
  2.5733
  4.6775
  2.0781
  3.5384
  4.5407
  2.4368
  3.1963
  2.7722
  2.2577
  1.8108
  3.4443
  1.9989
  2.8122
  2.3044
  2.2380
  1.7468
  3.3656
  3.1942
  3.0935
  3.0229
  1.8750
  3.7874
  2.0404
  3.1861
  2.6721
  1.7928
  2.9884
  2.6706
  3.5781
  1.8980
  1.8769
  4.4447
  3.8485
  2.7296
  3.0151
  2.6973
  2.6881
  2.3058
  3.6552
  3.2022
  2.8218
  8.4273
  2.8913
  2.8057
  2.6182
  4.2265
  2.2764
  1.8347
  4.5788
  3.0339
  2.8685
  2.2623
  2.6518
  3.0449
  2.9197
  3.7287
  3.0480
  2.3866
  2.1337
  3.3972
  2.9146
  2.1028
  3.5993
  3.4128
  2.6372
  3.8029
  2.2534
  3.8321
  5.6148
  2.6283
  1.7941
  1.6264
  3.4116
  3.1417
  3.8205
  2.2329
  1.9642
  3.2280
  3.2194
  5.3648
  5.2329
  1.9441
  2.0061
  4.3747
  3.3651
  3.1661
  2.1486
  4.8266
  2.5584
  2.3922
  2.8156
  4.2989
  1.7341
  2.4134
  2.5151
  2.3178
  2.3815
  3.4918
  3.9955
  2.0769
  2.6035
  2.6280
  3.0058
  2.4511
  2.4248
  2.4782
  1.8502
  3.5251
  2.8310
  3.8826
  3.1276
  2.2908
  3.7717
  2.6456
  3.1754
  3.1366
  2.6818
  2.5235
  2.4001
  2.9760
[torch.FloatTensor of size 256]
), ('layer3.0.conv2.weight', 
( 0 , 0 ,.,.) = 
 -3.5676e-02 -4.4159e-02 -3.1522e-02
 -3.0261e-02 -1.0305e-02 -3.0481e-02
 -2.9075e-02 -4.0986e-02 -4.4898e-02

( 0 , 1 ,.,.) = 
 -1.5381e-02 -3.0678e-02 -5.1069e-03
 -1.4952e-02 -2.5419e-02 -1.8033e-02
 -2.8191e-02 -2.7300e-02 -2.0191e-02

( 0 , 2 ,.,.) = 
  1.5533e-02  2.1185e-03  2.2384e-02
 -1.7511e-02 -7.2976e-03 -1.2996e-02
 -5.0300e-03 -1.6129e-02 -5.1342e-03
    ... 

( 0 ,253,.,.) = 
  4.2438e-02  5.1855e-02  4.6973e-02
  5.1620e-02  3.4322e-02  5.1256e-02
  3.3828e-02  4.6830e-02  3.3604e-02

( 0 ,254,.,.) = 
 -5.2238e-03  1.5749e-02 -9.5059e-03
  6.2110e-03  9.6251e-03 -3.9727e-03
 -2.9889e-03  9.8623e-03  1.1168e-02

( 0 ,255,.,.) = 
 -1.5140e-02 -6.9561e-03 -9.3649e-03
  2.9356e-02  1.6229e-02  2.7333e-02
  2.0968e-02  3.8168e-02  1.6188e-02
      ⋮  

( 1 , 0 ,.,.) = 
 -1.1559e-02 -4.2343e-04  2.8705e-04
 -1.7904e-03  6.0025e-04 -7.1231e-03
 -2.2644e-03 -6.7109e-03  1.1530e-02

( 1 , 1 ,.,.) = 
 -7.1435e-03 -7.7434e-03 -1.4644e-02
  4.5507e-03 -1.0593e-02 -1.1986e-02
  2.3416e-03  1.0748e-03 -9.5772e-03

( 1 , 2 ,.,.) = 
 -1.1639e-02 -1.4447e-02 -8.8890e-03
 -4.6701e-03 -9.8546e-03 -1.5753e-02
 -1.3746e-02 -1.0631e-02 -1.1963e-02
    ... 

( 1 ,253,.,.) = 
 -4.2594e-03  1.5171e-02  8.4769e-03
 -1.3176e-03  1.5288e-03  4.6005e-03
  2.1575e-03 -5.1395e-04 -5.0190e-03

( 1 ,254,.,.) = 
  2.2688e-03  1.5911e-03  6.7311e-03
  1.3108e-02  2.6229e-03  2.3430e-02
  8.5807e-03  5.5091e-03  1.6803e-02

( 1 ,255,.,.) = 
 -7.7545e-03 -1.6997e-02 -1.0051e-02
 -1.5485e-02 -1.4481e-02 -1.2779e-02
 -2.8206e-02 -2.1518e-02  7.1745e-03
      ⋮  

( 2 , 0 ,.,.) = 
 -1.0828e-02 -3.7245e-03  1.0759e-02
  1.1467e-02  2.3765e-02  1.7964e-02
 -2.0970e-03  9.4246e-03  1.0805e-02

( 2 , 1 ,.,.) = 
  1.2899e-02 -3.4170e-03  5.5603e-04
 -3.5987e-03 -1.7772e-02 -1.5287e-02
  1.4051e-03 -1.5824e-02 -7.2978e-03

( 2 , 2 ,.,.) = 
  2.1011e-02  1.4220e-02  8.9457e-03
  1.2371e-02  8.2597e-04 -4.4019e-03
  2.1452e-02  1.1267e-02  2.6312e-02
    ... 

( 2 ,253,.,.) = 
 -9.2010e-03  1.2929e-02  7.1729e-04
  4.2680e-03  2.8272e-02  7.7315e-03
 -8.5769e-03  1.2218e-02  7.7129e-03

( 2 ,254,.,.) = 
 -7.0493e-03  5.1563e-03  1.8776e-03
  3.5170e-03  3.2480e-03 -4.2209e-03
  5.5588e-04  9.9910e-03 -3.6488e-03

( 2 ,255,.,.) = 
  5.9354e-03  3.2923e-03  2.2074e-02
  2.0078e-02  3.4681e-02  2.2642e-02
  1.1302e-02  2.8044e-02  1.7096e-02
...     
      ⋮  

(253, 0 ,.,.) = 
 -3.1205e-03  1.4762e-02  2.4124e-03
 -1.4880e-02 -1.2291e-02 -8.1030e-03
  2.2872e-03 -8.4939e-03  4.0697e-03

(253, 1 ,.,.) = 
  2.9471e-03  2.1305e-02  2.1662e-02
 -2.0600e-03 -4.2156e-03 -1.6219e-02
  9.7027e-03  1.0036e-02  6.3643e-03

(253, 2 ,.,.) = 
  3.5831e-04 -5.8970e-03 -6.6091e-03
 -8.3111e-03 -1.1051e-02 -1.7800e-02
 -3.8104e-03 -5.1521e-03  2.7179e-03
    ... 

(253,253,.,.) = 
 -2.5658e-03  9.6313e-03  2.5836e-03
 -2.0589e-03 -1.4066e-02 -1.0730e-02
 -2.5331e-03  1.0627e-02  1.5832e-02

(253,254,.,.) = 
  1.7151e-02  9.3345e-04 -3.8019e-03
 -1.0191e-02 -2.3634e-02 -1.2856e-02
  2.6982e-02  2.7909e-02  9.9536e-03

(253,255,.,.) = 
 -6.3166e-03 -1.2048e-02  8.8745e-04
 -1.4119e-02 -1.2392e-03  6.1497e-03
  1.9563e-03  1.2811e-03 -1.0937e-02
      ⋮  

(254, 0 ,.,.) = 
  1.1956e-02  6.4232e-04 -4.1247e-03
  7.4465e-03  6.4470e-03  3.0302e-03
  2.5171e-02  8.7522e-03  1.7879e-03

(254, 1 ,.,.) = 
 -1.0236e-02 -2.2165e-02 -1.3870e-02
 -1.4780e-02 -1.2104e-02 -3.0022e-02
 -1.9075e-03 -2.5575e-02 -2.5349e-02

(254, 2 ,.,.) = 
  2.1705e-02  1.8870e-02  2.3259e-02
  1.3284e-02  1.8016e-02  2.2417e-02
  9.9931e-03 -8.8218e-03 -1.4500e-02
    ... 

(254,253,.,.) = 
  9.4819e-04  2.6234e-03  1.5482e-03
  4.4344e-03  1.1787e-02  1.2554e-03
  5.2904e-03  7.9525e-03  1.6375e-02

(254,254,.,.) = 
 -8.5535e-03 -1.7023e-02 -6.0318e-03
  9.4575e-03  1.5215e-02  4.4233e-03
  1.4191e-03  1.4246e-02  2.1175e-05

(254,255,.,.) = 
 -1.2326e-02 -2.4401e-02 -1.4684e-02
  1.1002e-03  7.0458e-03 -1.0717e-02
  1.8407e-02  1.3780e-02  1.3808e-02
      ⋮  

(255, 0 ,.,.) = 
  9.6928e-03  7.8074e-04 -2.2226e-03
  9.0362e-03  1.0869e-02 -1.5511e-02
  1.7422e-02  6.9707e-03 -4.4602e-03

(255, 1 ,.,.) = 
 -2.1246e-03  1.5536e-03 -4.5620e-03
  2.1761e-03  4.7383e-03  8.7563e-03
 -4.0447e-03  1.5037e-03 -2.7206e-03

(255, 2 ,.,.) = 
  2.7551e-03  1.3806e-03 -1.8366e-02
 -3.2277e-03 -4.5974e-03  3.9618e-03
 -1.7246e-02 -6.4586e-03  5.6523e-03
    ... 

(255,253,.,.) = 
  8.8406e-04  7.9389e-03  8.1300e-03
 -3.5277e-03  1.2971e-02  9.3556e-03
 -1.1059e-02  4.4324e-03  6.1244e-03

(255,254,.,.) = 
  5.7130e-03  1.1547e-02  8.9341e-03
  4.7261e-03  1.5896e-02  2.0979e-02
 -1.7759e-03  3.4242e-03  1.6707e-02

(255,255,.,.) = 
 -8.9361e-03  1.7687e-02  2.1593e-02
 -3.2897e-03 -1.4576e-03  2.7925e-02
 -2.8845e-02  1.7679e-03  3.3419e-02
[torch.FloatTensor of size 256x256x3x3]
), ('layer3.0.bn2.weight', 
 0.1798
 0.2306
 0.1811
 0.1527
 0.1615
 0.1532
 0.1687
 0.1446
 0.1776
 0.1907
 0.1606
 0.2337
 0.1601
 0.1798
 0.1751
 0.1669
 0.1665
 0.1605
 0.1671
 0.1809
 0.1530
 0.1717
 0.1897
 0.1645
 0.2235
 0.1936
 0.2246
 0.2008
 0.1645
 0.1610
 0.1420
 0.1509
 0.1806
 0.1448
 0.1270
 0.2224
 0.1789
 0.2264
 0.1821
 0.1755
 0.1461
 0.2929
 0.2234
 0.1878
 0.1610
 0.1763
 0.1294
 0.1606
 0.1466
 0.2336
 0.1513
 0.2153
 0.2029
 0.1582
 0.1753
 0.1860
 0.2356
 0.1781
 0.1715
 0.1715
 0.1853
 0.1973
 0.1530
 0.1336
 0.2012
 0.1465
 0.1354
 0.1315
 0.1840
 0.2009
 0.1503
 0.2067
 0.1372
 0.1486
 0.1679
 0.2088
 0.1826
 0.1920
 0.1597
 0.1963
 0.1671
 0.2259
 0.1842
 0.1435
 0.1951
 0.3224
 0.2211
 0.1801
 0.1352
 0.1874
 0.2391
 0.1712
 0.1542
 0.1833
 0.1848
 0.1757
 0.1965
 0.2287
 0.2049
 0.1782
 0.2380
 0.1750
 0.2017
 0.1478
 0.1726
 0.2122
 0.1788
 0.1515
 0.1937
 0.1991
 0.2175
 0.1795
 0.2181
 0.1910
 0.1728
 0.1956
 0.1803
 0.1994
 0.1631
 0.1628
 0.1756
 0.1994
 0.1747
 0.1733
 0.1676
 0.1529
 0.2056
 0.1913
 0.1591
 0.1386
 0.2002
 0.1996
 0.2460
 0.1689
 0.2371
 0.2237
 0.2015
 0.1659
 0.1808
 0.1637
 0.1391
 0.1660
 0.1779
 0.1808
 0.1496
 0.1422
 0.1513
 0.1878
 0.2188
 0.1713
 0.1808
 0.1708
 0.1740
 0.1948
 0.1855
 0.1471
 0.2028
 0.1877
 0.1652
 0.2179
 0.1838
 0.1411
 0.1505
 0.1518
 0.1280
 0.1933
 0.1554
 0.2108
 0.1802
 0.1989
 0.1377
 0.1663
 0.2136
 0.1776
 0.1433
 0.2122
 0.1375
 0.1992
 0.1610
 0.2016
 0.1363
 0.1538
 0.1745
 0.1832
 0.1642
 0.1766
 0.1907
 0.1841
 0.1769
 0.1363
 0.1773
 0.2047
 0.1605
 0.1429
 0.1888
 0.1546
 0.1621
 0.2075
 0.1818
 0.2592
 0.1556
 0.1623
 0.1629
 0.1437
 0.1796
 0.2407
 0.1981
 0.1521
 0.1912
 0.1876
 0.1903
 0.2092
 0.2329
 0.1720
 0.1952
 0.2492
 0.1949
 0.1370
 0.1582
 0.1846
 0.1572
 0.1790
 0.2107
 0.1947
 0.2062
 0.2048
 0.1569
 0.1942
 0.1783
 0.1753
 0.1565
 0.2462
 0.2125
 0.2283
 0.2043
 0.1430
 0.1792
 0.1885
 0.1889
 0.1545
 0.2051
 0.1732
 0.2047
 0.1689
 0.1760
 0.2027
 0.1972
 0.1848
 0.1899
 0.1931
 0.1787
 0.1904
 0.2183
 0.2140
 0.1466
 0.1988
[torch.FloatTensor of size 256]
), ('layer3.0.bn2.bias', 
 0.0773
-0.0444
-0.0011
 0.1554
-0.0344
 0.1005
 0.0093
 0.1300
 0.0225
 0.0268
 0.1820
-0.0870
 0.0875
-0.0556
-0.0356
 0.0574
 0.0247
 0.0101
 0.1577
 0.0743
 0.2682
 0.0857
-0.0070
 0.1249
-0.0706
-0.0712
-0.0088
-0.0447
 0.0214
 0.1029
 0.2384
 0.1282
 0.0671
 0.1777
 0.1912
-0.0200
-0.0020
-0.0530
-0.0078
-0.0101
 0.1210
-0.1773
 0.0087
-0.0088
 0.0232
 0.0687
 0.1901
 0.0318
 0.1751
-0.0254
 0.1010
 0.0832
-0.0733
 0.1143
 0.0120
-0.0234
-0.0493
 0.0348
-0.0171
 0.0118
-0.0069
-0.0326
 0.1289
 0.1441
 0.0088
 0.1439
 0.1763
 0.1491
 0.0492
-0.0350
 0.0852
 0.1501
 0.1618
 0.0455
-0.0269
-0.0567
 0.0437
 0.0069
 0.0741
-0.0112
 0.0835
 0.0410
 0.0302
 0.1220
-0.0325
-0.1232
 0.0673
-0.0367
 0.1539
-0.1075
-0.0162
 0.0483
 0.1149
 0.0188
 0.0244
-0.0230
 0.0430
 0.0331
-0.0470
-0.0014
-0.0946
 0.0166
-0.0577
 0.1232
-0.0745
 0.0574
-0.0054
 0.0449
 0.0357
 0.0356
-0.1227
-0.0197
-0.0299
-0.0971
 0.1370
-0.1095
-0.0213
-0.0317
 0.0216
 0.0603
 0.0327
-0.0267
 0.0322
 0.0711
 0.1230
 0.0849
-0.0542
-0.0803
 0.0505
 0.1593
 0.1004
 0.1389
-0.0184
 0.0572
-0.0356
-0.0851
-0.0102
 0.0718
-0.0122
 0.0814
 0.1117
-0.0144
-0.0080
 0.0370
 0.1115
 0.0804
 0.0444
-0.0104
-0.0648
 0.0352
-0.0378
 0.0429
 0.0117
 0.1067
-0.0010
 0.0942
-0.0599
-0.0109
 0.0166
-0.0481
 0.0112
 0.1343
 0.1379
 0.2243
 0.1111
-0.0404
 0.1319
-0.0412
 0.0487
-0.0235
 0.1136
 0.0616
-0.0009
 0.0025
 0.1923
-0.0109
 0.1585
 0.0392
 0.0456
 0.0624
 0.2003
 0.1642
-0.0360
-0.0110
 0.0634
 0.0840
-0.0357
 0.1361
 0.0075
 0.2502
 0.1275
-0.0573
 0.1072
 0.1343
-0.0718
 0.0359
 0.0222
-0.0470
 0.1273
-0.1589
 0.1498
 0.0466
 0.0935
 0.2316
-0.0205
-0.0258
-0.0233
-0.0309
-0.0345
-0.0752
 0.0002
-0.0760
-0.0305
 0.1273
 0.0917
-0.0648
 0.0051
 0.1696
 0.1308
-0.0488
 0.0752
-0.0411
-0.0846
-0.0431
 0.0355
 0.0601
 0.0774
-0.0078
 0.1132
 0.0668
 0.1066
-0.0187
-0.0244
-0.0844
-0.0265
 0.1637
-0.0145
 0.1225
 0.0041
 0.1196
-0.0401
 0.0708
-0.0002
 0.0604
-0.0266
-0.0951
 0.0017
-0.0708
-0.0062
-0.0031
-0.0458
 0.0098
 0.0545
-0.1338
 0.0139
-0.0439
[torch.FloatTensor of size 256]
), ('layer3.0.bn2.running_mean', 
 0.0044
-0.1629
-0.0800
-0.0574
-0.0410
-0.1045
-0.0629
 0.0809
 0.0419
-0.0127
 0.2574
-0.1921
-0.0982
 0.0328
 0.0053
 0.0307
 0.0396
-0.0194
-0.0132
-0.0848
 0.0141
-0.1479
-0.0396
-0.0144
-0.2087
-0.0630
-0.0483
-0.0547
 0.0036
 0.1065
-0.0314
 0.0137
-0.0001
-0.0378
-0.0237
-0.0254
-0.0272
-0.0004
-0.1066
-0.0567
-0.0445
-0.2568
-0.1193
-0.0133
-0.0247
 0.0297
-0.0249
 0.0087
 0.0443
-0.2172
-0.0902
-0.0212
 0.0097
-0.1511
 0.1422
-0.0181
-0.1075
-0.0263
-0.0195
 0.0265
-0.0468
-0.1227
-0.0198
-0.0730
-0.0643
 0.0850
-0.0462
 0.1647
 0.0395
-0.1327
-0.0745
-0.0112
 0.0584
 0.0662
-0.0552
-0.2293
-0.0772
-0.0540
 0.0139
-0.0963
 0.0353
-0.0854
-0.0592
 0.0579
-0.0307
-0.2702
-0.0574
-0.0895
 0.0314
-0.0743
-0.2317
-0.0152
 0.0009
-0.0204
-0.0238
-0.0385
-0.0585
-0.0665
-0.0252
-0.0540
-0.0714
 0.0097
 0.0330
 0.0220
 0.0218
-0.0790
-0.0693
-0.0107
-0.2191
-0.0772
-0.1596
 0.0023
-0.0300
-0.0122
-0.0099
-0.0351
 0.0223
-0.0608
-0.1223
-0.1506
-0.1217
 0.0679
-0.0373
-0.0291
 0.0752
-0.0182
-0.0707
-0.0404
-0.0428
-0.0601
-0.1075
 0.0196
-0.0513
-0.0788
-0.2436
-0.0791
-0.0523
 0.0030
-0.0561
 0.0257
-0.0247
-0.0867
 0.0246
 0.0080
 0.0501
 0.0115
-0.0487
-0.1961
-0.0445
 0.0905
-0.0334
-0.0330
 0.0545
 0.1578
-0.1848
-0.0068
-0.0907
-0.0930
-0.0983
-0.1131
 0.0307
-0.1501
-0.0273
-0.0142
-0.0572
 0.0061
-0.1919
-0.0707
-0.1607
 0.0013
 0.1151
-0.1160
-0.1812
-0.0324
-0.0300
-0.1577
 0.0179
-0.0777
-0.1055
-0.1377
-0.0528
 0.1116
-0.0437
-0.0515
-0.1109
 0.0319
-0.0352
 0.0342
 0.1359
-0.0416
 0.0177
-0.0411
 0.0526
-0.0875
-0.0531
 0.1461
-0.0906
-0.1177
-0.0040
-0.1969
-0.0513
 0.0342
-0.1155
 0.0167
-0.0264
-0.0013
 0.0126
-0.0191
-0.1237
-0.1031
-0.0185
-0.1057
-0.0360
-0.0470
-0.0367
-0.2804
-0.0404
 0.0695
 0.1689
-0.0081
-0.0282
-0.0563
-0.0909
-0.0117
-0.0915
-0.1327
-0.1364
-0.0753
 0.0251
-0.0744
-0.0349
-0.0324
 0.0616
-0.0659
-0.0979
 0.0018
-0.0730
 0.0332
-0.0540
-0.0365
 0.0798
-0.0213
 0.0507
-0.1608
-0.0232
-0.0413
-0.1323
-0.0528
-0.0779
-0.1470
 0.0343
-0.0351
 0.0087
-0.1721
-0.0409
-0.0286
[torch.FloatTensor of size 256]
), ('layer3.0.bn2.running_var', 
1.00000e-02 *
  2.9722
  7.8390
  3.9226
  3.9271
  1.5633
  6.0310
  2.0703
  2.2845
  2.8510
  3.5003
  3.3839
  5.7581
  2.8464
  2.0463
  1.7201
  2.2655
  2.6740
  1.6986
  3.1727
  4.0740
  4.0970
  2.9884
  2.3594
  3.3628
  2.8845
  2.2093
  4.2044
  2.7449
  3.5563
  2.8522
  1.9038
  2.5995
  4.4341
  2.1523
  1.9149
  4.6985
  2.4980
  3.6306
  4.2898
  2.1536
  2.7548
  6.2631
  4.4979
  2.1117
  1.9117
  2.8465
  1.6787
  2.1204
  2.4598
  3.6729
  4.6354
  2.4928
  2.6658
  3.0693
  3.7299
  1.8526
  3.3386
  3.0949
  1.8245
  2.1515
  2.1306
  2.6233
  3.1667
  2.1688
  3.3452
  2.2350
  2.1038
  2.0135
  3.0660
  2.4548
  2.4926
  5.6712
  2.2110
  1.9319
  1.6190
  5.3976
  3.9957
  3.1388
  2.1539
  3.2161
  2.2181
  3.9841
  2.8759
  2.1392
  2.9536
  7.5658
  5.9813
  2.4999
  2.2111
  1.7345
  5.2692
  2.5423
  3.5140
  2.5134
  1.9966
  1.7448
  3.1618
  8.8890
  2.1584
  2.7707
  4.5030
  2.4494
  2.6222
  5.0118
  1.6650
  4.0692
  2.3967
  2.1387
  3.9601
  4.1947
  2.4830
  2.5093
  6.9349
  2.3547
  2.9299
  2.1983
  1.9327
  3.3905
  3.5839
  2.7981
  4.2475
  2.7944
  2.2001
  3.7465
  3.0512
  2.9601
  1.8989
  2.2067
  1.7413
  2.0566
  4.2671
  5.4892
  9.2145
  3.4387
  9.5382
  2.1486
  3.2419
  2.1177
  1.6915
  3.3293
  5.3680
  3.7332
  2.3007
  2.8587
  2.9969
  2.2789
  2.0498
  2.5551
  2.4872
  3.7403
  3.1170
  2.9500
  2.8622
  4.0395
  3.2619
  2.0555
  2.1954
  2.4375
  5.3411
  4.4782
  3.9545
  2.0161
  1.9936
  3.6770
  2.2743
  1.6386
  2.9601
  2.3072
  5.4805
  2.0839
  1.9665
  1.9121
  3.8110
  1.8782
  2.4187
  5.8198
  2.1905
  4.0393
  1.9028
  8.1896
  1.9796
  2.3143
  2.0547
  1.7945
  2.9728
  3.7755
  2.5840
  5.1705
  3.7343
  1.7532
  3.5559
  2.7245
  3.2249
  2.3990
  2.1241
  2.1154
  1.5567
  2.9416
  7.2892
  5.2631
  2.3312
  2.9727
  4.8944
  2.4892
  2.3957
  3.9551
  2.4975
  1.6734
  1.5622
  1.9568
  2.5832
  4.6576
  5.2000
  3.2663
  3.3674
  5.4947
  2.1141
  2.3256
  3.0244
  1.7214
  2.1632
  1.8634
  4.0445
  2.5546
  3.5571
  4.3793
  2.2273
  2.2686
  4.0671
  2.9229
  3.8668
  9.9656
  3.4371
  3.4530
  3.5893
  2.2984
  1.7876
  5.3952
  2.2000
  1.8507
  3.2983
  3.1972
  2.6303
  3.9606
  2.3729
  2.4744
  2.6047
  2.3507
  3.3029
  5.3535
  2.3994
  2.1500
  4.1231
  2.5124
  2.0645
  2.2772
[torch.FloatTensor of size 256]
), ('layer3.0.conv3.weight', 
( 0  , 0  ,.,.) = 
 -9.9246e-03

( 0  , 1  ,.,.) = 
  1.9633e-02

( 0  , 2  ,.,.) = 
  3.7456e-02
      ... 

( 0  ,253 ,.,.) = 
 -7.0135e-03

( 0  ,254 ,.,.) = 
 -2.0328e-02

( 0  ,255 ,.,.) = 
 -2.6514e-02
        ⋮  

( 1  , 0  ,.,.) = 
 -1.6656e-03

( 1  , 1  ,.,.) = 
 -7.8092e-04

( 1  , 2  ,.,.) = 
  1.2820e-02
      ... 

( 1  ,253 ,.,.) = 
 -1.7758e-03

( 1  ,254 ,.,.) = 
 -4.2129e-03

( 1  ,255 ,.,.) = 
 -8.0088e-03
        ⋮  

( 2  , 0  ,.,.) = 
 -2.8890e-03

( 2  , 1  ,.,.) = 
 -5.8743e-03

( 2  , 2  ,.,.) = 
 -1.6458e-02
      ... 

( 2  ,253 ,.,.) = 
 -3.6337e-04

( 2  ,254 ,.,.) = 
 -2.6808e-02

( 2  ,255 ,.,.) = 
  3.0729e-02
 ...      
        ⋮  

(1021, 0  ,.,.) = 
 -9.9733e-03

(1021, 1  ,.,.) = 
 -4.3565e-03

(1021, 2  ,.,.) = 
 -1.4252e-02
      ... 

(1021,253 ,.,.) = 
 -1.0327e-02

(1021,254 ,.,.) = 
 -1.9345e-02

(1021,255 ,.,.) = 
  2.5460e-02
        ⋮  

(1022, 0  ,.,.) = 
  3.3327e-02

(1022, 1  ,.,.) = 
  6.7621e-04

(1022, 2  ,.,.) = 
 -9.6225e-03
      ... 

(1022,253 ,.,.) = 
 -1.5895e-02

(1022,254 ,.,.) = 
 -1.5170e-02

(1022,255 ,.,.) = 
 -1.9303e-02
        ⋮  

(1023, 0  ,.,.) = 
 -2.4183e-02

(1023, 1  ,.,.) = 
  5.4039e-03

(1023, 2  ,.,.) = 
 -2.7983e-02
      ... 

(1023,253 ,.,.) = 
  9.6453e-03

(1023,254 ,.,.) = 
  5.1977e-03

(1023,255 ,.,.) = 
 -1.6316e-02
[torch.FloatTensor of size 1024x256x1x1]
), ('layer3.0.bn3.weight', 
 1.6133e-01
 1.1563e-01
 8.9439e-02
     ⋮     
 1.0895e-01
 1.8421e-01
 1.3261e-01
[torch.FloatTensor of size 1024]
), ('layer3.0.bn3.bias', 
-1.4362e-02
-2.5333e-02
-2.6678e-02
     ⋮     
 1.2518e-02
 4.4802e-03
-8.8591e-03
[torch.FloatTensor of size 1024]
), ('layer3.0.bn3.running_mean', 
-2.5824e-02
-3.5349e-02
-3.0594e-02
     ⋮     
-6.8659e-02
-4.3388e-02
 4.2131e-02
[torch.FloatTensor of size 1024]
), ('layer3.0.bn3.running_var', 
 4.4377e-03
 1.8270e-03
 1.1492e-03
     ⋮     
 1.6631e-03
 5.3624e-03
 4.4409e-03
[torch.FloatTensor of size 1024]
), ('layer3.0.downsample.0.weight', 
( 0  , 0  ,.,.) = 
  2.8775e-02

( 0  , 1  ,.,.) = 
  2.0881e-02

( 0  , 2  ,.,.) = 
  8.0852e-03
      ... 

( 0  ,509 ,.,.) = 
  1.5601e-02

( 0  ,510 ,.,.) = 
  4.1752e-02

( 0  ,511 ,.,.) = 
 -2.3844e-02
        ⋮  

( 1  , 0  ,.,.) = 
 -9.3443e-03

( 1  , 1  ,.,.) = 
  3.6557e-03

( 1  , 2  ,.,.) = 
 -1.2121e-02
      ... 

( 1  ,509 ,.,.) = 
  1.2917e-03

( 1  ,510 ,.,.) = 
 -1.7267e-02

( 1  ,511 ,.,.) = 
 -3.3483e-03
        ⋮  

( 2  , 0  ,.,.) = 
  1.7818e-02

( 2  , 1  ,.,.) = 
 -1.5537e-02

( 2  , 2  ,.,.) = 
 -2.0420e-03
      ... 

( 2  ,509 ,.,.) = 
 -1.3075e-02

( 2  ,510 ,.,.) = 
 -8.0338e-03

( 2  ,511 ,.,.) = 
  1.0759e-03
 ...      
        ⋮  

(1021, 0  ,.,.) = 
 -3.4276e-03

(1021, 1  ,.,.) = 
 -1.3855e-02

(1021, 2  ,.,.) = 
 -1.1908e-02
      ... 

(1021,509 ,.,.) = 
 -4.2695e-03

(1021,510 ,.,.) = 
  8.3579e-03

(1021,511 ,.,.) = 
  5.0778e-02
        ⋮  

(1022, 0  ,.,.) = 
  2.5238e-02

(1022, 1  ,.,.) = 
 -1.8766e-02

(1022, 2  ,.,.) = 
 -1.3570e-02
      ... 

(1022,509 ,.,.) = 
  5.9170e-03

(1022,510 ,.,.) = 
  2.2531e-02

(1022,511 ,.,.) = 
  5.9379e-02
        ⋮  

(1023, 0  ,.,.) = 
 -3.9945e-02

(1023, 1  ,.,.) = 
 -1.7317e-02

(1023, 2  ,.,.) = 
  4.6550e-02
      ... 

(1023,509 ,.,.) = 
  7.9202e-03

(1023,510 ,.,.) = 
  1.0088e-02

(1023,511 ,.,.) = 
  1.4772e-02
[torch.FloatTensor of size 1024x512x1x1]
), ('layer3.0.downsample.1.weight', 
 1.0480e-01
 9.4626e-02
 6.6850e-02
     ⋮     
 9.3046e-02
 8.4846e-02
 1.7689e-01
[torch.FloatTensor of size 1024]
), ('layer3.0.downsample.1.bias', 
-1.4362e-02
-2.5333e-02
-2.6678e-02
     ⋮     
 1.2518e-02
 4.4802e-03
-8.8591e-03
[torch.FloatTensor of size 1024]
), ('layer3.0.downsample.1.running_mean', 
 3.5611e-02
 1.9434e-02
-4.6304e-02
     ⋮     
-1.4583e-01
 4.0409e-02
 1.8801e-02
[torch.FloatTensor of size 1024]
), ('layer3.0.downsample.1.running_var', 
 1.1367e-02
 4.0338e-03
 2.6370e-03
     ⋮     
 5.6357e-03
 6.3630e-03
 1.7682e-02
[torch.FloatTensor of size 1024]
), ('layer3.1.conv1.weight', 
( 0  , 0  ,.,.) = 
  3.9551e-03

( 0  , 1  ,.,.) = 
 -1.3039e-02

( 0  , 2  ,.,.) = 
  1.5156e-03
      ... 

( 0  ,1021,.,.) = 
  1.3798e-02

( 0  ,1022,.,.) = 
 -8.8331e-03

( 0  ,1023,.,.) = 
 -1.8208e-03
        ⋮  

( 1  , 0  ,.,.) = 
  3.8385e-03

( 1  , 1  ,.,.) = 
 -3.0152e-02

( 1  , 2  ,.,.) = 
 -9.4545e-04
      ... 

( 1  ,1021,.,.) = 
 -1.1199e-02

( 1  ,1022,.,.) = 
 -1.2743e-02

( 1  ,1023,.,.) = 
 -1.2080e-02
        ⋮  

( 2  , 0  ,.,.) = 
 -4.4737e-03

( 2  , 1  ,.,.) = 
  6.9822e-03

( 2  , 2  ,.,.) = 
  4.8189e-03
      ... 

( 2  ,1021,.,.) = 
 -8.4677e-03

( 2  ,1022,.,.) = 
 -6.3884e-03

( 2  ,1023,.,.) = 
 -1.3627e-02
 ...      
        ⋮  

(253 , 0  ,.,.) = 
 -1.4229e-02

(253 , 1  ,.,.) = 
 -7.7992e-03

(253 , 2  ,.,.) = 
 -6.5336e-03
      ... 

(253 ,1021,.,.) = 
  1.8012e-02

(253 ,1022,.,.) = 
 -1.0390e-03

(253 ,1023,.,.) = 
 -3.7104e-03
        ⋮  

(254 , 0  ,.,.) = 
 -3.9599e-03

(254 , 1  ,.,.) = 
 -2.9714e-03

(254 , 2  ,.,.) = 
 -2.7192e-03
      ... 

(254 ,1021,.,.) = 
 -3.1509e-03

(254 ,1022,.,.) = 
 -1.2503e-02

(254 ,1023,.,.) = 
  3.1340e-02
        ⋮  

(255 , 0  ,.,.) = 
  8.8526e-03

(255 , 1  ,.,.) = 
  4.2982e-03

(255 , 2  ,.,.) = 
  3.9464e-03
      ... 

(255 ,1021,.,.) = 
 -6.7523e-03

(255 ,1022,.,.) = 
 -2.6792e-03

(255 ,1023,.,.) = 
  1.1787e-03
[torch.FloatTensor of size 256x1024x1x1]
), ('layer3.1.bn1.weight', 
 0.1272
 0.1082
 0.1116
 0.1127
 0.1398
 0.1230
 0.1527
 0.1691
 0.1548
 0.0954
 0.1592
 0.1576
 0.1536
 0.1489
 0.1296
 0.1624
 0.2288
 0.1425
 0.1697
 0.1377
 0.1467
 0.1772
 0.1407
 0.1456
 0.1365
 0.1424
 0.1050
 0.1253
 0.1927
 0.1208
 0.1184
 0.1757
 0.1292
 0.1253
 0.1822
 0.1592
 0.1506
 0.1569
 0.1689
 0.1787
 0.1732
 0.1845
 0.1267
 0.1831
 0.1693
 0.1621
 0.1526
 0.1276
 0.1446
 0.1489
 0.1491
 0.1670
 0.1531
 0.1349
 0.1365
 0.1655
 0.1940
 0.1671
 0.1559
 0.1311
 0.2199
 0.2139
 0.1894
 0.1218
 0.1321
 0.1719
 0.1226
 0.1500
 0.1235
 0.1654
 0.1364
 0.1341
 0.1682
 0.1612
 0.1420
 0.1354
 0.1986
 0.1501
 0.1856
 0.2027
 0.1599
 0.1339
 0.2498
 0.1636
 0.1874
 0.1955
 0.1543
 0.1632
 0.1347
 0.1574
 0.2249
 0.1630
 0.1435
 0.1655
 0.1928
 0.1806
 0.0972
 0.1760
 0.1727
 0.1930
 0.1423
 0.1359
 0.1457
 0.2264
 0.1311
 0.1175
 0.1465
 0.1911
 0.2265
 0.1366
 0.1760
 0.2234
 0.1354
 0.1680
 0.1412
 0.1940
 0.1522
 0.2007
 0.1365
 0.1594
 0.1409
 0.1560
 0.1449
 0.1585
 0.1363
 0.1593
 0.1610
 0.1640
 0.1203
 0.1814
 0.1700
 0.1435
 0.1512
 0.1339
 0.1615
 0.1384
 0.1730
 0.1316
 0.1534
 0.1654
 0.1890
 0.1417
 0.1257
 0.1474
 0.1626
 0.1308
 0.1961
 0.1080
 0.1513
 0.1742
 0.1658
 0.1728
 0.1874
 0.1260
 0.1297
 0.1786
 0.1943
 0.1530
 0.1568
 0.1227
 0.1862
 0.1422
 0.1873
 0.1863
 0.1619
 0.1608
 0.1859
 0.1504
 0.2033
 0.1943
 0.1557
 0.2199
 0.1524
 0.1532
 0.1425
 0.1613
 0.1259
 0.1423
 0.1486
 0.1405
 0.1417
 0.1554
 0.1563
 0.1198
 0.1068
 0.1426
 0.1888
 0.1763
 0.1800
 0.1325
 0.1631
 0.1777
 0.1611
 0.1331
 0.1211
 0.1066
 0.1448
 0.1730
 0.1492
 0.1555
 0.1708
 0.1653
 0.1348
 0.1549
 0.3033
 0.1381
 0.1261
 0.1619
 0.1195
 0.1495
 0.1230
 0.1882
 0.1438
 0.1168
 0.1918
 0.1818
 0.1560
 0.1437
 0.1787
 0.1085
 0.1609
 0.1572
 0.1968
 0.1669
 0.1504
 0.2114
 0.1288
 0.1489
 0.1635
 0.1412
 0.1655
 0.1488
 0.1615
 0.1538
 0.1998
 0.1970
 0.1595
 0.1608
 0.1288
 0.1136
 0.1276
 0.1462
 0.1568
 0.1774
 0.1335
 0.1226
 0.1640
 0.2121
 0.1454
 0.1803
 0.1180
 0.1704
 0.1672
 0.1606
 0.1388
 0.1537
[torch.FloatTensor of size 256]
), ('layer3.1.bn1.bias', 
 0.1220
 0.0849
 0.0442
 0.0460
-0.0208
 0.0386
-0.0013
-0.0261
-0.0440
 0.1195
 0.0265
-0.1411
 0.0120
-0.0370
 0.0165
-0.0478
-0.1123
-0.0109
-0.0267
-0.0049
-0.0377
-0.0759
 0.0586
 0.0069
 0.0698
-0.0294
 0.1259
 0.0467
-0.0879
 0.0762
 0.0085
-0.0214
 0.0181
 0.0621
-0.0463
-0.0111
-0.0033
 0.0227
-0.0001
-0.0231
-0.0032
-0.0186
 0.0417
-0.0588
-0.0854
-0.0733
 0.0279
 0.0692
 0.0105
 0.0680
-0.0148
 0.0032
 0.0212
 0.0705
-0.0056
-0.0139
-0.0966
-0.0424
 0.0265
 0.0201
-0.1035
-0.1270
-0.0084
 0.0102
-0.0042
-0.0963
 0.0177
 0.0248
-0.0235
-0.0568
 0.0188
 0.0118
-0.0356
-0.0626
-0.0140
-0.0008
-0.1244
-0.0798
-0.0727
-0.1069
-0.0327
-0.0002
-0.0998
-0.0035
-0.0289
-0.1033
-0.0064
-0.0954
-0.0065
 0.0204
-0.1070
-0.0471
-0.0214
-0.0733
-0.0438
-0.0614
 0.0871
-0.0884
-0.0560
-0.1042
 0.0648
-0.0031
-0.0457
-0.1412
 0.0462
 0.0481
 0.0017
-0.0226
-0.1208
 0.0036
-0.0413
-0.0780
 0.0093
-0.0410
-0.0152
-0.0845
 0.0667
-0.0947
-0.0061
-0.0189
 0.0402
-0.0470
-0.0234
-0.0337
 0.0383
 0.0031
-0.0054
-0.0378
 0.0299
-0.1009
-0.0339
-0.0025
-0.0001
 0.0298
-0.0078
 0.0273
-0.0539
 0.0369
-0.0140
 0.0068
-0.0454
 0.0312
 0.0116
 0.0357
-0.0164
 0.1747
-0.0539
 0.0508
 0.0657
-0.0289
-0.0171
-0.0593
-0.1397
 0.0482
 0.0834
-0.0039
-0.0679
 0.0024
-0.0332
 0.0445
-0.1076
-0.0029
-0.0289
-0.0701
-0.0616
-0.0060
-0.0572
-0.0224
-0.1307
-0.0643
 0.0093
-0.1303
-0.0412
-0.0119
-0.0028
 0.0171
 0.0652
 0.1337
 0.0225
-0.0117
 0.0160
-0.0572
 0.0256
 0.0753
 0.0293
 0.0021
-0.0720
-0.0424
-0.1075
 0.0388
-0.0436
-0.0470
-0.0393
 0.0345
 0.0537
 0.0608
 0.0419
-0.0454
 0.0227
-0.0682
-0.0075
-0.0520
 0.0636
 0.0231
-0.1542
 0.0548
 0.0749
-0.0156
 0.1031
 0.0514
 0.0645
-0.1241
 0.0237
 0.0170
-0.0505
-0.0809
-0.0303
-0.0425
-0.0454
 0.1494
-0.0689
-0.0385
-0.0466
-0.0403
 0.0466
-0.1054
 0.0470
-0.0253
-0.0310
-0.0036
-0.0601
-0.0155
 0.0056
-0.0120
-0.1160
-0.0940
-0.0471
-0.0230
-0.0016
 0.0907
 0.0355
 0.0026
-0.0536
-0.0204
 0.0107
 0.0135
 0.0067
-0.1120
-0.0177
-0.0384
 0.0662
-0.0432
-0.0568
-0.0502
 0.0091
 0.0111
[torch.FloatTensor of size 256]
), ('layer3.1.bn1.running_mean', 
-0.1521
-0.1860
-0.0927
-0.0640
-0.0980
-0.0157
-0.2127
 0.0760
-0.0917
-0.2690
-0.1285
-0.1508
-0.1668
-0.1044
-0.0947
 0.0707
-0.2370
-0.0029
 0.0508
-0.0767
-0.2028
-0.0666
 0.0023
-0.0592
 0.5581
-0.0814
 0.0471
 0.0154
-0.0630
-0.0682
-0.1300
-0.0399
-0.0204
-0.0502
 0.1476
-0.0422
-0.0474
-0.0116
 0.0605
 0.0485
-0.0259
-0.0152
-0.1224
 0.2592
 0.0664
-0.0474
-0.0359
 0.0782
 0.1661
-0.0648
 0.0593
-0.1237
 0.0039
-0.0943
 0.0896
-0.1397
-0.0856
-0.1115
-0.0476
 0.0670
-0.1158
-0.0540
-0.4436
-0.3148
-0.1509
-0.0923
-0.0076
 0.0174
-0.2207
-0.0682
 0.0731
-0.0759
-0.0593
-0.0530
-0.1188
-0.0645
-0.1184
-0.0898
-0.1107
 0.0416
-0.0275
 0.0584
-0.1935
-0.1012
-0.0815
-0.1169
-0.0240
-0.0221
-0.1217
-0.0095
 0.0457
-0.0511
 0.0227
-0.0945
-0.0463
-0.0430
 0.0135
-0.0368
-0.0023
-0.1987
-0.0040
-0.0611
 0.0185
 0.0808
-0.0648
 0.0381
-0.3859
-0.0534
-0.0624
 0.0409
 0.2033
-0.1153
-0.0871
-0.0465
-0.0338
-0.1096
 0.0809
-0.0102
 0.0372
-0.1390
-0.0737
-0.0363
-0.1714
 0.2070
-0.0323
-0.0215
-0.0687
-0.0441
-0.2103
-0.0911
 0.0420
-0.1765
-0.0114
 0.0603
-0.1398
 0.2306
-0.2664
-0.1116
-0.1426
-0.0350
-0.0527
-0.1856
-0.0571
 0.0461
 0.0704
-0.8834
-0.0312
-0.0326
-0.3989
-0.0212
-0.1082
-0.0328
-0.0758
 0.1116
 0.0114
-0.0546
 0.0383
-0.0438
-0.0103
-0.2716
-0.0405
-0.0611
-0.2086
 0.0570
-0.0706
-0.0429
 0.0121
-0.2720
-0.0724
 0.1927
-0.0493
-0.0952
 0.1653
-0.1668
-0.0479
-0.0258
 0.0796
 0.1044
-0.0126
-0.0206
-0.0314
-0.0153
-0.0119
-0.1747
-0.0439
 0.1415
-0.0444
 0.0031
-0.1303
-0.0300
 0.0588
 0.0136
-0.0448
 0.0874
-0.1661
-0.1722
 0.0872
-0.0602
 0.0059
 0.0660
 0.0231
-0.0013
-0.1117
 0.1343
 0.0895
-0.1743
 0.0614
-0.0956
-0.0496
 0.0194
 0.0733
 0.0407
-0.0209
-0.1287
 0.1338
-0.0689
-0.0302
-0.0354
 0.0666
-0.4464
 0.0097
-0.0841
 0.0754
-0.0154
 0.0068
 0.1230
-0.7395
-0.0958
 0.0358
-0.0206
-0.0295
-0.1238
-0.1232
-0.0327
-0.0407
-0.0496
-0.0315
 0.0465
-0.0315
-0.1186
 0.0482
 0.0318
-0.1662
 0.1327
 0.1162
-0.0924
-0.1575
-0.1092
-0.0765
 0.0665
 0.0994
 0.0197
 0.0680
-0.0911
-0.0275
-0.0229
[torch.FloatTensor of size 256]
), ('layer3.1.bn1.running_var', 
1.00000e-02 *
  2.7381
  2.7120
  1.5981
  1.9595
  1.2562
  1.6953
  3.8208
  5.3395
  1.6276
  1.8773
  2.5420
  0.8488
  2.9595
  1.7871
  1.6458
  1.2528
  3.4191
  2.2441
  1.6395
  2.6396
  2.0278
  2.0931
  5.7663
  2.1441
  2.5447
  2.0429
  2.0752
  1.7846
  1.6165
  4.1718
  1.5387
  3.5365
  2.2855
  4.2262
  1.7257
  2.0236
  2.3043
  2.3820
  2.8404
  2.8173
  2.0141
  3.1002
  1.6949
  3.2275
  1.3916
  1.8309
  1.9927
  1.8556
  2.7260
  2.5204
  2.2503
  2.0871
  1.9377
  2.1981
  1.3983
  2.2260
  1.8867
  1.9553
  2.4097
  1.5947
  2.2328
  1.5074
  7.4370
  1.5814
  2.1675
  1.3154
  1.6360
  3.4511
  1.3003
  1.9864
  1.2231
  1.9046
  1.5950
  1.2262
  1.9095
  1.7537
  1.3218
  1.3805
  1.6992
  1.9932
  2.1464
  2.0061
  3.0108
  2.3614
  1.8169
  2.5560
  1.4591
  1.3378
  2.2977
  3.4288
  2.0566
  1.5042
  1.7234
  1.2196
  1.5241
  2.1982
  1.4716
  1.6301
  1.1468
  2.4596
  3.4573
  1.4364
  1.2301
  2.8245
  1.5827
  1.6760
  2.6572
  3.3353
  1.9457
  1.5024
  1.6197
  2.8170
  1.6822
  1.5089
  1.7929
  3.0236
  3.6782
  1.7605
  1.4851
  2.1846
  2.7562
  1.9348
  1.7110
  1.9169
  4.8901
  2.0622
  4.4067
  2.3740
  2.2853
  1.8793
  1.9624
  2.4318
  2.4372
  1.9281
  3.7182
  2.2239
  1.9855
  1.6550
  2.1403
  2.5499
  2.8193
  2.3538
  1.4003
  2.3867
  2.1801
  3.7521
  3.4814
  2.3892
  3.4516
  3.3721
  2.0941
  1.3039
  1.2790
  1.5566
  1.9187
  2.7680
  1.7672
  2.2809
  1.7276
  1.6740
  1.5160
  1.8971
  1.9967
  1.2259
  1.9748
  1.5342
  1.8764
  1.8411
  1.7060
  1.7353
  3.1344
  1.8097
  2.3112
  1.8962
  1.4888
  2.6376
  2.0601
  3.4121
  2.5825
  1.1351
  1.4212
  2.0127
  2.3397
  2.4784
  1.7882
  2.4631
  2.1547
  1.5415
  1.4264
  1.8973
  2.0788
  1.7516
  2.1109
  1.5851
  2.7168
  1.9091
  1.9756
  1.6168
  2.9326
  1.4264
  1.7787
  1.8059
  3.4741
  2.8068
  5.0164
  2.9587
  3.5976
  2.7313
  2.9291
  2.7637
  2.2653
  1.7689
  3.6254
  2.2727
  1.7041
  1.3445
  1.9671
  1.4668
  1.6960
  2.1168
  1.2584
  1.2309
  2.1621
  1.4171
  3.2435
  2.1893
  2.7236
  1.7743
  1.4747
  1.7262
  1.4662
  2.1435
  1.9070
  1.8385
  1.7463
  1.7060
  1.1695
  2.1939
  1.8390
  1.6626
  2.0432
  2.2603
  1.4241
  1.8773
  2.0035
  2.0626
  2.6285
  1.3413
  2.6778
  1.8417
  1.4417
  1.4618
  2.0294
  1.7894
  1.7766
  2.7691
[torch.FloatTensor of size 256]
), ('layer3.1.conv2.weight', 
( 0 , 0 ,.,.) = 
 -2.2209e-03  2.3680e-03 -9.0066e-03
  3.2583e-03 -1.5061e-03  5.6289e-03
  1.4029e-03 -1.7712e-04 -2.0088e-03

( 0 , 1 ,.,.) = 
  2.6363e-02  3.1852e-02  8.0712e-03
  5.2496e-03  1.5645e-03 -6.0794e-03
 -9.5910e-03 -2.8320e-02 -1.0562e-02

( 0 , 2 ,.,.) = 
 -4.6022e-03  4.7448e-03 -1.1191e-03
 -1.2140e-02  7.7773e-04 -1.7544e-03
  1.5518e-02  1.3508e-02  1.2042e-02
    ... 

( 0 ,253,.,.) = 
  1.2558e-02  6.9358e-03  4.4854e-03
 -1.4017e-02 -4.9447e-03  1.1854e-02
  1.2114e-02  1.4067e-02  1.1647e-02

( 0 ,254,.,.) = 
  4.3703e-03  7.9354e-03  4.4662e-03
  1.8583e-03  6.6458e-03  3.0552e-03
  1.7924e-03  3.8443e-03 -4.4407e-03

( 0 ,255,.,.) = 
 -2.9302e-02 -2.7518e-02 -1.3885e-02
 -1.4232e-02 -1.6384e-03 -1.3593e-04
  9.8944e-03  2.4967e-02  9.9636e-03
      ⋮  

( 1 , 0 ,.,.) = 
  1.1274e-03  1.9467e-03 -4.3517e-03
  8.5271e-03  3.2832e-03  6.5055e-04
 -2.2252e-02 -2.9143e-02 -1.4377e-02

( 1 , 1 ,.,.) = 
 -7.6910e-04  1.3005e-02 -1.3075e-02
 -8.2980e-03  9.6980e-03 -7.3141e-03
 -1.3871e-03  2.9279e-03  8.6375e-03

( 1 , 2 ,.,.) = 
 -8.5891e-03 -1.7072e-02 -1.3134e-02
  9.6039e-03  1.1632e-02 -1.0393e-02
  6.5844e-03  1.6355e-02  6.8924e-03
    ... 

( 1 ,253,.,.) = 
  7.2898e-03 -2.5705e-02 -5.8938e-03
  1.0163e-02  1.7513e-03  2.4627e-03
  1.3991e-02  2.0732e-02  1.6026e-02

( 1 ,254,.,.) = 
  2.6139e-03  4.8715e-03  5.5723e-03
  9.5424e-03  5.0184e-03  1.3310e-02
 -9.0100e-03 -1.8758e-03  4.6210e-03

( 1 ,255,.,.) = 
 -1.0261e-02 -1.5864e-02 -3.7571e-02
 -2.8334e-02 -4.4040e-02 -5.0564e-02
 -4.4633e-02 -7.7627e-02 -6.3077e-02
      ⋮  

( 2 , 0 ,.,.) = 
 -8.0014e-03 -3.2197e-02 -3.3246e-02
  1.4146e-02  6.8184e-03  3.4801e-03
  5.5120e-02  6.0446e-02  5.1732e-02

( 2 , 1 ,.,.) = 
  2.0362e-04  1.8760e-02  1.0876e-02
 -8.8109e-03  1.2179e-02  1.3240e-02
 -1.0618e-02 -6.2681e-03 -1.7830e-03

( 2 , 2 ,.,.) = 
  1.7463e-02  9.1279e-03  6.0448e-03
  9.0012e-03  9.2734e-03  3.0767e-03
 -4.4393e-03 -1.9744e-02 -1.1693e-02
    ... 

( 2 ,253,.,.) = 
  1.9538e-02  3.8320e-02  3.2642e-02
 -9.7889e-03  1.8268e-03  7.8323e-03
 -3.1404e-02 -3.0507e-02 -1.4063e-02

( 2 ,254,.,.) = 
  1.7386e-03  4.6947e-03  1.3606e-02
 -3.8464e-03 -9.0910e-04  7.1564e-03
 -1.2988e-02 -3.1593e-03 -5.1058e-03

( 2 ,255,.,.) = 
 -3.9896e-03 -1.3200e-02 -1.2730e-02
  8.8646e-03 -2.0722e-03 -2.9879e-03
  1.6947e-03 -3.6365e-03  5.4581e-03
...     
      ⋮  

(253, 0 ,.,.) = 
  1.1026e-03  7.2603e-04  1.1215e-03
  1.4615e-02  1.5489e-02  1.1199e-02
  5.5280e-03  1.8136e-02  1.3456e-02

(253, 1 ,.,.) = 
 -2.3531e-03 -1.7455e-03  1.1275e-03
 -9.8037e-03 -1.4158e-02 -2.5560e-03
 -2.8319e-04  4.9942e-03  2.3133e-03

(253, 2 ,.,.) = 
  6.3604e-03  6.0603e-04  4.6913e-03
  1.8093e-02  5.0797e-03 -2.1611e-03
  2.0538e-02  8.5691e-03 -4.9416e-04
    ... 

(253,253,.,.) = 
 -1.4583e-02 -1.8952e-03 -9.9537e-03
  3.2319e-03  1.1480e-02 -2.0378e-04
 -6.8825e-03 -2.2193e-02 -2.6134e-02

(253,254,.,.) = 
  1.8739e-02  5.0367e-03  3.4698e-03
  2.9795e-04  1.4573e-02  5.4955e-04
  7.8781e-03  1.0652e-02  8.1486e-04

(253,255,.,.) = 
  1.8247e-03 -4.9992e-03  7.0484e-03
 -2.2999e-02 -1.8280e-02 -8.2337e-03
  1.1130e-02  9.6509e-03 -9.9123e-03
      ⋮  

(254, 0 ,.,.) = 
  9.9050e-03  4.4067e-04  5.1800e-03
 -1.0217e-02  6.4841e-03 -3.8916e-03
  4.3049e-03  4.3258e-03  2.2527e-03

(254, 1 ,.,.) = 
  9.3521e-03  6.7698e-03  8.7522e-04
 -4.9717e-03 -2.2795e-02 -7.7713e-03
  1.3400e-02 -6.3761e-03  8.1015e-03

(254, 2 ,.,.) = 
 -9.7663e-03 -9.9591e-03  1.9212e-03
 -2.8908e-03 -1.3595e-02  3.3229e-03
  1.0392e-02  1.1982e-02  1.5520e-02
    ... 

(254,253,.,.) = 
  1.1608e-02  2.0908e-03  1.7663e-02
  1.6227e-02 -4.2351e-03  2.6666e-03
  1.3186e-02  5.4400e-03  7.4682e-03

(254,254,.,.) = 
 -8.6877e-03 -1.8332e-02 -2.1043e-02
 -9.4582e-03 -7.6206e-03 -1.2001e-02
 -1.9868e-03 -2.1778e-02 -1.0427e-02

(254,255,.,.) = 
  8.3325e-03  1.7789e-02  2.0029e-02
  1.1773e-02  3.6510e-02  2.0913e-02
 -1.1721e-02 -3.7434e-04 -1.5423e-02
      ⋮  

(255, 0 ,.,.) = 
  2.5556e-06  1.2850e-03  1.2534e-03
 -5.3958e-03  2.3388e-02  8.0807e-03
 -1.0583e-02  1.1113e-02 -1.0231e-02

(255, 1 ,.,.) = 
  1.2460e-02  3.9356e-03  4.9127e-04
  4.1070e-02  5.5333e-03 -1.7081e-02
  2.5684e-02 -5.8720e-05 -4.9908e-03

(255, 2 ,.,.) = 
  2.3689e-03 -9.2193e-03 -1.6447e-03
 -8.1313e-03 -6.4708e-03  1.3475e-02
  3.4459e-03  5.3489e-03  2.1980e-02
    ... 

(255,253,.,.) = 
  1.2527e-02 -1.6218e-02 -1.8189e-02
  5.9836e-02 -9.9851e-03 -4.4260e-02
  3.1816e-02 -7.3521e-03 -2.8641e-02

(255,254,.,.) = 
 -5.4672e-03  6.5547e-03  1.8037e-02
 -1.2493e-03  1.8533e-03  1.8728e-02
 -1.0131e-02  2.2487e-03  1.1088e-02

(255,255,.,.) = 
 -2.9215e-04 -1.0023e-02 -2.2871e-02
  1.4144e-02 -1.2906e-03 -1.3304e-02
  8.5294e-03 -6.6538e-03  1.4985e-03
[torch.FloatTensor of size 256x256x3x3]
), ('layer3.1.bn2.weight', 
 0.1731
 0.2074
 0.1871
 0.1670
 0.1743
 0.1365
 0.2004
 0.1824
 0.2402
 0.1968
 0.1630
 0.4164
 0.1519
 0.1665
 0.2208
 0.1864
 0.1493
 0.1711
 0.1702
 0.2000
 0.2133
 0.2083
 0.1884
 0.2025
 0.1830
 0.1845
 0.2036
 0.1779
 0.3539
 0.2236
 0.1405
 0.2074
 0.2006
 0.2243
 0.2032
 0.1363
 0.1512
 0.1482
 0.2272
 0.2192
 0.1916
 0.2044
 0.2011
 0.2135
 0.2072
 0.2053
 0.2050
 0.2323
 0.1812
 0.1631
 0.1622
 0.1982
 0.1931
 0.1841
 0.2158
 0.4189
 0.1667
 0.1853
 0.1501
 0.1280
 0.1877
 0.1226
 0.1902
 0.1675
 0.2236
 0.2411
 0.1310
 0.1536
 0.1590
 0.1794
 0.1336
 0.1989
 0.2203
 0.1763
 0.2166
 0.2028
 0.1512
 0.1346
 0.1707
 0.1583
 0.1901
 0.1736
 0.1830
 0.1583
 0.3012
 0.2218
 0.1625
 0.2095
 0.1862
 0.1600
 0.1737
 0.2166
 0.2180
 0.1315
 0.1386
 0.1776
 0.2055
 0.2165
 0.1934
 0.1610
 0.1864
 0.2007
 0.1906
 0.2107
 0.2152
 0.2188
 0.1987
 0.1693
 0.2073
 0.1779
 0.1385
 0.1956
 0.2203
 0.2052
 0.1521
 0.1754
 0.1018
 0.1567
 0.1643
 0.2062
 0.1977
 0.1523
 0.1541
 0.1947
 0.1393
 0.2942
 0.1816
 0.2287
 0.1166
 0.2058
 0.2029
 0.1816
 0.1845
 0.1960
 0.1667
 0.1753
 0.1789
 0.3119
 0.1645
 0.1912
 0.1760
 0.1181
 0.1657
 0.1974
 0.1536
 0.1472
 0.1989
 0.1978
 0.1810
 0.1807
 0.1541
 0.1739
 0.1682
 0.1769
 0.1755
 0.1692
 0.1688
 0.1553
 0.1559
 0.1652
 0.2011
 0.1417
 0.2715
 0.2047
 0.1816
 0.2231
 0.1999
 0.2152
 0.1256
 0.1918
 0.1789
 0.1762
 0.1772
 0.1765
 0.2098
 0.1923
 0.1752
 0.1506
 0.1658
 0.1828
 0.2622
 0.1481
 0.1924
 0.2001
 0.1969
 0.1991
 0.1572
 0.2127
 0.1484
 0.1429
 0.1161
 0.1614
 0.2227
 0.1616
 0.2000
 0.1774
 0.1983
 0.1605
 0.1347
 0.1104
 0.2107
 0.1986
 0.2226
 0.2348
 0.1589
 0.1757
 0.1925
 0.1488
 0.1878
 0.1897
 0.1988
 0.1849
 0.1856
 0.2192
 0.2019
 0.2119
 0.2107
 0.1189
 0.2126
 0.1324
 0.1402
 0.2058
 0.1748
 0.1862
 0.1895
 0.1913
 0.1620
 0.2373
 0.2028
 0.1856
 0.2267
 0.1578
 0.1179
 0.1834
 0.1901
 0.1920
 0.1785
 0.2026
 0.1830
 0.1807
 0.1281
 0.1472
 0.1712
 0.1617
 0.1966
 0.1334
 0.1619
 0.1734
 0.2112
 0.1783
 0.2074
 0.2375
 0.1459
 0.2741
 0.1850
 0.2070
[torch.FloatTensor of size 256]
), ('layer3.1.bn2.bias', 
-0.0682
-0.0851
-0.0391
-0.0351
-0.0896
 0.1120
-0.1100
-0.0800
-0.2344
-0.0984
-0.0308
-0.3539
-0.0335
-0.0437
-0.0521
-0.0073
-0.0157
-0.0517
-0.0711
-0.0671
-0.0659
-0.0882
-0.0913
-0.0722
-0.0784
 0.0092
-0.0671
-0.0151
-0.3271
-0.0666
 0.0887
-0.0824
-0.0853
-0.0969
-0.0878
 0.0835
-0.0145
-0.0468
-0.0697
-0.1085
-0.0448
-0.0667
-0.0889
-0.0864
-0.0976
-0.1196
-0.0993
-0.1718
-0.1018
 0.0340
 0.0450
-0.0912
-0.0546
-0.0343
-0.0963
-0.3843
-0.1074
-0.1241
 0.0003
-0.0112
-0.0981
 0.0642
 0.0102
-0.0269
-0.1398
-0.0941
 0.0107
-0.0116
 0.0233
-0.0522
 0.0125
-0.0986
-0.0823
-0.0231
-0.1280
-0.0397
-0.0063
 0.0787
-0.0260
 0.0622
-0.0408
-0.0391
-0.0817
 0.0169
-0.1824
-0.0140
 0.0718
-0.1180
-0.0929
 0.0562
-0.0665
-0.1822
-0.1039
 0.0412
 0.0474
-0.0959
-0.1025
-0.1063
-0.0606
-0.0087
-0.0768
-0.0650
-0.0905
-0.0862
-0.1085
-0.1113
-0.0962
-0.0326
-0.1589
-0.0490
 0.0086
-0.0740
-0.1155
-0.0948
 0.0280
-0.0645
 0.1358
 0.1967
 0.0292
-0.0370
-0.0452
-0.0463
-0.0176
-0.0805
 0.0257
-0.2905
-0.0805
-0.1202
 0.0980
-0.1287
-0.1120
-0.0723
-0.1051
-0.1412
-0.0519
-0.0583
-0.0315
-0.2446
-0.0379
-0.0453
 0.0040
 0.1514
-0.0498
-0.0638
 0.0038
 0.0538
-0.0915
-0.0760
 0.0302
-0.0889
-0.0263
-0.0875
 0.0158
-0.0589
-0.0357
-0.0569
 0.0634
 0.0105
-0.0070
 0.0627
-0.0758
 0.0863
-0.2153
-0.0810
-0.0738
-0.1313
-0.0570
-0.1886
 0.0618
-0.0812
-0.0471
-0.0338
-0.0388
-0.0150
-0.0710
-0.1072
-0.1244
 0.0584
-0.0344
-0.0279
-0.0645
-0.0091
-0.0496
-0.0857
-0.1132
-0.1535
-0.0086
-0.0866
-0.0175
 0.0114
 0.1744
-0.0258
-0.1125
-0.0245
-0.0331
-0.0503
-0.1195
 0.0278
 0.0685
 0.1036
-0.0832
-0.1362
-0.1125
-0.0856
-0.0136
-0.0638
-0.0670
 0.0159
-0.1076
-0.1329
-0.0898
-0.0535
-0.0888
-0.1127
-0.1038
-0.0894
-0.0815
 0.0355
-0.0972
 0.0592
 0.0117
-0.0993
-0.0567
 0.0058
-0.0914
-0.0783
 0.0089
-0.1392
-0.0782
-0.1044
-0.0695
-0.0227
 0.0968
-0.0742
-0.0867
-0.0921
-0.0921
-0.0667
-0.0957
-0.0875
 0.0540
-0.0079
-0.0345
 0.0074
-0.1009
 0.0737
-0.0077
-0.0230
-0.0790
-0.0560
-0.0897
-0.1555
 0.0294
-0.2282
-0.0963
-0.1058
[torch.FloatTensor of size 256]
), ('layer3.1.bn2.running_mean', 
 0.1257
-0.0945
 0.0087
-0.0431
 0.0081
 0.1648
-0.0371
 0.5170
-0.0749
-0.0574
 0.0229
-0.0504
 0.0632
 0.0316
-0.0758
 0.0046
 0.1097
-0.0258
 0.0444
-0.2042
 0.0055
-0.0347
-0.0887
-0.0662
-0.1050
 0.1085
-0.1472
-0.0580
 0.1338
-0.7589
 0.1726
 0.0439
 0.0340
-0.0638
 0.2054
 0.2341
 0.0503
-0.1226
-0.2965
-0.0769
-0.0562
-0.0575
-0.0152
-0.1100
-0.0669
 0.0037
-0.0481
-0.0462
-0.0943
 0.0962
 0.1246
-0.0953
 0.0648
-0.1613
-0.0928
-0.3439
-0.0169
-0.0194
 0.0638
-0.0053
-0.0645
-0.0158
 0.0539
 0.0164
-0.1406
-0.1291
 0.0575
 0.0759
 0.0378
-0.0377
-0.0138
-0.0452
-0.1122
-0.0423
-0.0353
 0.0157
 0.1100
 0.1927
-0.0012
 0.2500
-0.0213
 0.1487
-0.0347
-0.2071
-0.1479
-0.0347
 0.2210
-0.0605
-0.1088
-0.0038
-0.0460
 0.1460
-0.0335
 0.0724
-0.0299
 0.1017
-0.0609
-0.1604
 0.0191
-0.0114
 0.0804
 0.0019
-0.0449
-0.0277
-0.0393
-0.0699
-0.0744
-0.1124
 0.1796
 0.0583
-0.0119
-0.0594
-0.2422
-0.0936
 0.0519
 0.0078
 0.5742
-0.3431
 0.0507
 0.0381
-0.0288
-0.0443
 0.0487
-0.1629
-0.1806
-0.1999
-0.0656
-0.0537
-0.1324
-0.1118
-0.1049
-0.0056
-0.0210
-0.0579
-0.1017
-0.0920
 0.0224
-0.3508
 0.1133
-0.2186
 0.0523
 0.4211
 0.0403
-0.0593
 0.0938
 0.0848
-0.1579
-0.2098
-0.0928
-0.1504
 0.0332
 0.0338
 0.0247
 0.0513
 0.1191
-0.0088
-0.1028
-0.1826
-0.1123
-0.1776
-0.0490
-0.1422
-0.0715
-0.0319
-0.1108
-0.1917
-0.0645
-0.0336
 0.0305
 0.1972
-0.0679
 0.0019
-0.0097
 0.0073
-0.1929
 0.0029
-0.0531
-0.1383
 0.0251
 0.0270
-0.8972
-0.1870
-0.0644
-0.0891
-0.0556
 0.1031
-0.1242
-0.0725
-0.0313
-0.1514
 0.1496
-0.0280
-0.0943
-0.0300
-0.0493
-0.0588
-0.0312
 0.0066
-0.0987
 0.0265
-0.0799
-0.1195
-0.0628
-0.0419
 0.0156
 0.0001
-0.0080
 0.0339
 0.0306
-0.0922
-0.0132
 0.0253
-0.0091
 0.0862
-0.0477
-0.0545
-0.0065
-0.0655
-0.0715
 0.0987
-0.0669
-0.0476
-0.0397
-0.1556
-0.0295
-0.0453
 0.0091
-0.0441
-0.2172
 0.1002
-0.1390
-0.0118
 0.1911
 0.0065
-0.2276
-0.0450
-0.0732
 0.0248
-0.0607
-0.0808
-0.2030
 0.0143
 0.0074
-0.0252
 0.0372
-0.0684
-0.0813
 0.0665
-0.1296
 0.0552
 0.0001
-0.1814
 0.0280
-0.2406
 0.0108
 0.0525
[torch.FloatTensor of size 256]
), ('layer3.1.bn2.running_var', 
1.00000e-02 *
  1.8654
  2.3288
  2.4987
  1.3106
  1.4353
  3.7386
  1.5570
  2.0490
  3.5515
  1.8750
  2.6628
  6.6246
  1.8214
  2.0128
  4.5900
  3.2519
  1.6840
  1.5913
  1.8359
  2.0059
  3.1484
  2.9865
  1.3724
  2.6748
  1.8357
  3.1462
  2.3046
  2.6668
  6.3552
  5.3755
  3.7532
  2.3889
  1.6273
  2.6563
  1.8810
  3.6358
  0.9269
  1.2227
  3.6932
  2.6903
  2.0283
  2.8578
  2.4948
  3.4462
  2.0353
  1.6886
  1.8455
  1.4424
  1.4690
  2.5495
  4.0952
  2.2018
  1.7842
  1.5143
  3.1182
  6.8132
  1.1843
  1.5619
  2.1363
  1.2121
  1.6382
  1.5046
  2.6884
  1.7568
  1.6645
  4.2799
  1.0866
  2.9729
  3.3098
  1.4851
  1.5253
  1.8243
  2.3902
  3.6876
  2.1414
  2.7864
  2.1876
  2.5273
  2.2672
  3.1414
  2.0815
  2.6194
  1.3546
  1.8455
  3.1420
  4.5877
  4.7817
  2.0450
  1.6236
  3.9536
  1.1610
  1.3280
  3.3556
  1.2800
  2.0928
  1.8446
  1.7221
  1.9861
  1.6313
  1.9568
  1.5591
  1.9309
  1.8834
  2.6628
  2.1475
  3.0951
  1.9211
  2.3215
  2.6247
  1.3028
  1.6689
  1.9158
  1.9291
  2.2561
  2.4528
  1.5176
  1.3307
  2.8166
  3.6403
  2.6568
  1.8733
  0.9623
  1.9437
  1.6321
  1.5717
  1.8284
  1.7048
  3.0224
  1.3394
  1.9650
  1.5131
  1.5134
  1.9455
  1.6143
  1.4642
  1.1353
  1.9649
  3.5294
  2.6248
  1.8921
  2.4550
  1.5275
  2.0832
  2.2432
  1.7391
  2.8202
  1.6454
  2.8465
  2.6071
  1.4963
  1.4066
  1.3312
  1.9901
  2.4358
  2.5440
  1.6416
  3.0298
  1.6724
  1.6910
  3.1396
  2.5293
  3.2034
  1.9697
  3.0340
  1.8848
  1.8559
  2.9249
  1.4842
  2.4857
  2.3244
  1.8184
  2.4312
  2.3457
  1.9388
  2.1106
  2.0349
  1.0640
  2.7337
  1.9735
  1.7594
  7.0275
  1.5245
  1.8218
  3.0226
  1.9107
  1.5647
  1.6201
  2.5578
  1.4466
  1.3996
  1.3835
  1.7870
  2.6029
  1.1378
  2.5869
  1.5825
  1.6314
  2.1578
  2.2401
  1.4145
  2.4102
  1.9212
  2.3204
  3.7032
  1.7298
  1.4920
  2.0154
  1.0274
  2.2004
  1.4253
  2.2398
  2.0924
  2.0136
  1.8740
  2.0158
  2.3350
  1.9002
  1.0928
  1.9079
  2.4873
  1.7073
  1.4445
  1.0336
  3.3473
  2.0680
  3.0095
  1.8389
  2.2431
  2.5526
  2.0285
  4.8626
  1.6937
  2.2475
  2.1528
  1.3352
  1.9429
  1.3345
  2.7742
  2.0979
  1.5241
  1.9232
  1.7114
  2.0238
  1.8786
  1.8749
  2.6014
  1.6305
  2.3923
  1.7599
  1.3218
  1.7764
  1.7568
  2.5339
  2.7266
  1.3896
  1.9461
[torch.FloatTensor of size 256]
), ('layer3.1.conv3.weight', 
( 0  , 0  ,.,.) = 
 -6.3724e-03

( 0  , 1  ,.,.) = 
  2.3534e-02

( 0  , 2  ,.,.) = 
  2.4592e-03
      ... 

( 0  ,253 ,.,.) = 
  4.1078e-03

( 0  ,254 ,.,.) = 
  2.4874e-03

( 0  ,255 ,.,.) = 
 -1.0990e-02
        ⋮  

( 1  , 0  ,.,.) = 
 -8.3257e-03

( 1  , 1  ,.,.) = 
  1.2592e-02

( 1  , 2  ,.,.) = 
 -8.2291e-03
      ... 

( 1  ,253 ,.,.) = 
  1.0739e-02

( 1  ,254 ,.,.) = 
 -7.6186e-03

( 1  ,255 ,.,.) = 
  1.3689e-02
        ⋮  

( 2  , 0  ,.,.) = 
 -1.0180e-02

( 2  , 1  ,.,.) = 
 -9.5879e-03

( 2  , 2  ,.,.) = 
 -5.3071e-04
      ... 

( 2  ,253 ,.,.) = 
  1.5783e-02

( 2  ,254 ,.,.) = 
  1.0041e-02

( 2  ,255 ,.,.) = 
  1.8456e-03
 ...      
        ⋮  

(1021, 0  ,.,.) = 
  3.2607e-02

(1021, 1  ,.,.) = 
 -9.3498e-03

(1021, 2  ,.,.) = 
  2.1410e-02
      ... 

(1021,253 ,.,.) = 
  4.0840e-02

(1021,254 ,.,.) = 
 -1.1005e-02

(1021,255 ,.,.) = 
  4.2452e-02
        ⋮  

(1022, 0  ,.,.) = 
  1.2486e-02

(1022, 1  ,.,.) = 
  6.3653e-03

(1022, 2  ,.,.) = 
 -1.2754e-02
      ... 

(1022,253 ,.,.) = 
 -1.4025e-02

(1022,254 ,.,.) = 
 -7.0006e-03

(1022,255 ,.,.) = 
 -1.3153e-02
        ⋮  

(1023, 0  ,.,.) = 
 -1.4763e-02

(1023, 1  ,.,.) = 
  2.1255e-02

(1023, 2  ,.,.) = 
  5.7463e-03
      ... 

(1023,253 ,.,.) = 
 -6.0617e-03

(1023,254 ,.,.) = 
 -6.3444e-03

(1023,255 ,.,.) = 
 -8.2349e-03
[torch.FloatTensor of size 1024x256x1x1]
), ('layer3.1.bn3.weight', 
 4.8032e-02
 8.2487e-02
 1.1153e-01
     ⋮     
 1.3482e-01
 1.0690e-01
 1.0801e-01
[torch.FloatTensor of size 1024]
), ('layer3.1.bn3.bias', 
-0.0647
-0.0260
-0.0057
   ⋮   
-0.0232
-0.0627
-0.0947
[torch.FloatTensor of size 1024]
), ('layer3.1.bn3.running_mean', 
-3.9798e-02
-5.2506e-04
 1.1214e-02
     ⋮     
 1.5222e-02
 1.4079e-02
-3.5314e-02
[torch.FloatTensor of size 1024]
), ('layer3.1.bn3.running_var', 
 5.8304e-04
 7.6715e-04
 1.2868e-03
     ⋮     
 1.6551e-03
 1.5171e-03
 1.5573e-03
[torch.FloatTensor of size 1024]
), ('layer3.2.conv1.weight', 
( 0  , 0  ,.,.) = 
  2.8827e-03

( 0  , 1  ,.,.) = 
 -2.8368e-02

( 0  , 2  ,.,.) = 
 -2.5213e-03
      ... 

( 0  ,1021,.,.) = 
 -4.1888e-02

( 0  ,1022,.,.) = 
 -2.7244e-02

( 0  ,1023,.,.) = 
 -1.0362e-02
        ⋮  

( 1  , 0  ,.,.) = 
 -1.2423e-02

( 1  , 1  ,.,.) = 
 -3.0724e-03

( 1  , 2  ,.,.) = 
  1.1820e-02
      ... 

( 1  ,1021,.,.) = 
  4.6387e-02

( 1  ,1022,.,.) = 
 -3.1436e-03

( 1  ,1023,.,.) = 
  2.2673e-02
        ⋮  

( 2  , 0  ,.,.) = 
  3.5264e-03

( 2  , 1  ,.,.) = 
 -2.7772e-02

( 2  , 2  ,.,.) = 
  1.9115e-02
      ... 

( 2  ,1021,.,.) = 
 -6.3041e-03

( 2  ,1022,.,.) = 
 -3.1398e-02

( 2  ,1023,.,.) = 
  1.3827e-02
 ...      
        ⋮  

(253 , 0  ,.,.) = 
  5.8742e-03

(253 , 1  ,.,.) = 
  5.8682e-03

(253 , 2  ,.,.) = 
  3.8751e-03
      ... 

(253 ,1021,.,.) = 
 -1.2832e-02

(253 ,1022,.,.) = 
 -1.6594e-02

(253 ,1023,.,.) = 
  1.1244e-02
        ⋮  

(254 , 0  ,.,.) = 
  1.5227e-02

(254 , 1  ,.,.) = 
 -9.1758e-03

(254 , 2  ,.,.) = 
 -8.0038e-03
      ... 

(254 ,1021,.,.) = 
 -1.0496e-02

(254 ,1022,.,.) = 
 -1.5176e-02

(254 ,1023,.,.) = 
  2.1703e-02
        ⋮  

(255 , 0  ,.,.) = 
  1.6154e-02

(255 , 1  ,.,.) = 
  1.6533e-02

(255 , 2  ,.,.) = 
  6.5476e-03
      ... 

(255 ,1021,.,.) = 
 -1.4464e-02

(255 ,1022,.,.) = 
  1.0980e-03

(255 ,1023,.,.) = 
 -1.0425e-02
[torch.FloatTensor of size 256x1024x1x1]
), ('layer3.2.bn1.weight', 
 0.2235
 0.1873
 0.1171
 0.1336
 0.1773
 0.1729
 0.1879
 0.1848
 0.1492
 0.1673
 0.1416
 0.1602
 0.1017
 0.0980
 0.1936
 0.1305
 0.1814
 0.1877
 0.1290
 0.1670
 0.1490
 0.1928
 0.1335
 0.1712
 0.2340
 0.2333
 0.1540
 0.1447
 0.1428
 0.2174
 0.1549
 0.1337
 0.1050
 0.1420
 0.1685
 0.1133
 0.1701
 0.1309
 0.1737
 0.1704
 0.1224
 0.1849
 0.1040
 0.1580
 0.1581
 0.1564
 0.1767
 0.1913
 0.1188
 0.1222
 0.1635
 0.1957
 0.1553
 0.2126
 0.1243
 0.1475
 0.1571
 0.1037
 0.1739
 0.1467
 0.1578
 0.1570
 0.1661
 0.1471
 0.1935
 0.1229
 0.1083
 0.1505
 0.1414
 0.1675
 0.1396
 0.2291
 0.1739
 0.1506
 0.1584
 0.1762
 0.1559
 0.1363
 0.1720
 0.1711
 0.1619
 0.1372
 0.1843
 0.1320
 0.1509
 0.1483
 0.1984
 0.1078
 0.1616
 0.2366
 0.1402
 0.1534
 0.1739
 0.1415
 0.1707
 0.1558
 0.1246
 0.1815
 0.1427
 0.2065
 0.1840
 0.1461
 0.1611
 0.1643
 0.1481
 0.2103
 0.1430
 0.1069
 0.1375
 0.1731
 0.1506
 0.1766
 0.1287
 0.0985
 0.1829
 0.1552
 0.1715
 0.0993
 0.1151
 0.1370
 0.1645
 0.1582
 0.1257
 0.1490
 0.1467
 0.1230
 0.1268
 0.1470
 0.1401
 0.1523
 0.1669
 0.1628
 0.1419
 0.1525
 0.1438
 0.1671
 0.1612
 0.1437
 0.1443
 0.1590
 0.1501
 0.1948
 0.1376
 0.1607
 0.1591
 0.1243
 0.1584
 0.1740
 0.1976
 0.1319
 0.1418
 0.1325
 0.1438
 0.1928
 0.1405
 0.1653
 0.1511
 0.1458
 0.1797
 0.1337
 0.1317
 0.1902
 0.1464
 0.1427
 0.1532
 0.1824
 0.1675
 0.1986
 0.1489
 0.1340
 0.1183
 0.1376
 0.1944
 0.1688
 0.1288
 0.1371
 0.1695
 0.1649
 0.1248
 0.1500
 0.1819
 0.1421
 0.1907
 0.1399
 0.1824
 0.1318
 0.1926
 0.1596
 0.1611
 0.1655
 0.1560
 0.1445
 0.1513
 0.1360
 0.1715
 0.1498
 0.1814
 0.1285
 0.1634
 0.2138
 0.1419
 0.1172
 0.1420
 0.1975
 0.1398
 0.1707
 0.1561
 0.1520
 0.1897
 0.1404
 0.2061
 0.1286
 0.1501
 0.1298
 0.1451
 0.1348
 0.1901
 0.1154
 0.1924
 0.1203
 0.1630
 0.1382
 0.1270
 0.1348
 0.1255
 0.1802
 0.1576
 0.1258
 0.1703
 0.1120
 0.1179
 0.1749
 0.1213
 0.1250
 0.1880
 0.1126
 0.1604
 0.1708
 0.1650
 0.1671
 0.1411
 0.1908
 0.1489
 0.1521
 0.2002
 0.1725
 0.1181
 0.1745
 0.1503
 0.1096
 0.1502
 0.1724
 0.1517
 0.1597
 0.1411
 0.1436
[torch.FloatTensor of size 256]
), ('layer3.2.bn1.bias', 
-0.1153
-0.1992
 0.0628
 0.0422
-0.0964
-0.1043
-0.0946
-0.1055
-0.0161
-0.0874
-0.0508
-0.0519
 0.1040
 0.0593
-0.0399
 0.0009
-0.0857
-0.1124
 0.0024
-0.0689
-0.0599
-0.1073
 0.0200
-0.0628
-0.1832
-0.1608
-0.0777
-0.0135
-0.0229
-0.1460
-0.1282
-0.0234
 0.0851
-0.0166
-0.0403
 0.0237
-0.0470
 0.0321
-0.1158
-0.0258
 0.0330
-0.1253
 0.0473
-0.0672
-0.0544
 0.0179
-0.0576
-0.1480
 0.0362
-0.0002
-0.0598
-0.0935
-0.1279
-0.1138
 0.0243
 0.0085
-0.0976
 0.1186
-0.0898
-0.0741
-0.0487
-0.0462
-0.0780
-0.0190
-0.0050
 0.0345
 0.0936
-0.0416
-0.0457
-0.1221
-0.0061
-0.2316
-0.0468
-0.0584
-0.0322
-0.1026
-0.0254
-0.0301
-0.0734
-0.0109
-0.0446
 0.0218
-0.0049
 0.0203
-0.0990
 0.0190
-0.1278
 0.0519
-0.0292
-0.0879
 0.0071
-0.0577
-0.0466
-0.0300
-0.1127
-0.0791
-0.0110
-0.1309
-0.0399
-0.1383
-0.1191
-0.0569
-0.0807
-0.0822
 0.0002
-0.2088
 0.0078
 0.0311
-0.0306
-0.0592
-0.0380
-0.1168
 0.0270
 0.0711
-0.0910
-0.0429
-0.0818
 0.1021
 0.0863
-0.0272
-0.0480
-0.0459
-0.0025
-0.0241
-0.0229
-0.0039
 0.0128
-0.0811
-0.0465
-0.0577
-0.0664
-0.0260
 0.0169
-0.0575
-0.0408
-0.0583
-0.0747
-0.0098
-0.0481
-0.0953
-0.0299
-0.0947
-0.0090
-0.0821
-0.0753
 0.0148
-0.0669
-0.0864
-0.1115
 0.0252
-0.0163
-0.0107
-0.0034
-0.1146
-0.0008
-0.1254
-0.0722
-0.0411
-0.0878
-0.0354
 0.0252
-0.1397
-0.0852
-0.0475
-0.1516
-0.1362
-0.0456
-0.1530
-0.0146
 0.0129
 0.0501
-0.0238
-0.1141
-0.0091
 0.0130
-0.0172
-0.0405
-0.0968
 0.0388
-0.0249
-0.0939
-0.0435
-0.0896
-0.0161
-0.0463
-0.0165
-0.1177
-0.0453
-0.0835
-0.0824
-0.0726
-0.0417
-0.0990
-0.0350
-0.0870
-0.0277
-0.1810
 0.0337
 0.0543
-0.1367
-0.0697
 0.0301
-0.0135
-0.1152
-0.0763
-0.0659
-0.0534
-0.0412
-0.1344
-0.0177
-0.1512
-0.0178
-0.1142
-0.0093
-0.0622
-0.0654
-0.1472
 0.0123
-0.1172
 0.0396
-0.0550
-0.0423
 0.0328
-0.0191
 0.1064
-0.0650
-0.0401
-0.0076
-0.0657
 0.0405
 0.0278
-0.1149
 0.0298
-0.0368
-0.0929
 0.0423
-0.0758
-0.1033
-0.0653
-0.0877
-0.0171
-0.1000
-0.0304
-0.0560
-0.1225
-0.0064
 0.0038
-0.0925
-0.0310
 0.0688
-0.0719
-0.0698
 0.0347
-0.0650
-0.0171
-0.0507
[torch.FloatTensor of size 256]
), ('layer3.2.bn1.running_mean', 
-0.1391
 0.0362
 0.0078
-0.0047
-0.1341
-0.0179
-0.0185
-0.0786
-0.0354
-0.0987
 0.0192
-0.1007
-0.0704
-0.1749
-0.1059
-0.0712
-0.1234
-0.0227
-0.0740
-0.1905
 0.0009
-0.1361
-0.0846
-0.0590
-0.1966
-0.0266
-0.1096
-0.0667
-0.0743
-0.1186
 0.0140
 0.0042
-0.0562
-0.1017
-0.0452
-0.0410
-0.0417
-0.0510
 0.0927
 0.0433
-0.0799
-0.1132
 0.0072
-0.0034
-0.0836
-0.1191
-0.0102
 0.0335
-0.0954
-0.0220
-0.0466
-0.0802
-0.0259
-0.0582
-0.0004
 0.0258
-0.0201
-0.0240
-0.0157
 0.0099
 0.0732
 0.0268
 0.0037
 0.0244
 0.0414
-0.0463
-0.1639
-0.0578
-0.0064
-0.0622
-0.0235
-0.0668
 0.0263
-0.0640
-0.0930
-0.0533
-0.0845
-0.0646
-0.0120
-0.0706
-0.1183
 0.0890
-0.1839
-0.0333
-0.0151
-0.1114
-0.0813
 0.0148
-0.0741
 0.0158
-0.0950
 0.0059
-0.0242
-0.0698
-0.0716
 0.0109
-0.0993
-0.0566
 0.0352
-0.1506
-0.0336
-0.0541
 0.0496
-0.0379
 0.0151
-0.0221
-0.0764
-0.1936
-0.0832
 0.0188
-0.1457
-0.1108
-0.0356
 0.0400
-0.0344
-0.0064
-0.0181
-0.1594
-0.0453
-0.0648
-0.0577
 0.0238
-0.1951
 0.0037
-0.1513
-0.1395
-0.2038
-0.0270
 0.0025
-0.0549
 0.0384
-0.0715
 0.0045
 0.0457
 0.0081
 0.1031
-0.1186
-0.1136
-0.0874
-0.1261
-0.0817
-0.0885
 0.0021
 0.0392
-0.0006
-0.1246
-0.0126
 0.0625
 0.0506
 0.0227
-0.1458
-0.0244
 0.0672
-0.0835
-0.0883
 0.0328
-0.0068
-0.0136
-0.0554
-0.0456
 0.0122
-0.0857
-0.0610
-0.0383
 0.0421
-0.0210
-0.0536
-0.0220
-0.0297
-0.2137
 0.0223
 0.0072
-0.1544
 0.0321
-0.0044
-0.0250
-0.1639
 0.0554
 0.0437
-0.1635
-0.1010
-0.0816
 0.0015
-0.0801
-0.0196
-0.0050
-0.0872
-0.0368
-0.0873
-0.0860
-0.1243
-0.0721
 0.0537
-0.1031
-0.0923
-0.0626
 0.0741
-0.0548
 0.0046
 0.0133
-0.0288
-0.1309
-0.1371
-0.0488
 0.0379
-0.0694
-0.0103
-0.1105
-0.1374
-0.0515
-0.1601
-0.2197
 0.0267
-0.0323
-0.0206
-0.0657
-0.0023
-0.0854
-0.1016
-0.0997
-0.0510
-0.0173
 0.0164
 0.0079
 0.0340
-0.0432
 0.0223
-0.0087
-0.0024
-0.0800
 0.0012
-0.0952
-0.0832
-0.0422
 0.2109
-0.1598
-0.0298
-0.0813
-0.1952
-0.0942
-0.1275
-0.1324
-0.0414
-0.0839
-0.0977
-0.1292
-0.1481
-0.0240
-0.1079
-0.0476
-0.0450
-0.1788
-0.1621
-0.0613
-0.0916
-0.0517
[torch.FloatTensor of size 256]
), ('layer3.2.bn1.running_var', 
1.00000e-02 *
  2.1157
  0.8130
  1.8300
  2.7750
  1.8085
  0.9382
  1.9593
  2.3717
  1.6957
  1.3550
  1.5946
  1.3485
  2.1811
  1.2613
  3.3269
  1.7321
  2.1482
  1.4097
  1.5636
  1.3635
  1.0445
  1.5884
  1.7192
  1.1225
  1.9450
  1.6528
  2.0165
  1.5346
  2.3381
  1.8757
  1.0875
  1.6784
  1.5365
  1.7856
  2.3237
  1.7359
  2.4815
  2.1699
  1.2648
  2.0137
  3.0828
  3.5187
  2.2353
  1.1353
  1.8315
  2.5212
  1.8481
  0.8365
  1.7252
  1.4616
  1.8551
  1.5591
  0.9180
  3.3377
  1.9018
  2.6895
  0.9864
  1.7973
  2.0111
  1.6495
  2.4859
  2.2144
  1.3348
  2.5047
  2.6209
  2.2846
  1.7761
  1.9521
  1.7703
  1.6860
  2.5497
  2.0666
  2.4002
  1.4808
  1.7954
  1.0758
  1.5743
  2.1015
  2.6055
  2.1732
  1.9789
  2.5900
  4.6577
  1.5291
  1.1036
  3.0833
  1.4658
  2.7258
  2.3086
  2.7501
  2.4062
  1.6036
  2.3313
  1.6759
  1.0977
  1.0538
  1.1711
  1.3549
  1.8445
  1.7869
  1.4691
  1.8291
  1.5248
  1.8118
  2.7101
  2.2696
  2.6022
  1.5881
  1.4191
  2.2015
  2.0847
  1.7824
  1.9046
  1.3822
  1.7564
  1.2256
  1.8959
  1.6672
  3.2884
  1.9862
  2.4359
  2.2572
  1.5456
  2.2892
  1.9503
  1.5492
  1.8642
  1.7530
  1.0501
  1.3275
  1.3431
  2.4266
  2.5200
  1.5682
  1.2038
  1.6810
  1.2620
  1.6175
  1.0478
  2.5480
  1.6383
  1.6011
  1.6160
  1.3765
  2.4303
  1.2816
  1.1333
  1.5787
  1.2785
  2.3683
  2.2549
  2.0515
  1.2841
  1.5897
  2.2013
  1.7175
  2.0106
  1.5745
  1.6836
  1.4645
  2.2869
  1.3642
  0.8676
  1.2763
  0.9878
  2.3498
  1.9908
  1.8344
  2.1684
  2.1818
  2.0829
  1.4447
  1.5371
  2.9380
  2.3279
  1.6044
  1.3469
  1.4017
  1.5637
  1.6916
  1.4600
  1.1251
  2.0289
  1.6130
  2.7961
  1.5139
  2.3005
  1.5517
  0.9892
  1.1876
  1.7718
  1.5764
  1.6032
  1.5742
  1.3639
  1.5247
  1.1678
  1.2917
  4.9792
  1.7837
  0.8609
  1.2324
  1.5759
  1.3095
  1.2111
  2.2694
  1.2857
  2.0519
  1.0635
  1.5425
  2.8023
  1.3132
  0.9442
  1.3605
  1.3483
  1.0198
  1.9222
  1.3308
  1.0048
  1.9147
  2.2181
  1.3785
  2.2509
  1.9636
  3.0214
  2.3265
  2.2001
  1.9574
  1.9766
  1.4494
  2.1667
  1.3347
  2.0490
  1.2711
  2.0486
  2.3780
  1.4001
  2.1383
  1.7706
  2.2980
  1.4439
  1.8186
  1.6980
  1.8398
  2.0689
  2.7154
  1.4446
  1.9593
  2.2672
  1.6627
  1.5182
  1.8643
  3.3837
  1.6585
  1.5816
  1.3852
[torch.FloatTensor of size 256]
), ('layer3.2.conv2.weight', 
( 0 , 0 ,.,.) = 
 -3.5802e-03 -1.1028e-02  1.3011e-03
  1.1572e-02  2.0107e-02  1.2585e-02
  1.8766e-02  2.7120e-02  2.3125e-02

( 0 , 1 ,.,.) = 
  8.0946e-03  1.2307e-03 -5.1088e-03
  6.1331e-03  3.0108e-02  8.6952e-03
 -4.5609e-03  8.1927e-03 -4.2226e-03

( 0 , 2 ,.,.) = 
  1.9080e-02  1.8056e-02  9.1025e-03
 -2.0389e-02 -3.6315e-02 -1.7799e-02
  1.5768e-02  3.6663e-02  1.7026e-02
    ... 

( 0 ,253,.,.) = 
 -2.1304e-02 -1.9290e-02 -1.3309e-02
 -1.5200e-02 -9.1943e-03  9.8947e-03
  5.3930e-03  4.1778e-02  1.4718e-02

( 0 ,254,.,.) = 
  6.7867e-03  1.7099e-02  2.1589e-02
 -5.3290e-03 -1.9471e-03  1.4815e-02
 -8.7725e-03 -1.3762e-02  1.2047e-02

( 0 ,255,.,.) = 
  5.6504e-02  2.7372e-02 -4.0524e-02
  8.3591e-02  8.5810e-03 -7.8745e-02
  6.5760e-02 -1.1203e-03 -5.9587e-02
      ⋮  

( 1 , 0 ,.,.) = 
  1.6778e-03  2.1333e-03 -8.0604e-03
  3.3667e-03  3.3859e-02  2.6999e-03
 -4.1019e-03  1.7711e-03 -7.0186e-04

( 1 , 1 ,.,.) = 
 -3.6651e-03  5.4135e-03  1.2987e-02
 -2.2535e-02 -4.4939e-03 -9.9636e-03
 -1.3400e-02 -1.9440e-02 -1.1785e-02

( 1 , 2 ,.,.) = 
 -2.0187e-03  4.0392e-03  8.7115e-03
 -3.7967e-03 -1.9434e-03  8.2896e-03
  2.1840e-03  7.2859e-03  8.2192e-03
    ... 

( 1 ,253,.,.) = 
 -6.5260e-03 -1.0520e-02  5.6257e-05
 -8.5037e-03 -2.9023e-03 -2.1524e-03
 -1.8586e-02 -1.3367e-02 -1.1288e-02

( 1 ,254,.,.) = 
  1.4722e-02  1.2381e-02  1.4357e-02
  4.3434e-02  2.2864e-02  3.7622e-02
  2.6757e-02  2.9881e-02  1.3510e-02

( 1 ,255,.,.) = 
  1.3036e-02 -3.8851e-04 -8.0173e-03
  6.8872e-03  1.0882e-04 -1.6729e-02
  1.5520e-02 -8.3424e-05 -1.7248e-02
      ⋮  

( 2 , 0 ,.,.) = 
  9.3224e-03 -8.5108e-03 -8.3709e-03
  8.5115e-03 -1.0038e-02  8.2710e-03
  2.2448e-03 -8.4826e-04  1.6983e-02

( 2 , 1 ,.,.) = 
 -1.1610e-02 -1.7194e-02 -1.3838e-02
 -7.7607e-03 -6.2310e-03 -8.2235e-03
  2.7842e-03  2.8286e-03 -7.1709e-03

( 2 , 2 ,.,.) = 
 -1.3140e-03 -9.6142e-03 -1.5964e-03
  5.9721e-04 -3.8326e-03  2.8747e-03
 -1.6226e-02 -1.0415e-02 -4.6902e-03
    ... 

( 2 ,253,.,.) = 
  1.7541e-02  2.9383e-02 -1.8580e-03
 -3.8670e-03  6.0731e-03 -2.6307e-03
 -3.4498e-02 -1.6476e-02  1.0094e-02

( 2 ,254,.,.) = 
 -5.6360e-03 -5.3894e-03 -4.8574e-03
 -1.0288e-02 -3.4400e-03 -4.3404e-03
  1.7583e-03  6.7847e-04  1.0746e-02

( 2 ,255,.,.) = 
 -6.1896e-04 -2.7444e-03 -8.4192e-03
 -9.1858e-03  1.3946e-03  7.9272e-03
 -1.8049e-03  2.5581e-03  1.5893e-02
...     
      ⋮  

(253, 0 ,.,.) = 
  1.2330e-02 -1.7113e-02  4.6529e-03
  1.1574e-02 -2.4131e-02  1.3006e-02
  1.3733e-02 -5.7683e-03  8.1247e-03

(253, 1 ,.,.) = 
  2.5049e-03 -1.0780e-03 -3.9002e-03
  1.6765e-02  9.8542e-03  1.4043e-02
 -1.1201e-03 -6.0599e-03 -3.4802e-03

(253, 2 ,.,.) = 
  8.0176e-03 -1.0833e-02  2.9448e-03
  7.0284e-03 -1.6885e-02 -6.4530e-03
 -1.5313e-03 -1.2809e-02 -1.7555e-02
    ... 

(253,253,.,.) = 
  7.2650e-04 -3.4599e-02  3.6178e-03
 -1.1468e-03 -1.6426e-02  8.2671e-03
  2.9852e-03 -1.2011e-02  2.7804e-03

(253,254,.,.) = 
  1.3518e-02 -3.6790e-02 -2.8406e-03
 -3.6350e-03 -3.7368e-02 -9.5802e-03
  1.3845e-02 -2.0064e-02  1.7846e-03

(253,255,.,.) = 
 -2.9108e-03  3.5288e-03  1.1068e-02
 -9.1812e-03  2.1687e-04  1.5875e-02
  4.1262e-03 -1.6199e-04  6.8643e-04
      ⋮  

(254, 0 ,.,.) = 
  2.3126e-02  1.7599e-02  1.8742e-02
  2.9786e-02  1.1471e-02  7.7411e-03
  1.9394e-03  3.7610e-03 -1.0510e-02

(254, 1 ,.,.) = 
 -2.6477e-02 -1.8215e-02  1.5537e-02
 -3.9110e-02 -3.6499e-02  2.6256e-02
 -3.0353e-02 -4.6805e-02 -1.6427e-02

(254, 2 ,.,.) = 
 -7.5252e-03  2.2827e-03  3.6183e-03
 -1.2927e-02 -9.0444e-03  2.2144e-02
 -6.5457e-03 -1.3059e-02 -3.4611e-03
    ... 

(254,253,.,.) = 
 -1.4686e-02 -6.7829e-03 -5.8789e-03
 -1.1703e-02 -3.4396e-03  8.6091e-04
 -9.0684e-03 -4.9701e-03  2.1739e-02

(254,254,.,.) = 
  3.5613e-03 -5.1085e-03 -2.2686e-02
  5.7618e-03 -5.8313e-04 -1.1181e-02
 -7.6683e-03 -1.2790e-02 -8.8351e-03

(254,255,.,.) = 
 -8.7081e-03 -2.4242e-03  1.7277e-02
 -6.8912e-03 -8.9876e-03  7.6058e-03
 -4.3620e-03 -1.2196e-02 -1.0771e-02
      ⋮  

(255, 0 ,.,.) = 
 -1.1395e-02 -1.6485e-02 -8.1376e-03
 -1.0096e-04 -3.0751e-02  4.5764e-03
  7.2135e-03 -1.9183e-02 -8.7631e-04

(255, 1 ,.,.) = 
 -2.5444e-02 -1.2167e-02 -1.5170e-02
 -2.6833e-02 -1.3808e-02 -1.7042e-02
 -2.0713e-02 -1.7758e-02 -1.7370e-02

(255, 2 ,.,.) = 
 -1.6505e-02 -8.0067e-03  1.1924e-03
 -1.7738e-02 -3.1269e-03  1.2873e-02
 -2.0834e-02 -1.8683e-03  1.0609e-02
    ... 

(255,253,.,.) = 
  4.2366e-03  1.1488e-02 -7.5118e-03
 -1.2653e-02 -4.1932e-03 -1.6243e-02
 -1.8112e-02 -2.3787e-02  4.6444e-03

(255,254,.,.) = 
 -1.3521e-02  4.4411e-03 -1.8358e-02
 -3.1651e-02  6.4925e-03 -2.0924e-02
 -2.9287e-02 -6.6709e-04 -7.9085e-03

(255,255,.,.) = 
 -4.8704e-03 -2.1171e-03  4.6133e-03
 -6.9341e-03 -2.1932e-02  5.6086e-03
 -2.1563e-05 -4.4543e-03  2.2337e-02
[torch.FloatTensor of size 256x256x3x3]
), ('layer3.2.bn2.weight', 
 0.0969
 0.1814
 0.1938
 0.1642
 0.2110
 0.2123
 0.1423
 0.1863
 0.1374
 0.1904
 0.1703
 0.1753
 0.2232
 0.1309
 0.1904
 0.1800
 0.2098
 0.2059
 0.2137
 0.1609
 0.1975
 0.2097
 0.1341
 0.1360
 0.1813
 0.2388
 0.1784
 0.1816
 0.1364
 0.2038
 0.1830
 0.2292
 0.2131
 0.2544
 0.2114
 0.1966
 0.1961
 0.1324
 0.1402
 0.2292
 0.1947
 0.2015
 0.1747
 0.1883
 0.1947
 0.1927
 0.2368
 0.2083
 0.2113
 0.1313
 0.1306
 0.1675
 0.1935
 0.1699
 0.1958
 0.1847
 0.1984
 0.1885
 0.1834
 0.1234
 0.2094
 0.1791
 0.1495
 0.1383
 0.1268
 0.1357
 0.2262
 0.1566
 0.1690
 0.1925
 0.2146
 0.1678
 0.1544
 0.1702
 0.2020
 0.2086
 0.2057
 0.2180
 0.1834
 0.1976
 0.1290
 0.1304
 0.2164
 0.1939
 0.1878
 0.1262
 0.1837
 0.2008
 0.1922
 0.2035
 0.2134
 0.2244
 0.2234
 0.1850
 0.1372
 0.1804
 0.2006
 0.2246
 0.1644
 0.1731
 0.1765
 0.2073
 0.1688
 0.1802
 0.2063
 0.2126
 0.2001
 0.2032
 0.1740
 0.2060
 0.2358
 0.1634
 0.1929
 0.1728
 0.1168
 0.1076
 0.2326
 0.2094
 0.1260
 0.1913
 0.2373
 0.2136
 0.1605
 0.1765
 0.1873
 0.1220
 0.2000
 0.1367
 0.1592
 0.1939
 0.1984
 0.2398
 0.1705
 0.2116
 0.1427
 0.1169
 0.1843
 0.1933
 0.1586
 0.2246
 0.1880
 0.1705
 0.1947
 0.1136
 0.1762
 0.2038
 0.1890
 0.1363
 0.2078
 0.1812
 0.1731
 0.2160
 0.2013
 0.1872
 0.1884
 0.1926
 0.2031
 0.1792
 0.1819
 0.2186
 0.2463
 0.1658
 0.1785
 0.1689
 0.1977
 0.1764
 0.1882
 0.2066
 0.1976
 0.1820
 0.1959
 0.1923
 0.1890
 0.2039
 0.1843
 0.1626
 0.1936
 0.1497
 0.1687
 0.1552
 0.1930
 0.2002
 0.1945
 0.1496
 0.1902
 0.1932
 0.1964
 0.1960
 0.1302
 0.2041
 0.1938
 0.1172
 0.1964
 0.1644
 0.1392
 0.2242
 0.2161
 0.1988
 0.1831
 0.1781
 0.2014
 0.2313
 0.1659
 0.1729
 0.1504
 0.1237
 0.1461
 0.2231
 0.2200
 0.1365
 0.1267
 0.1903
 0.1304
 0.1314
 0.1488
 0.2102
 0.2209
 0.2202
 0.2071
 0.1849
 0.1693
 0.2050
 0.1968
 0.1972
 0.1638
 0.1653
 0.2203
 0.1555
 0.1691
 0.1945
 0.1849
 0.2019
 0.2369
 0.1930
 0.2014
 0.2247
 0.2038
 0.1653
 0.1384
 0.3183
 0.1976
 0.2010
 0.1879
 0.1703
 0.1880
 0.2127
 0.1852
 0.2193
 0.2023
 0.2166
 0.1547
 0.1343
 0.1245
 0.2063
 0.1767
 0.1484
[torch.FloatTensor of size 256]
), ('layer3.2.bn2.bias', 
 0.1261
-0.0592
-0.1530
-0.0470
-0.1184
-0.1236
 0.1090
-0.0293
-0.0089
-0.0829
-0.0502
-0.0755
-0.1208
-0.0099
-0.1175
-0.0523
-0.1513
-0.1094
-0.1369
-0.0541
-0.0950
-0.1112
 0.0632
 0.0442
-0.0875
-0.1306
-0.0721
-0.0688
 0.0597
-0.1114
-0.1094
-0.0986
-0.1523
-0.1476
-0.1077
-0.1678
-0.0850
-0.0130
 0.0036
-0.1019
-0.0578
-0.1377
-0.0667
-0.1334
-0.1261
-0.0940
-0.1323
-0.1400
-0.1320
 0.0218
-0.0023
-0.0984
-0.1066
-0.0318
-0.0892
-0.0049
-0.1687
-0.0458
-0.0667
 0.0597
-0.1081
-0.0626
 0.0059
 0.0809
 0.1068
 0.0169
-0.1402
-0.0211
-0.0945
-0.0861
-0.1509
-0.0913
-0.0681
-0.0479
-0.0934
-0.1006
-0.1083
-0.1893
-0.0356
-0.1046
 0.0581
 0.0141
-0.1715
-0.0722
-0.1160
 0.1053
-0.0325
-0.1663
-0.1044
-0.1551
-0.1544
-0.2016
-0.1557
-0.0711
 0.0726
-0.0614
-0.1188
-0.0827
-0.0001
-0.0473
-0.0773
-0.0592
-0.0785
-0.0534
-0.0847
-0.1179
-0.1452
-0.1172
-0.1300
-0.1376
-0.1530
-0.0843
-0.0920
-0.0855
 0.1771
 0.1697
-0.1238
-0.1649
 0.0508
-0.0626
-0.1231
-0.1618
-0.0673
-0.1061
-0.1146
 0.0651
-0.0486
 0.0356
-0.0439
-0.0775
-0.1106
-0.1380
-0.0860
-0.0879
 0.0390
 0.0845
-0.0404
-0.0728
 0.0040
-0.1621
-0.0895
-0.0345
-0.0582
 0.0985
-0.0677
-0.1493
-0.0908
 0.0902
-0.1058
-0.0888
-0.0291
-0.1475
-0.0818
-0.1098
-0.0837
-0.1302
-0.1154
-0.0978
-0.0429
-0.1099
-0.1344
-0.1137
-0.0440
-0.0506
-0.1452
-0.1070
-0.0797
-0.0914
-0.0804
-0.0723
-0.1296
-0.0922
-0.0959
-0.1298
-0.0512
-0.0195
-0.0682
 0.0165
-0.0518
-0.0142
-0.0713
-0.0857
-0.1434
-0.0327
-0.0978
-0.0656
-0.1323
-0.0737
 0.1082
-0.1328
-0.1087
 0.0707
-0.1221
-0.0268
-0.0098
-0.0983
-0.1573
-0.1404
-0.0860
-0.0738
-0.0679
-0.1446
-0.0943
-0.0618
 0.0091
 0.0540
 0.0458
-0.1048
-0.1252
 0.0623
 0.2188
-0.1045
 0.0056
 0.0210
 0.0219
-0.1138
-0.1230
-0.1054
-0.0924
-0.1264
-0.0353
-0.1399
-0.0961
-0.0533
-0.0229
-0.0244
-0.1230
 0.0004
-0.0940
-0.1104
-0.0959
-0.1058
-0.0490
-0.1199
-0.1409
-0.1094
-0.1123
-0.0217
 0.0100
-0.2202
-0.1588
-0.1203
-0.1066
-0.0357
-0.0422
-0.1826
-0.0714
-0.1066
-0.0994
-0.0816
-0.0014
 0.0262
 0.0881
-0.1068
-0.0539
-0.0139
[torch.FloatTensor of size 256]
), ('layer3.2.bn2.running_mean', 
 0.0280
-0.1631
-0.0610
-0.0621
-0.1243
-0.0841
-0.1397
-0.0544
-0.1210
-0.0354
 0.1244
-0.0525
-0.0803
-0.1058
-0.0559
-0.0579
-0.0691
 0.1341
-0.0642
-0.0482
-0.0725
 0.0027
-0.0821
-0.0949
-0.0998
-0.1803
-0.1119
-0.1888
-0.1409
-0.0822
-0.0422
-0.1019
-0.0921
-0.1883
-0.0699
 0.2106
-0.0600
-0.0537
-0.0306
-0.2361
-0.0442
-0.0050
-0.0738
-0.1505
-0.1754
-0.0260
-0.1705
-0.0500
-0.1772
-0.0172
-0.1295
 0.0630
-0.1136
-0.0651
-0.0849
-0.0774
-0.0358
-0.0529
-0.0501
-0.0746
-0.0565
-0.0655
 0.0254
-0.0409
-0.0350
-0.0319
-0.1313
-0.0026
-0.0694
-0.1143
-0.0857
-0.0951
-0.0351
-0.0016
-0.0823
-0.0217
-0.0394
-0.3301
 0.0518
 0.2626
 0.0286
-0.0180
-0.1216
-0.1075
-0.0650
-0.1403
 0.0075
-0.0562
-0.0948
-0.0787
-0.1036
-0.1057
-0.0203
-0.0552
 0.0913
-0.1322
-0.0464
-0.0479
-0.0065
 0.0404
-0.0121
-0.0673
-0.0515
-0.0785
-0.0234
 0.0448
-0.0928
-0.0181
 0.0006
-0.0909
-0.0100
-0.0294
-0.0819
-0.0010
-0.0888
 0.1035
-0.0563
-0.0898
-0.0416
-0.1086
-0.1353
-0.0739
-0.0456
-0.0076
-0.0117
-0.1661
-0.0881
 0.0440
-0.0498
-0.0984
-0.0581
-0.1592
-0.0948
-0.0767
 0.0627
-0.1239
-0.0268
-0.1769
 0.0206
-0.0042
-0.0865
-0.0172
-0.0993
 0.1221
-0.0679
-0.0313
-0.0146
-0.0859
-0.1099
-0.0433
-0.0322
-0.0777
-0.0565
-0.0811
-0.0810
-0.1173
-0.0689
-0.0515
 0.0434
-0.0173
-0.0816
-0.0797
-0.0438
-0.0587
-0.0234
-0.0520
-0.0556
-0.1281
-0.0469
-0.0010
-0.0551
-0.0604
-0.1159
-0.0981
-0.0533
 0.0100
-0.0522
 0.0339
-0.0018
-0.0489
-0.0360
-0.0262
-0.1155
-0.0386
-0.0798
-0.0882
-0.0523
-0.0447
 0.0812
-0.0791
 0.0443
 0.0497
-0.0952
-0.0505
-0.0611
-0.0664
-0.0385
-0.0729
-0.1064
-0.0801
-0.0140
-0.0623
-0.0427
-0.1056
-0.0849
-0.0640
 0.0191
-0.0426
-0.0831
-0.0424
 0.3430
-0.1057
-0.0153
-0.0887
-0.0756
-0.0256
-0.0413
-0.2017
-0.0777
-0.0476
-0.0108
-0.0789
-0.1795
-0.0283
-0.0446
-0.0964
-0.1495
-0.0939
 0.0029
 0.0143
-0.0538
-0.0285
-0.1425
-0.0721
-0.0279
-0.1219
-0.1224
-0.0514
-0.0708
-0.2411
-0.1236
-0.0667
-0.1032
 0.0440
-0.0572
-0.0755
-0.0447
-0.0765
-0.0992
-0.0115
-0.0288
-0.1226
-0.0767
-0.1137
 0.0171
-0.0544
[torch.FloatTensor of size 256]
), ('layer3.2.bn2.running_var', 
1.00000e-02 *
  0.9398
  1.1613
  1.5231
  0.7169
  2.1487
  1.4916
  2.0101
  0.9578
  0.9353
  1.8956
  1.3265
  1.1088
  1.8347
  0.8069
  1.1163
  1.7917
  1.0240
  1.3373
  2.0733
  0.9413
  2.0426
  1.7146
  1.1851
  1.4582
  1.2674
  2.8889
  1.1286
  1.1377
  1.5683
  2.1584
  1.0933
  4.2956
  1.8258
  4.2579
  1.6085
  1.6353
  2.3136
  0.9432
  1.3842
  4.3304
  1.7580
  1.2218
  0.8055
  1.1167
  0.8911
  1.4284
  3.5962
  1.0498
  1.6069
  1.1219
  0.8739
  0.7353
  1.0462
  1.0786
  1.0983
  2.4187
  1.3685
  1.2853
  1.4762
  1.0646
  2.0695
  0.8511
  1.1960
  1.4547
  1.7186
  0.9003
  1.7087
  1.1137
  1.1429
  1.6914
  0.9180
  1.0170
  0.8738
  1.1289
  2.0505
  1.9811
  1.3615
  1.1237
  2.0124
  1.8933
  1.6379
  0.7961
  1.9416
  1.0271
  1.1962
  2.0423
  1.3294
  0.8218
  1.1591
  1.5277
  1.5374
  1.0070
  2.2702
  1.7693
  2.5919
  0.9391
  0.9567
  1.5574
  1.7452
  1.2774
  1.8826
  1.8161
  0.8821
  1.7019
  1.5459
  1.4062
  1.7692
  1.8088
  0.8389
  1.5023
  2.2353
  1.0486
  1.3366
  0.8529
  1.2659
  1.9023
  2.9460
  2.0006
  1.1505
  1.2317
  2.8450
  1.0574
  0.9546
  0.9191
  0.8755
  1.0854
  1.5166
  1.1219
  1.2463
  1.2522
  1.7916
  2.7774
  0.8268
  1.4245
  1.3794
  1.1426
  1.6078
  1.6928
  1.5473
  2.0153
  1.2480
  1.4657
  1.7347
  1.0812
  0.9035
  1.5194
  1.2599
  1.7043
  1.3959
  1.1168
  2.6119
  1.5912
  2.4153
  1.0739
  1.0324
  0.9997
  2.0768
  1.1846
  2.2004
  2.0373
  2.5887
  1.4059
  1.4757
  1.2065
  0.8253
  0.8585
  1.3949
  3.6762
  1.8700
  1.2236
  1.2849
  1.6906
  1.0030
  1.1175
  1.4908
  1.6099
  1.5176
  1.2596
  1.2789
  1.6469
  1.4040
  1.7396
  1.1761
  0.8281
  1.3434
  1.1085
  0.8215
  1.3237
  2.0611
  1.5330
  0.9134
  1.1655
  1.3039
  0.8332
  0.9323
  3.8858
  1.6702
  1.0499
  0.8176
  1.3497
  1.9574
  2.0340
  1.2460
  1.1442
  1.2851
  1.2236
  1.4414
  2.0833
  2.2795
  1.2763
  1.6341
  1.1182
  1.3351
  1.1579
  1.2107
  1.5012
  1.4374
  1.9907
  1.5457
  1.0846
  1.2871
  1.8006
  1.3487
  2.0837
  1.5745
  0.9582
  1.5957
  0.9530
  0.8750
  2.0703
  1.2355
  2.4259
  4.5063
  1.9746
  1.3410
  2.3785
  2.0730
  1.0216
  1.3056
  5.7417
  1.9419
  0.8876
  1.3711
  1.0917
  1.5880
  1.7142
  1.2639
  2.2152
  1.6191
  2.1237
  1.4970
  1.0005
  1.4350
  1.3046
  1.8987
  1.6466
[torch.FloatTensor of size 256]
), ('layer3.2.conv3.weight', 
( 0  , 0  ,.,.) = 
 -2.3780e-02

( 0  , 1  ,.,.) = 
  8.7015e-03

( 0  , 2  ,.,.) = 
  1.8315e-02
      ... 

( 0  ,253 ,.,.) = 
 -4.2013e-02

( 0  ,254 ,.,.) = 
  3.4015e-03

( 0  ,255 ,.,.) = 
 -9.9530e-03
        ⋮  

( 1  , 0  ,.,.) = 
 -2.0450e-02

( 1  , 1  ,.,.) = 
 -1.2731e-02

( 1  , 2  ,.,.) = 
 -1.6573e-02
      ... 

( 1  ,253 ,.,.) = 
  1.6109e-02

( 1  ,254 ,.,.) = 
 -1.3517e-02

( 1  ,255 ,.,.) = 
 -1.6127e-02
        ⋮  

( 2  , 0  ,.,.) = 
  1.0084e-02

( 2  , 1  ,.,.) = 
  2.4977e-03

( 2  , 2  ,.,.) = 
 -1.5499e-02
      ... 

( 2  ,253 ,.,.) = 
 -1.1960e-02

( 2  ,254 ,.,.) = 
  1.1546e-02

( 2  ,255 ,.,.) = 
  2.3375e-02
 ...      
        ⋮  

(1021, 0  ,.,.) = 
 -2.9875e-02

(1021, 1  ,.,.) = 
  5.2212e-02

(1021, 2  ,.,.) = 
  5.5405e-03
      ... 

(1021,253 ,.,.) = 
 -1.4726e-02

(1021,254 ,.,.) = 
  5.1513e-03

(1021,255 ,.,.) = 
  3.5456e-02
        ⋮  

(1022, 0  ,.,.) = 
 -1.1258e-02

(1022, 1  ,.,.) = 
 -3.4418e-02

(1022, 2  ,.,.) = 
 -1.6843e-03
      ... 

(1022,253 ,.,.) = 
  4.3885e-04

(1022,254 ,.,.) = 
 -1.5460e-02

(1022,255 ,.,.) = 
  1.8400e-02
        ⋮  

(1023, 0  ,.,.) = 
  2.4465e-02

(1023, 1  ,.,.) = 
  1.4555e-05

(1023, 2  ,.,.) = 
 -9.1697e-03
      ... 

(1023,253 ,.,.) = 
  1.8262e-02

(1023,254 ,.,.) = 
 -4.5732e-03

(1023,255 ,.,.) = 
 -1.8989e-02
[torch.FloatTensor of size 1024x256x1x1]
), ('layer3.2.bn3.weight', 
 8.6847e-02
 8.2131e-02
 1.3081e-01
     ⋮     
 9.9649e-02
 5.0544e-02
 3.9904e-02
[torch.FloatTensor of size 1024]
), ('layer3.2.bn3.bias', 
 0.0686
-0.0592
-0.0134
   ⋮   
-0.0592
-0.0634
-0.0250
[torch.FloatTensor of size 1024]
), ('layer3.2.bn3.running_mean', 
 1.3994e-02
-1.5965e-02
-6.1856e-03
     ⋮     
 2.7276e-02
-3.7119e-02
 4.5246e-03
[torch.FloatTensor of size 1024]
), ('layer3.2.bn3.running_var', 
 2.0980e-03
 4.4469e-04
 1.0749e-03
     ⋮     
 8.8956e-04
 5.9883e-04
 4.1411e-04
[torch.FloatTensor of size 1024]
), ('layer3.3.conv1.weight', 
( 0  , 0  ,.,.) = 
 -1.5120e-02

( 0  , 1  ,.,.) = 
 -9.6008e-03

( 0  , 2  ,.,.) = 
 -1.6491e-04
      ... 

( 0  ,1021,.,.) = 
 -3.1888e-02

( 0  ,1022,.,.) = 
  2.5355e-02

( 0  ,1023,.,.) = 
 -2.2190e-02
        ⋮  

( 1  , 0  ,.,.) = 
  6.7242e-03

( 1  , 1  ,.,.) = 
  1.3888e-03

( 1  , 2  ,.,.) = 
  3.9784e-03
      ... 

( 1  ,1021,.,.) = 
  3.5906e-03

( 1  ,1022,.,.) = 
 -2.0859e-02

( 1  ,1023,.,.) = 
  7.3752e-03
        ⋮  

( 2  , 0  ,.,.) = 
  7.9607e-03

( 2  , 1  ,.,.) = 
  2.0202e-03

( 2  , 2  ,.,.) = 
  1.1938e-02
      ... 

( 2  ,1021,.,.) = 
 -1.4445e-02

( 2  ,1022,.,.) = 
 -1.2397e-02

( 2  ,1023,.,.) = 
 -8.0933e-03
 ...      
        ⋮  

(253 , 0  ,.,.) = 
 -1.4771e-02

(253 , 1  ,.,.) = 
 -4.0543e-03

(253 , 2  ,.,.) = 
  4.8153e-02
      ... 

(253 ,1021,.,.) = 
  3.5248e-02

(253 ,1022,.,.) = 
  4.6956e-03

(253 ,1023,.,.) = 
  2.8171e-02
        ⋮  

(254 , 0  ,.,.) = 
 -2.1148e-03

(254 , 1  ,.,.) = 
 -7.5839e-03

(254 , 2  ,.,.) = 
  2.1941e-02
      ... 

(254 ,1021,.,.) = 
 -7.1092e-03

(254 ,1022,.,.) = 
 -9.4924e-03

(254 ,1023,.,.) = 
 -1.0650e-02
        ⋮  

(255 , 0  ,.,.) = 
  9.0164e-03

(255 , 1  ,.,.) = 
  1.3322e-02

(255 , 2  ,.,.) = 
 -1.8748e-03
      ... 

(255 ,1021,.,.) = 
  1.1272e-02

(255 ,1022,.,.) = 
 -9.9430e-03

(255 ,1023,.,.) = 
 -6.4172e-04
[torch.FloatTensor of size 256x1024x1x1]
), ('layer3.3.bn1.weight', 
 0.2229
 0.1737
 0.1807
 0.1743
 0.2112
 0.2004
 0.1866
 0.1774
 0.1899
 0.1719
 0.1934
 0.1197
 0.1730
 0.2153
 0.1575
 0.1363
 0.1404
 0.1833
 0.1611
 0.1692
 0.1554
 0.1915
 0.1046
 0.1651
 0.1605
 0.1914
 0.2084
 0.1711
 0.1402
 0.1871
 0.1471
 0.1801
 0.1525
 0.1472
 0.1256
 0.1489
 0.1293
 0.1581
 0.0954
 0.1441
 0.1945
 0.1885
 0.1622
 0.1384
 0.2003
 0.1829
 0.2135
 0.1420
 0.1324
 0.1230
 0.2077
 0.1245
 0.1603
 0.1811
 0.1482
 0.1650
 0.1502
 0.1305
 0.1799
 0.1917
 0.1686
 0.1494
 0.1098
 0.1833
 0.1073
 0.1492
 0.1856
 0.1065
 0.2008
 0.1309
 0.1700
 0.1822
 0.1689
 0.1534
 0.1665
 0.1794
 0.1505
 0.1328
 0.1693
 0.1629
 0.1650
 0.1527
 0.1627
 0.1938
 0.1708
 0.1397
 0.1746
 0.1271
 0.2141
 0.1936
 0.1817
 0.1516
 0.1131
 0.1471
 0.1978
 0.1723
 0.1694
 0.1051
 0.1875
 0.1729
 0.1972
 0.1727
 0.1708
 0.1723
 0.1200
 0.2013
 0.1295
 0.1470
 0.1613
 0.1778
 0.2323
 0.1622
 0.1757
 0.1291
 0.2197
 0.1441
 0.1044
 0.1629
 0.1796
 0.1346
 0.1685
 0.1939
 0.1750
 0.1967
 0.1755
 0.1417
 0.1717
 0.1709
 0.1698
 0.1879
 0.1836
 0.1870
 0.1532
 0.1133
 0.1326
 0.1823
 0.1561
 0.1784
 0.1598
 0.1860
 0.1816
 0.1350
 0.1126
 0.1335
 0.1228
 0.1490
 0.2010
 0.1749
 0.1740
 0.1583
 0.1481
 0.1624
 0.2132
 0.1403
 0.1671
 0.1582
 0.1300
 0.1463
 0.1615
 0.2289
 0.2171
 0.1641
 0.0988
 0.1869
 0.1842
 0.1168
 0.1693
 0.2242
 0.1005
 0.1915
 0.1709
 0.1660
 0.1837
 0.2081
 0.1536
 0.1580
 0.1237
 0.1597
 0.1689
 0.1735
 0.1842
 0.1718
 0.1431
 0.1978
 0.1284
 0.1829
 0.1815
 0.2245
 0.1361
 0.1437
 0.1653
 0.1587
 0.1626
 0.1660
 0.1746
 0.1734
 0.1873
 0.1886
 0.1831
 0.1484
 0.1438
 0.1444
 0.1328
 0.1518
 0.1265
 0.1693
 0.1622
 0.1627
 0.2238
 0.1581
 0.1725
 0.2309
 0.1473
 0.1756
 0.2244
 0.1528
 0.1398
 0.1826
 0.1346
 0.2191
 0.2090
 0.1422
 0.1609
 0.1321
 0.1493
 0.1340
 0.1712
 0.1931
 0.1718
 0.1942
 0.1771
 0.1857
 0.2283
 0.1482
 0.1387
 0.1013
 0.1606
 0.1327
 0.1716
 0.1277
 0.1832
 0.1371
 0.1285
 0.1129
 0.1627
 0.1624
 0.2447
 0.1932
 0.1969
 0.1170
 0.1720
 0.1099
 0.1518
 0.1755
 0.1738
 0.1695
[torch.FloatTensor of size 256]
), ('layer3.3.bn1.bias', 
-0.1493
-0.1098
-0.1351
-0.1251
-0.1526
-0.1701
-0.1150
-0.1162
-0.0975
-0.0508
-0.1816
 0.0423
-0.0725
-0.1806
-0.0772
-0.0545
-0.0323
-0.1313
-0.0705
-0.0686
-0.0902
-0.1652
 0.0729
-0.1132
-0.1488
-0.1346
-0.1620
-0.1035
-0.0289
-0.1819
-0.0022
-0.1459
-0.0465
-0.0622
-0.0050
-0.0371
-0.0060
-0.0868
 0.0608
-0.0289
-0.1516
-0.1710
-0.0588
-0.0257
-0.1779
-0.1216
-0.1475
-0.0150
 0.0263
-0.0091
-0.0892
-0.0462
-0.0417
-0.1193
-0.0470
-0.0961
-0.0475
 0.0049
-0.1421
-0.1069
-0.1048
-0.0306
 0.0331
-0.1258
 0.0479
-0.0238
-0.1015
 0.0266
-0.1187
 0.0004
-0.0962
-0.1173
-0.0677
-0.0691
-0.0827
-0.1355
-0.0343
 0.0071
-0.0839
-0.1333
-0.1169
-0.0489
-0.0440
-0.1690
-0.0856
-0.0032
-0.0717
 0.0133
-0.0350
-0.1415
-0.1403
 0.0029
 0.0337
-0.0251
-0.1236
-0.1280
-0.0916
 0.0970
-0.1121
-0.0810
-0.1883
-0.1045
-0.0731
-0.0749
 0.0001
-0.1319
 0.0051
-0.0193
-0.1233
-0.0567
-0.1921
-0.0311
-0.0915
 0.0667
-0.2421
-0.0052
 0.0807
-0.1208
-0.1202
-0.0206
-0.1534
-0.1107
-0.1222
-0.1651
-0.1074
-0.0079
-0.0767
-0.0923
-0.0478
-0.0925
-0.0668
-0.1157
-0.0595
 0.0106
 0.0069
-0.1168
-0.0846
-0.1390
-0.0946
-0.0974
-0.0993
 0.0064
 0.0283
-0.0450
 0.0176
-0.0621
-0.1985
-0.0639
-0.0938
-0.0822
-0.0751
-0.0804
-0.2168
-0.0401
-0.0584
-0.0840
-0.0069
-0.0093
-0.1150
-0.2161
-0.2233
-0.0365
 0.0641
-0.1025
-0.1237
 0.0387
-0.0812
-0.2594
 0.0589
-0.1672
-0.1029
-0.0724
-0.1084
-0.1673
-0.0826
-0.0617
 0.0635
-0.0243
-0.1157
-0.1184
-0.1096
-0.1167
-0.0362
-0.2068
 0.0518
-0.0868
-0.1299
-0.1225
-0.0028
-0.0547
-0.1010
-0.0875
-0.0721
-0.0831
-0.1297
-0.1451
-0.0961
-0.1315
-0.1254
-0.0628
-0.0598
-0.0460
-0.0526
-0.0797
 0.0034
-0.1196
-0.0946
-0.0493
-0.1365
-0.0949
-0.0641
-0.2518
-0.0419
-0.1462
-0.1635
-0.0351
-0.0299
-0.1489
-0.0268
-0.1413
-0.2461
-0.0131
-0.0891
 0.0193
-0.0712
 0.0130
-0.1153
-0.1656
-0.1328
-0.1235
-0.1661
-0.1652
-0.1108
-0.0142
-0.0138
 0.0834
-0.1170
-0.0050
-0.1065
 0.0409
-0.1423
 0.0520
-0.0144
 0.0022
-0.0853
-0.0658
-0.2020
-0.0852
-0.1372
 0.1105
-0.1073
 0.0372
-0.1313
-0.0850
-0.1663
-0.0795
[torch.FloatTensor of size 256]
), ('layer3.3.bn1.running_mean', 
-0.2628
-0.0415
-0.1579
-0.0478
-0.0680
-0.0162
-0.0907
-0.0926
-0.0761
-0.1511
-0.1117
-0.1137
-0.0562
-0.0418
-0.0741
-0.1073
 0.0101
-0.0870
-0.0456
-0.0247
-0.0057
 0.0209
-0.1500
-0.0935
-0.1267
-0.0472
-0.1340
-0.0605
-0.0753
 0.0369
-0.1745
 0.0130
-0.0819
-0.0550
-0.1080
-0.0775
-0.0282
-0.0433
-0.0140
-0.0750
-0.0292
 0.0676
-0.0311
-0.1475
-0.0799
-0.0589
-0.1158
-0.0533
-0.1456
-0.0574
-0.0371
-0.0625
-0.0843
-0.0717
-0.0617
-0.0572
-0.0545
-0.1245
-0.0705
-0.1529
-0.0425
-0.0795
 0.0125
-0.0352
-0.1122
-0.0436
-0.2359
-0.0272
-0.2274
-0.0549
-0.0362
-0.0183
-0.0317
 0.0138
 0.1822
-0.0537
-0.0505
-0.0520
-0.0648
-0.0431
 0.0155
 0.0039
 0.0162
-0.0707
-0.0183
-0.1237
-0.1553
-0.1044
-0.2051
-0.1379
-0.1271
-0.1494
-0.1676
-0.1148
-0.1627
-0.0391
 0.0011
-0.0865
-0.1112
-0.1610
 0.0562
-0.0566
-0.0086
-0.0455
-0.0679
-0.1920
 0.0350
-0.1286
-0.0658
-0.1085
 0.0864
-0.0197
-0.1050
-0.0078
-0.1595
-0.1000
-0.0816
-0.0038
-0.0533
-0.0555
-0.0302
-0.0783
-0.0661
-0.0312
-0.0051
-0.0279
 0.1280
-0.0040
-0.0207
-0.1418
-0.0880
-0.0828
-0.1079
-0.1060
-0.1516
-0.0852
-0.0889
 0.0210
-0.1225
-0.1566
-0.1220
 0.0651
 0.0030
-0.0680
-0.0519
-0.0215
-0.0801
-0.0439
-0.0062
-0.0239
-0.0683
-0.0768
-0.0941
 0.0226
-0.0207
-0.0144
-0.0557
 0.0817
-0.0081
-0.0211
 0.0199
-0.0016
-0.1317
-0.0230
-0.0737
-0.1995
-0.0098
-0.0356
-0.0344
-0.0525
-0.0877
 0.0095
-0.0771
-0.0828
-0.0731
-0.0246
-0.0937
-0.1359
-0.0266
-0.1220
-0.0337
-0.0246
-0.0884
-0.0460
-0.1136
-0.0823
-0.0561
-0.0779
-0.0891
 0.0457
-0.1083
-0.0661
-0.1202
-0.0210
 0.0023
 0.0654
-0.1598
-0.0746
 0.0300
-0.0259
-0.0424
 0.0468
-0.0572
-0.0306
-0.0406
-0.1520
-0.0152
 0.0093
-0.1774
-0.0237
-0.1360
 0.0128
-0.0763
-0.0377
-0.0909
-0.0147
 0.0292
-0.0557
-0.0853
-0.0872
-0.1195
-0.0427
 0.0169
-0.0651
-0.1222
-0.1340
 0.0225
-0.0658
-0.0193
 0.0295
-0.0313
-0.0826
-0.0965
-0.1309
-0.0516
 0.0072
 0.0053
-0.0011
-0.0073
-0.1518
 0.0002
-0.1356
-0.1235
-0.0231
-0.0489
-0.0338
-0.0856
-0.0807
-0.1412
 0.0014
-0.0420
-0.0130
-0.0090
 0.0409
-0.0241
-0.0423
[torch.FloatTensor of size 256]
), ('layer3.3.bn1.running_var', 
1.00000e-02 *
  2.0729
  1.3258
  1.6309
  1.1356
  1.7239
  1.3362
  1.9816
  1.5125
  2.4985
  1.8246
  1.3521
  1.4685
  1.4938
  1.1915
  1.2404
  1.2807
  1.3186
  1.6596
  1.5701
  1.6455
  1.6953
  1.8203
  2.0550
  1.1361
  1.1147
  1.6545
  1.7201
  1.2449
  2.0241
  1.1698
  2.2457
  1.5047
  2.0053
  1.4105
  1.2854
  1.8773
  1.3781
  1.5693
  1.5523
  1.6369
  1.3659
  1.9161
  1.4745
  1.4428
  1.2891
  1.2111
  1.6803
  1.4026
  2.1006
  1.0905
  4.7454
  1.1056
  2.1716
  1.4658
  1.7569
  1.8273
  1.4974
  1.6618
  1.2878
  1.3268
  1.5848
  1.7172
  1.9927
  1.1736
  1.6290
  1.8228
  1.9110
  1.4547
  2.4020
  1.7071
  1.9665
  1.4297
  1.7866
  1.3729
  1.7467
  1.3329
  2.0094
  1.9561
  1.4426
  1.2980
  1.1175
  1.5468
  1.9634
  1.5752
  2.1393
  1.7533
  2.1134
  1.6591
  2.7459
  1.7622
  1.4593
  2.0526
  2.1168
  1.6951
  1.4533
  1.7164
  1.5768
  1.7404
  1.7753
  1.7857
  1.8178
  1.9777
  1.8347
  1.9371
  1.0409
  1.5105
  1.6814
  1.5484
  2.0877
  2.2474
  1.7753
  1.9641
  1.5134
  2.7863
  1.3530
  2.0125
  1.6247
  0.9421
  1.0903
  1.5176
  0.9675
  2.0091
  1.5271
  1.3624
  1.4496
  1.9673
  1.8505
  1.5986
  1.7636
  2.4310
  2.5651
  1.4175
  1.6650
  1.3545
  2.2838
  1.7781
  1.4633
  1.9901
  1.6739
  1.9068
  1.9097
  1.3148
  1.5541
  1.3136
  1.3661
  1.6282
  1.2702
  2.7073
  1.6140
  1.2884
  1.5728
  1.2833
  1.4661
  1.5271
  1.6715
  1.4541
  1.9058
  2.1527
  1.1548
  1.7349
  1.6188
  1.8898
  1.2059
  1.8637
  1.7425
  2.0665
  1.5620
  1.0753
  1.9075
  1.3589
  1.5342
  1.3921
  1.4014
  1.4289
  1.6201
  1.5655
  2.0320
  1.4281
  1.1803
  1.5463
  1.3602
  0.8881
  1.4630
  0.8629
  3.0105
  2.5161
  1.7047
  3.4926
  1.3206
  1.2860
  1.2032
  1.4813
  2.1881
  1.4585
  0.8780
  1.3181
  1.6852
  1.2316
  1.6577
  1.2918
  1.0174
  1.6346
  0.8906
  1.0795
  1.6538
  1.2949
  1.1640
  1.4284
  1.8222
  1.1970
  1.6199
  1.8215
  1.3714
  1.1049
  1.3316
  1.4748
  1.5981
  1.5642
  1.8505
  1.9906
  1.3085
  2.0629
  1.1688
  1.5511
  1.3690
  1.5039
  1.6132
  1.2937
  1.4766
  1.5943
  1.1264
  1.2394
  3.1600
  1.6275
  1.2874
  2.1233
  1.1927
  1.4283
  1.7039
  1.4465
  1.1140
  3.0053
  1.5250
  1.1644
  1.7369
  1.2591
  1.7819
  2.4489
  1.2556
  2.9304
  1.5087
  1.4517
  1.1106
  1.8061
  1.0041
  1.4778
[torch.FloatTensor of size 256]
), ('layer3.3.conv2.weight', 
( 0 , 0 ,.,.) = 
 -2.0021e-02 -1.8551e-02 -1.7702e-02
  3.1990e-03  2.4608e-02  8.0167e-03
  6.3028e-04  1.6046e-02  1.4803e-02

( 0 , 1 ,.,.) = 
 -9.5785e-03 -2.7887e-02 -2.9613e-02
 -3.1373e-02 -3.2521e-02 -3.0791e-02
 -1.4816e-02 -2.8355e-02 -1.3990e-02

( 0 , 2 ,.,.) = 
 -1.3157e-02 -2.2484e-02 -1.0739e-02
 -3.1500e-02 -2.3580e-02  6.3404e-03
 -2.5687e-02 -2.5521e-02  4.3534e-03
    ... 

( 0 ,253,.,.) = 
  1.2378e-02 -1.2427e-04  1.5254e-02
 -6.0275e-04 -6.7721e-03 -2.6011e-03
 -1.7559e-02  1.8965e-02 -4.6312e-03

( 0 ,254,.,.) = 
 -3.6175e-03 -4.2797e-03 -1.7449e-03
 -3.8641e-03  7.3817e-04 -1.3511e-02
  6.3750e-03  1.6524e-02  2.8192e-04

( 0 ,255,.,.) = 
 -3.1021e-03 -1.0760e-02 -1.0820e-02
  7.6638e-03  2.9162e-03 -5.7753e-03
  1.1449e-02  1.4833e-02  9.0975e-03
      ⋮  

( 1 , 0 ,.,.) = 
  1.5001e-02  2.4205e-02  1.2504e-02
  3.4874e-03 -1.1496e-03  3.4027e-03
  2.7936e-05  1.3407e-02  1.1299e-02

( 1 , 1 ,.,.) = 
 -1.4379e-03  7.5750e-03  2.6118e-03
  7.8706e-03  1.8343e-02  1.5806e-02
  1.9598e-02  1.7216e-02  1.5445e-02

( 1 , 2 ,.,.) = 
  1.8630e-03  1.0110e-02  6.0079e-03
 -1.5777e-03  1.1053e-02  1.2386e-02
  3.2049e-04  6.9257e-03  1.6020e-02
    ... 

( 1 ,253,.,.) = 
  3.1639e-02  5.4614e-02  2.8195e-02
  1.7428e-03 -5.5484e-03  3.2060e-03
  7.9264e-03  1.1677e-02 -7.1334e-05

( 1 ,254,.,.) = 
  2.5534e-03  2.4595e-02  5.9373e-03
 -1.7774e-02 -1.3784e-02 -2.2712e-02
 -1.6822e-02 -2.6017e-02 -2.7600e-02

( 1 ,255,.,.) = 
 -1.0540e-02  8.9218e-03 -1.3999e-02
 -9.6549e-03  2.3946e-02 -8.7016e-03
 -7.9950e-03  9.3906e-04 -9.1844e-03
      ⋮  

( 2 , 0 ,.,.) = 
  1.0220e-02  1.3448e-02  3.1288e-03
  1.5761e-02  1.8226e-02 -6.3891e-03
 -2.4425e-02 -1.9363e-02 -1.5872e-02

( 2 , 1 ,.,.) = 
 -3.9879e-03  8.0336e-03 -2.0417e-02
  8.3537e-04  2.8709e-02  6.7334e-03
  7.4572e-03  7.4434e-03  1.9225e-03

( 2 , 2 ,.,.) = 
  4.3888e-03 -1.2994e-02 -2.3043e-02
  2.7014e-02  2.6314e-02 -2.5255e-02
 -4.4141e-02  2.8281e-02  1.8219e-02
    ... 

( 2 ,253,.,.) = 
  2.1192e-02  3.0951e-02  1.8161e-02
  2.0311e-02 -5.5866e-03  8.4678e-03
  1.8127e-02  1.2721e-02 -1.8260e-02

( 2 ,254,.,.) = 
  5.3415e-04  1.0669e-02 -1.4406e-03
 -4.9864e-03 -3.8027e-03 -1.1200e-02
 -6.7161e-03 -1.5710e-03 -5.0357e-03

( 2 ,255,.,.) = 
  5.3110e-03 -2.5468e-02  4.4721e-03
  1.7959e-02  3.0402e-02  2.5463e-02
  5.8145e-03  2.1388e-02  6.7136e-03
...     
      ⋮  

(253, 0 ,.,.) = 
  4.9446e-03  1.7032e-02 -4.1625e-03
  3.4180e-03  1.2317e-02 -5.9132e-03
 -8.0229e-03  1.0697e-03 -4.7393e-03

(253, 1 ,.,.) = 
 -1.3029e-02  4.4715e-03  1.3844e-02
  1.9024e-02  6.9983e-03 -1.8130e-02
 -8.5130e-03 -1.0460e-02  5.6612e-03

(253, 2 ,.,.) = 
 -2.8004e-02  1.6113e-03 -1.1320e-02
  7.5326e-04 -1.1488e-03  2.6678e-03
  1.0549e-02  1.7583e-02  5.4505e-03
    ... 

(253,253,.,.) = 
  3.1530e-03 -2.0338e-03 -6.5455e-03
  1.5567e-03  9.2643e-03  1.6332e-03
  1.0746e-03  1.2560e-02  3.5943e-02

(253,254,.,.) = 
 -1.0374e-02 -7.9724e-03  1.0137e-02
 -1.2538e-03 -4.4716e-03  1.2623e-02
  7.9506e-03 -8.7204e-03 -4.3344e-03

(253,255,.,.) = 
  9.0244e-03 -2.2564e-02 -1.3702e-02
 -1.5199e-02  2.4168e-03 -7.6975e-04
 -1.2563e-03 -4.3446e-03  1.3148e-02
      ⋮  

(254, 0 ,.,.) = 
 -1.2275e-02 -1.2754e-02 -5.1172e-03
 -1.1370e-03 -9.2372e-03  8.6416e-04
  4.9617e-03 -9.3581e-03  3.2511e-03

(254, 1 ,.,.) = 
 -6.6198e-03 -6.9348e-03 -1.3156e-02
 -5.2331e-03 -7.3249e-03 -1.1559e-02
 -5.0624e-03  9.6311e-03 -7.8505e-03

(254, 2 ,.,.) = 
  4.0814e-02  7.9380e-03 -2.7049e-03
  2.2294e-02  9.4540e-03 -1.5283e-02
 -6.1956e-03 -1.7091e-02 -3.0600e-02
    ... 

(254,253,.,.) = 
  1.4390e-02  1.5792e-03  3.7335e-03
 -1.1748e-02 -1.1045e-02  1.5483e-03
  1.5680e-02  2.7082e-02  1.0973e-02

(254,254,.,.) = 
 -1.8610e-03 -1.1977e-02  4.8226e-03
 -1.4524e-03 -2.0644e-03  3.1556e-03
 -8.9026e-03  2.0469e-03 -2.4178e-03

(254,255,.,.) = 
 -1.4402e-02 -9.4209e-03 -1.2492e-02
 -1.1095e-02 -9.3371e-03 -1.4696e-02
  5.7585e-03  1.5352e-02 -3.0816e-03
      ⋮  

(255, 0 ,.,.) = 
  6.7708e-03 -2.6976e-02  6.9671e-04
  6.6295e-03 -2.3181e-02 -6.4667e-03
  3.4421e-03 -1.4330e-02  1.7412e-03

(255, 1 ,.,.) = 
 -2.3924e-02 -1.9910e-02 -1.2937e-02
 -2.9762e-03  6.6023e-03 -1.2093e-02
  8.2873e-03 -1.0582e-02  1.4183e-03

(255, 2 ,.,.) = 
  4.2226e-03  1.5073e-03  1.7971e-02
  6.0556e-03 -5.8182e-03  7.9520e-03
 -3.6708e-03 -2.5598e-02 -1.9481e-02
    ... 

(255,253,.,.) = 
 -5.5009e-03 -5.5731e-03 -4.2189e-03
  3.2219e-04 -1.4452e-02 -1.0267e-02
 -1.7291e-03  6.5764e-04  7.7291e-03

(255,254,.,.) = 
 -3.4515e-03 -7.9305e-03 -7.1792e-03
 -9.6562e-03  7.9661e-03  3.4565e-03
 -1.2274e-03  6.5865e-03  1.6233e-03

(255,255,.,.) = 
  8.3886e-03  2.5827e-03  1.8070e-04
 -6.6978e-03 -6.0422e-03  1.6910e-03
  3.1690e-03  6.6799e-03  8.1717e-03
[torch.FloatTensor of size 256x256x3x3]
), ('layer3.3.bn2.weight', 
 0.1172
 0.1486
 0.1118
 0.1961
 0.1318
 0.1768
 0.2088
 0.1235
 0.1939
 0.1717
 0.1465
 0.1779
 0.1724
 0.1284
 0.2134
 0.1629
 0.1602
 0.1782
 0.1857
 0.1772
 0.2084
 0.1832
 0.1883
 0.2201
 0.1983
 0.1766
 0.1609
 0.1723
 0.1201
 0.1766
 0.1504
 0.1930
 0.1928
 0.1722
 0.1899
 0.1772
 0.1429
 0.1551
 0.1959
 0.1659
 0.1552
 0.1888
 0.1171
 0.1534
 0.1623
 0.1706
 0.1560
 0.1285
 0.1875
 0.1632
 0.2061
 0.1069
 0.1679
 0.1918
 0.1611
 0.1705
 0.1820
 0.1939
 0.1656
 0.2021
 0.2128
 0.2180
 0.2011
 0.1673
 0.1869
 0.1146
 0.1936
 0.1755
 0.1847
 0.1840
 0.1738
 0.1549
 0.1633
 0.2015
 0.1985
 0.1701
 0.1513
 0.1041
 0.2055
 0.1405
 0.1966
 0.1671
 0.1687
 0.1042
 0.1770
 0.1891
 0.2306
 0.1723
 0.2230
 0.1693
 0.1772
 0.1330
 0.1588
 0.1652
 0.1205
 0.1041
 0.0999
 0.1309
 0.1637
 0.1794
 0.1612
 0.1924
 0.1571
 0.1692
 0.1796
 0.2162
 0.1776
 0.2294
 0.1652
 0.1751
 0.1924
 0.1629
 0.1791
 0.2089
 0.1569
 0.2452
 0.1906
 0.1794
 0.1796
 0.1883
 0.1424
 0.1731
 0.1424
 0.1372
 0.1542
 0.1715
 0.1811
 0.1651
 0.1873
 0.1902
 0.1952
 0.2106
 0.1683
 0.1800
 0.1559
 0.1818
 0.1734
 0.1359
 0.1813
 0.1515
 0.1618
 0.1718
 0.1377
 0.1904
 0.1427
 0.1301
 0.1870
 0.1454
 0.2006
 0.1744
 0.1534
 0.1650
 0.1948
 0.1972
 0.1850
 0.1945
 0.1683
 0.2064
 0.1805
 0.2091
 0.1008
 0.1721
 0.1616
 0.1782
 0.1447
 0.1548
 0.1390
 0.1507
 0.1814
 0.1880
 0.1669
 0.1784
 0.2107
 0.1724
 0.1471
 0.1792
 0.1121
 0.1802
 0.1687
 0.1774
 0.1681
 0.1896
 0.1893
 0.1795
 0.1421
 0.2329
 0.1789
 0.1458
 0.1941
 0.1956
 0.2023
 0.1319
 0.1597
 0.2023
 0.1692
 0.1192
 0.1909
 0.1831
 0.2032
 0.1739
 0.1729
 0.2098
 0.1490
 0.1687
 0.1818
 0.2041
 0.1640
 0.1716
 0.1760
 0.2110
 0.1896
 0.1573
 0.2008
 0.1776
 0.1976
 0.1955
 0.1647
 0.1438
 0.1871
 0.1431
 0.1685
 0.1582
 0.1737
 0.2530
 0.1759
 0.1787
 0.1516
 0.1858
 0.2028
 0.1262
 0.1840
 0.1693
 0.1607
 0.1612
 0.1993
 0.0968
 0.1830
 0.1167
 0.1818
 0.1432
 0.1611
 0.1969
 0.1837
 0.1743
 0.1907
 0.1227
 0.1540
 0.2193
 0.1719
 0.1709
 0.1867
 0.1634
 0.2592
 0.1809
 0.1939
 0.1892
[torch.FloatTensor of size 256]
), ('layer3.3.bn2.bias', 
 0.0455
 0.0114
 0.0543
-0.1035
-0.0039
-0.0757
-0.1422
 0.0248
-0.0766
-0.0710
-0.0464
-0.0480
-0.0542
 0.0290
-0.1238
-0.0988
-0.0508
-0.0901
-0.0405
-0.0784
-0.1533
-0.0721
-0.0873
-0.1499
-0.0973
-0.1233
-0.0495
-0.0790
 0.0652
-0.0384
-0.0393
-0.1157
-0.0972
-0.0794
-0.1821
-0.0861
-0.0169
-0.0537
-0.1305
-0.0633
-0.0254
-0.0855
 0.0479
-0.0556
-0.0613
-0.0671
-0.0475
 0.0369
-0.1246
-0.0789
-0.1112
 0.0760
-0.1156
-0.1098
-0.0638
-0.1069
-0.0435
-0.1172
-0.0613
-0.1543
-0.1708
-0.1784
-0.0690
-0.0399
-0.0932
 0.0419
-0.1154
-0.1171
-0.0923
-0.0639
-0.1087
-0.1061
-0.0857
-0.0949
-0.1137
-0.0477
-0.0715
 0.0761
-0.1130
 0.0511
-0.0953
-0.0851
-0.0670
 0.1117
-0.0071
-0.0763
-0.0939
-0.0684
-0.0762
-0.0408
-0.0951
 0.0644
-0.0695
-0.0269
 0.0201
 0.1716
 0.0915
 0.0008
-0.0494
-0.0853
-0.0514
-0.1249
-0.0699
-0.0653
-0.0570
-0.1052
-0.0793
-0.0728
-0.0819
-0.0286
-0.1101
-0.1126
-0.0954
-0.1218
-0.0236
-0.1533
-0.1140
-0.0763
-0.0401
-0.1015
-0.0260
-0.0764
-0.0094
 0.0063
-0.0123
-0.0924
-0.0917
-0.0843
-0.1459
-0.1110
-0.1098
-0.1222
-0.0820
-0.0868
-0.0236
-0.0188
-0.1086
-0.0003
-0.1303
-0.0408
-0.0333
-0.0577
 0.0165
-0.1304
-0.0085
 0.0016
-0.0682
 0.0156
-0.1162
-0.0478
-0.0403
-0.0831
-0.1351
-0.1109
-0.0897
-0.1121
-0.1022
-0.1640
-0.0827
-0.0951
 0.1404
-0.0773
-0.0689
-0.1049
-0.0135
-0.0545
-0.0377
-0.0545
-0.1116
-0.0349
-0.0096
-0.1127
-0.0501
-0.0897
-0.0169
-0.1017
 0.1593
-0.0742
-0.1001
-0.0708
-0.0835
-0.1478
-0.1208
-0.0753
 0.0439
-0.1303
-0.0392
-0.0390
-0.0879
-0.1050
-0.1420
 0.0123
-0.0536
-0.1472
-0.1299
 0.0310
-0.0642
-0.0952
-0.2083
-0.0673
-0.1081
-0.1043
-0.0352
-0.0746
-0.1174
-0.0754
-0.1023
-0.0971
-0.0601
-0.1341
-0.1156
-0.0317
-0.1022
-0.1406
-0.1641
-0.1076
-0.0631
-0.0371
-0.1357
-0.0344
-0.0920
-0.0624
-0.0232
-0.1844
-0.1186
-0.0680
-0.0322
-0.0491
-0.1180
 0.1064
-0.1022
-0.1092
-0.0403
-0.0583
-0.0546
 0.1480
-0.1122
 0.0864
-0.1407
 0.0219
-0.0551
-0.1320
-0.1212
-0.0852
-0.1310
 0.0837
 0.0098
-0.1358
-0.0812
-0.0854
-0.0734
-0.0591
-0.0997
-0.0921
-0.1200
-0.1237
[torch.FloatTensor of size 256]
), ('layer3.3.bn2.running_mean', 
-0.0425
-0.0583
 0.0084
-0.1422
-0.1029
-0.0564
 0.0414
 0.0064
-0.1389
-0.0500
-0.0403
-0.0310
 0.0258
-0.0427
-0.0424
-0.0494
-0.0681
-0.0233
 0.0009
-0.0425
-0.0885
-0.1190
-0.0812
-0.1627
-0.1439
-0.0337
-0.0142
-0.1240
-0.0555
-0.1004
-0.0887
-0.1090
-0.0261
 0.0205
 0.0094
 0.1656
 0.0334
 0.0335
-0.0653
-0.0521
-0.0889
-0.0859
 0.0809
 0.0016
-0.0318
 0.0011
-0.0844
 0.0034
-0.0320
-0.0526
-0.1295
 0.0541
-0.0248
-0.0338
-0.0962
-0.0530
-0.0750
-0.1071
-0.0595
-0.0784
-0.0713
-0.1040
-0.0735
 0.0308
-0.0625
 0.0319
-0.0329
-0.0981
-0.0753
-0.1224
-0.0923
-0.1480
-0.0446
-0.1491
-0.1194
 0.0500
-0.0010
-0.0179
-0.0561
-0.0388
-0.0800
-0.0818
 0.1211
 0.1735
 0.0378
-0.0645
-0.1615
-0.1233
-0.0906
-0.1202
-0.0073
 0.0309
 0.0309
-0.0223
-0.0705
 0.0144
-0.0812
-0.0919
-0.0006
-0.0637
-0.0681
-0.0659
-0.0913
-0.1253
-0.0818
-0.0546
-0.0680
-0.1015
-0.0670
-0.0141
-0.0042
-0.0839
-0.0599
-0.0056
-0.1479
-0.0521
-0.1482
 0.0382
-0.0533
 0.0695
-0.0533
-0.1100
-0.0355
-0.1098
 0.0129
-0.0417
-0.0844
 0.3767
-0.0206
-0.0403
-0.1033
-0.0337
-0.0448
-0.0764
-0.1458
 0.0095
-0.0246
-0.0031
-0.0570
-0.0230
-0.0660
-0.0116
-0.0957
 0.0126
 0.0573
-0.0928
 0.0142
-0.1229
-0.0528
-0.0934
-0.0161
-0.1206
-0.0671
-0.0419
-0.0799
-0.1231
-0.0349
-0.0970
-0.1199
-0.0711
 0.0487
 0.0493
-0.0731
-0.0541
-0.0509
-0.0284
-0.0684
-0.0697
-0.0315
 0.0492
-0.1064
-0.0546
-0.0696
-0.0912
-0.1397
-0.0837
-0.0675
-0.0248
 0.0026
-0.0812
-0.0215
-0.1071
-0.0801
-0.1322
-0.1198
-0.1125
-0.1066
-0.0409
-0.0455
-0.0344
 0.1693
-0.0678
-0.0878
-0.1113
-0.0580
-0.0059
 0.0123
-0.0454
-0.0908
-0.0785
-0.0493
-0.0866
 0.0135
-0.0468
-0.1571
-0.0705
-0.0626
-0.0528
-0.0989
 0.0683
-0.0161
-0.0083
-0.1197
-0.0165
-0.0902
-0.1177
-0.1003
-0.0301
-0.0934
-0.0626
-0.0964
 0.0148
-0.0928
-0.0375
-0.0920
-0.0439
-0.0571
 0.0068
-0.1297
 0.1034
-0.0405
-0.0412
-0.0920
-0.1518
-0.0699
 0.0469
-0.1260
-0.0689
-0.0683
-0.0710
-0.0673
-0.0225
-0.0895
-0.0067
-0.1238
 0.0437
-0.0575
-0.0266
-0.0564
 0.0076
-0.0181
-0.1408
-0.1029
-0.0785
-0.0489
-0.0795
[torch.FloatTensor of size 256]
), ('layer3.3.bn2.running_var', 
1.00000e-02 *
  0.6149
  1.0938
  0.7960
  1.1718
  0.7106
  0.7583
  1.6340
  1.0925
  1.4386
  0.8065
  0.7511
  0.9272
  0.8165
  0.9047
  0.9741
  0.8133
  0.6702
  1.0154
  1.3321
  1.0993
  0.8915
  1.0143
  1.4020
  0.9803
  0.9543
  0.8033
  0.7830
  1.2468
  0.8361
  1.2781
  1.0056
  0.8782
  0.9406
  0.6229
  1.0771
  1.0874
  1.4115
  0.8003
  0.7938
  0.7501
  0.9131
  1.0544
  1.3193
  0.7201
  0.7434
  0.7797
  0.7129
  1.1199
  0.8188
  0.6309
  0.8779
  1.0485
  0.6479
  1.0017
  0.6889
  0.6295
  1.1369
  0.7430
  0.8061
  0.7701
  0.8233
  0.8839
  1.0184
  0.7356
  1.0540
  1.5481
  1.0514
  0.9841
  0.7960
  1.1532
  0.6448
  0.5995
  0.5713
  1.0849
  1.0204
  0.9749
  0.6324
  0.7682
  0.8863
  1.1525
  1.1562
  0.7425
  1.3073
  0.9849
  1.2987
  0.8072
  1.3093
  0.8634
  3.0963
  0.9333
  0.8156
  1.0802
  0.7696
  0.9091
  0.9648
  1.1369
  0.7594
  0.7441
  1.1237
  1.0221
  0.7272
  0.7330
  0.6805
  0.5345
  0.8992
  2.2594
  1.0943
  4.5893
  0.7501
  1.4971
  1.0567
  0.5897
  1.2023
  1.2737
  0.7950
  1.5702
  0.9243
  0.8467
  1.0642
  1.1940
  0.6876
  0.9130
  0.8343
  0.7493
  0.7343
  0.8297
  0.8107
  2.0489
  0.6227
  0.9322
  0.6003
  1.1191
  0.9551
  1.0505
  0.8557
  1.8551
  0.6779
  0.7508
  0.8041
  1.0547
  0.8624
  0.7410
  0.8170
  0.6620
  1.4945
  0.8116
  1.0004
  1.0192
  1.0226
  0.8524
  1.1658
  0.6141
  0.7118
  0.8027
  1.2154
  1.1086
  0.6771
  0.8977
  0.9126
  2.3325
  1.6535
  1.1835
  0.6044
  0.8845
  0.6764
  0.9627
  0.9585
  0.7264
  0.8390
  1.7142
  1.1914
  0.7450
  3.2588
  0.7842
  0.9210
  0.8543
  0.8807
  1.2942
  1.1771
  0.8082
  0.7376
  0.7889
  0.9659
  1.0940
  1.2166
  1.4549
  1.0201
  0.6848
  0.9392
  0.5370
  1.4530
  0.9263
  1.3033
  1.1677
  0.5866
  1.3584
  1.1769
  0.8204
  0.8148
  0.8626
  0.9592
  1.0858
  0.7172
  0.6437
  1.0087
  1.7208
  0.5860
  0.7850
  1.0754
  1.7813
  0.9155
  0.8800
  1.0653
  0.5859
  0.6813
  0.7009
  1.1196
  0.7429
  0.7673
  0.6342
  0.7044
  0.8798
  0.7618
  1.5775
  0.7305
  0.9437
  0.7829
  1.6302
  1.1388
  1.0425
  0.8685
  0.6127
  1.1039
  0.9886
  1.1090
  0.7817
  1.0874
  0.9461
  0.6380
  0.9920
  0.7106
  1.0383
  0.8514
  0.7433
  0.9021
  1.5676
  0.8786
  1.8297
  1.1373
  0.7088
  0.9743
  0.8583
  3.6610
  0.9277
  0.8298
  0.7417
[torch.FloatTensor of size 256]
), ('layer3.3.conv3.weight', 
( 0  , 0  ,.,.) = 
  3.1927e-03

( 0  , 1  ,.,.) = 
  1.4805e-02

( 0  , 2  ,.,.) = 
 -3.9558e-02
      ... 

( 0  ,253 ,.,.) = 
 -1.3219e-03

( 0  ,254 ,.,.) = 
  1.5086e-02

( 0  ,255 ,.,.) = 
 -1.1912e-02
        ⋮  

( 1  , 0  ,.,.) = 
 -5.6582e-03

( 1  , 1  ,.,.) = 
  2.2918e-02

( 1  , 2  ,.,.) = 
 -2.6290e-02
      ... 

( 1  ,253 ,.,.) = 
  3.1419e-02

( 1  ,254 ,.,.) = 
 -2.5987e-02

( 1  ,255 ,.,.) = 
 -1.2404e-03
        ⋮  

( 2  , 0  ,.,.) = 
 -1.3780e-03

( 2  , 1  ,.,.) = 
 -3.3550e-03

( 2  , 2  ,.,.) = 
  1.2346e-02
      ... 

( 2  ,253 ,.,.) = 
  2.9028e-02

( 2  ,254 ,.,.) = 
  8.1570e-03

( 2  ,255 ,.,.) = 
  9.2157e-04
 ...      
        ⋮  

(1021, 0  ,.,.) = 
  9.3495e-03

(1021, 1  ,.,.) = 
 -3.9317e-02

(1021, 2  ,.,.) = 
 -1.5141e-03
      ... 

(1021,253 ,.,.) = 
  9.5364e-03

(1021,254 ,.,.) = 
 -2.4709e-02

(1021,255 ,.,.) = 
  3.2996e-03
        ⋮  

(1022, 0  ,.,.) = 
 -5.9284e-03

(1022, 1  ,.,.) = 
  1.8051e-02

(1022, 2  ,.,.) = 
 -4.4164e-02
      ... 

(1022,253 ,.,.) = 
 -7.3454e-03

(1022,254 ,.,.) = 
  7.7065e-03

(1022,255 ,.,.) = 
 -7.1259e-03
        ⋮  

(1023, 0  ,.,.) = 
 -7.3073e-03

(1023, 1  ,.,.) = 
 -1.1021e-02

(1023, 2  ,.,.) = 
 -1.2927e-02
      ... 

(1023,253 ,.,.) = 
  1.1101e-03

(1023,254 ,.,.) = 
 -6.2908e-03

(1023,255 ,.,.) = 
 -5.6577e-03
[torch.FloatTensor of size 1024x256x1x1]
), ('layer3.3.bn3.weight', 
 3.6002e-02
 1.3361e-01
 1.0711e-01
     ⋮     
 1.1089e-01
 6.0103e-02
 1.6406e-02
[torch.FloatTensor of size 1024]
), ('layer3.3.bn3.bias', 
 0.0043
-0.0521
-0.0891
   ⋮   
-0.0850
-0.0222
-0.0078
[torch.FloatTensor of size 1024]
), ('layer3.3.bn3.running_mean', 
1.00000e-02 *
 -2.2138
 -3.0617
 -3.8086
    ⋮   
 -1.8925
 -0.2076
  0.7458
[torch.FloatTensor of size 1024]
), ('layer3.3.bn3.running_var', 
 5.9709e-04
 1.1454e-03
 8.3702e-04
     ⋮     
 8.7320e-04
 6.6668e-04
 1.6457e-04
[torch.FloatTensor of size 1024]
), ('layer3.4.conv1.weight', 
( 0  , 0  ,.,.) = 
 -3.4127e-02

( 0  , 1  ,.,.) = 
  3.2188e-02

( 0  , 2  ,.,.) = 
 -2.0301e-02
      ... 

( 0  ,1021,.,.) = 
  4.1546e-03

( 0  ,1022,.,.) = 
  1.2702e-02

( 0  ,1023,.,.) = 
 -5.6641e-03
        ⋮  

( 1  , 0  ,.,.) = 
  3.2940e-03

( 1  , 1  ,.,.) = 
 -9.8673e-03

( 1  , 2  ,.,.) = 
 -8.8082e-03
      ... 

( 1  ,1021,.,.) = 
 -1.5857e-03

( 1  ,1022,.,.) = 
  7.6469e-03

( 1  ,1023,.,.) = 
 -5.9873e-03
        ⋮  

( 2  , 0  ,.,.) = 
 -2.4149e-02

( 2  , 1  ,.,.) = 
  1.2970e-02

( 2  , 2  ,.,.) = 
 -6.4782e-03
      ... 

( 2  ,1021,.,.) = 
  1.3755e-02

( 2  ,1022,.,.) = 
  1.3282e-02

( 2  ,1023,.,.) = 
  1.6999e-04
 ...      
        ⋮  

(253 , 0  ,.,.) = 
 -1.1186e-02

(253 , 1  ,.,.) = 
 -4.2158e-03

(253 , 2  ,.,.) = 
  2.9875e-02
      ... 

(253 ,1021,.,.) = 
  1.6177e-02

(253 ,1022,.,.) = 
  7.1540e-03

(253 ,1023,.,.) = 
  5.6421e-03
        ⋮  

(254 , 0  ,.,.) = 
  4.7531e-03

(254 , 1  ,.,.) = 
 -2.9031e-03

(254 , 2  ,.,.) = 
 -6.5355e-03
      ... 

(254 ,1021,.,.) = 
  2.8871e-02

(254 ,1022,.,.) = 
 -1.5105e-02

(254 ,1023,.,.) = 
 -1.3327e-03
        ⋮  

(255 , 0  ,.,.) = 
 -1.2044e-02

(255 , 1  ,.,.) = 
 -1.7834e-03

(255 , 2  ,.,.) = 
 -1.6628e-02
      ... 

(255 ,1021,.,.) = 
 -1.4216e-02

(255 ,1022,.,.) = 
 -1.5398e-02

(255 ,1023,.,.) = 
  1.6957e-03
[torch.FloatTensor of size 256x1024x1x1]
), ('layer3.4.bn1.weight', 
 0.1289
 0.1811
 0.1434
 0.1497
 0.1642
 0.2092
 0.1794
 0.1881
 0.1535
 0.1468
 0.1609
 0.1833
 0.1200
 0.1987
 0.1594
 0.1837
 0.2195
 0.1638
 0.1447
 0.2000
 0.1600
 0.1625
 0.1672
 0.1554
 0.1694
 0.1334
 0.1867
 0.1898
 0.1418
 0.1985
 0.2040
 0.1359
 0.2024
 0.1679
 0.1409
 0.1710
 0.1171
 0.1267
 0.1705
 0.1479
 0.1864
 0.1502
 0.2114
 0.2251
 0.1741
 0.1688
 0.1057
 0.1641
 0.1542
 0.1765
 0.1358
 0.1193
 0.1366
 0.1934
 0.1655
 0.1889
 0.1728
 0.1615
 0.1355
 0.1368
 0.1782
 0.1291
 0.1392
 0.1907
 0.1541
 0.1765
 0.1072
 0.1467
 0.1842
 0.1374
 0.1605
 0.1637
 0.1314
 0.1463
 0.2017
 0.1425
 0.1926
 0.1525
 0.1305
 0.1450
 0.1296
 0.1885
 0.1971
 0.1925
 0.1545
 0.1706
 0.2379
 0.2122
 0.1125
 0.1800
 0.1602
 0.2117
 0.1842
 0.1107
 0.1372
 0.2442
 0.1042
 0.1182
 0.0949
 0.1530
 0.2418
 0.1407
 0.1725
 0.1684
 0.1633
 0.1663
 0.1553
 0.1665
 0.1538
 0.1521
 0.1838
 0.1724
 0.1525
 0.1191
 0.2689
 0.2067
 0.1609
 0.2066
 0.1505
 0.1249
 0.1743
 0.1818
 0.1720
 0.1662
 0.1839
 0.1219
 0.1762
 0.1704
 0.2420
 0.1757
 0.1415
 0.1631
 0.1480
 0.1309
 0.1715
 0.2615
 0.2070
 0.1373
 0.1571
 0.1903
 0.2086
 0.1327
 0.1527
 0.1692
 0.1480
 0.1080
 0.2477
 0.1751
 0.1824
 0.1472
 0.1249
 0.1503
 0.1501
 0.1819
 0.1820
 0.1727
 0.1731
 0.2000
 0.1898
 0.1394
 0.1588
 0.2024
 0.1223
 0.1661
 0.1468
 0.1820
 0.1327
 0.1352
 0.1986
 0.1774
 0.1645
 0.1668
 0.1750
 0.1990
 0.1404
 0.1675
 0.1671
 0.1795
 0.2062
 0.2018
 0.1402
 0.1090
 0.2003
 0.1751
 0.2018
 0.1448
 0.2129
 0.1697
 0.1422
 0.1965
 0.2124
 0.1867
 0.1635
 0.1548
 0.1512
 0.1250
 0.1871
 0.1386
 0.1309
 0.1906
 0.1693
 0.2194
 0.1601
 0.1940
 0.1565
 0.1534
 0.1686
 0.1828
 0.1698
 0.1652
 0.1527
 0.1592
 0.2534
 0.1596
 0.1241
 0.1393
 0.1909
 0.1468
 0.1454
 0.1538
 0.1681
 0.2015
 0.1332
 0.1100
 0.1251
 0.1419
 0.1402
 0.1850
 0.1382
 0.1208
 0.1329
 0.1655
 0.2290
 0.2108
 0.1896
 0.2278
 0.1625
 0.1533
 0.1672
 0.1810
 0.1588
 0.1610
 0.1820
 0.1441
 0.1734
 0.1845
 0.1972
 0.1542
 0.1601
 0.1929
 0.1244
 0.1860
 0.1983
 0.1758
 0.1489
 0.2357
[torch.FloatTensor of size 256]
), ('layer3.4.bn1.bias', 
 0.0006
-0.0725
-0.0153
-0.0658
-0.0610
-0.2592
-0.1810
-0.1490
-0.0697
-0.0946
-0.1224
-0.1609
 0.0562
-0.0820
-0.1319
-0.1549
-0.1721
-0.0335
-0.0094
-0.1652
-0.1359
-0.1245
-0.0630
-0.0685
-0.1001
-0.0173
-0.1559
-0.1228
-0.0454
-0.1486
-0.1025
-0.0473
-0.1797
-0.1243
-0.0262
-0.0965
-0.0025
 0.0196
-0.0636
-0.0974
-0.2064
-0.0293
-0.1324
-0.1492
-0.1018
-0.1239
 0.0444
-0.0776
-0.1004
-0.1010
-0.0011
-0.0309
-0.0339
-0.1574
-0.0913
-0.1674
-0.0259
-0.0579
-0.0208
-0.0215
-0.1362
 0.0058
-0.0442
-0.1441
-0.0608
-0.0837
 0.0099
 0.0036
-0.1816
-0.0252
-0.1104
-0.0940
-0.0362
-0.0573
-0.1263
-0.0292
-0.1641
-0.0536
 0.0529
-0.0321
-0.0257
-0.0947
-0.1793
-0.1640
-0.0930
-0.1015
-0.1547
-0.1717
 0.0330
-0.1644
-0.0914
-0.1588
-0.1597
 0.0845
-0.0404
-0.2776
 0.0810
-0.0329
 0.1247
-0.1345
-0.3245
-0.0499
-0.1568
-0.1060
-0.0913
-0.0739
-0.0661
-0.1299
-0.0879
-0.0423
-0.1727
-0.1591
-0.0041
 0.0004
-0.3203
-0.0862
-0.0757
-0.1353
-0.0466
 0.0621
-0.1229
-0.1409
-0.1227
-0.1133
-0.0855
-0.0182
-0.1056
-0.0657
-0.1713
-0.1127
-0.0370
-0.0839
-0.0119
 0.0098
-0.1237
-0.2439
-0.0838
-0.0089
-0.0526
-0.0859
-0.1498
-0.0226
-0.0194
-0.1387
-0.0418
 0.0877
-0.3100
-0.1448
-0.1051
-0.0301
-0.0122
-0.0606
-0.0785
-0.1823
-0.0825
-0.1156
-0.1209
-0.2377
-0.1006
 0.0264
-0.0861
-0.1095
 0.0006
-0.0606
-0.0640
-0.1517
 0.0061
-0.0355
-0.1887
-0.0947
-0.1044
-0.1002
-0.0760
-0.1056
 0.0033
-0.0441
-0.0809
-0.0876
-0.1879
-0.1663
-0.0124
 0.0668
-0.1649
-0.1384
-0.2013
-0.0407
-0.1811
-0.0985
-0.0375
-0.0793
-0.1937
-0.1418
-0.1152
-0.0526
-0.0784
-0.0468
-0.1456
-0.0240
-0.0382
-0.1198
-0.1637
-0.2104
-0.1062
-0.2088
-0.1195
-0.0362
-0.1108
-0.1116
-0.1301
-0.0474
-0.0746
-0.0762
-0.2479
-0.0820
 0.0135
-0.0572
-0.1498
-0.0102
-0.0639
-0.0599
-0.1032
-0.1265
-0.0680
 0.0054
-0.0025
 0.0403
-0.0209
-0.1277
-0.0383
 0.0797
 0.0019
-0.1176
-0.1810
-0.2023
-0.1025
-0.1213
-0.0214
-0.0593
-0.1143
-0.1140
-0.1222
-0.0670
-0.1253
-0.0126
-0.0788
-0.1192
-0.1596
-0.0995
-0.0627
-0.0890
 0.0575
-0.1436
-0.1016
-0.1194
-0.0882
-0.1911
[torch.FloatTensor of size 256]
), ('layer3.4.bn1.running_mean', 
-0.0619
-0.0579
-0.1085
-0.0345
-0.1164
-0.0218
-0.1157
-0.1036
 0.0052
-0.0614
 0.0465
-0.0316
-0.1609
-0.1442
-0.0816
-0.0472
-0.1003
-0.1375
-0.0417
-0.1353
-0.0697
-0.0126
-0.0913
-0.1670
 0.0483
-0.0226
-0.0259
-0.0234
-0.0332
-0.0914
-0.1821
-0.0625
-0.0778
-0.0177
-0.0506
-0.0059
-0.0773
-0.0406
-0.0278
 0.0430
-0.0601
-0.0913
-0.1864
-0.1345
-0.0345
 0.1143
-0.0127
-0.0910
-0.1302
-0.1677
-0.0243
-0.0051
-0.0837
-0.0585
-0.0792
-0.0580
-0.0612
-0.0239
-0.0173
-0.0494
 0.0345
-0.1151
-0.1805
-0.0669
-0.1186
-0.0988
-0.1283
-0.0378
-0.0732
-0.0345
-0.0874
-0.0809
-0.0367
 0.0238
-0.0084
-0.0241
-0.0912
-0.0504
-0.1216
-0.0421
-0.0555
-0.0548
-0.0879
-0.0080
-0.0094
-0.0854
-0.0703
-0.1254
-0.0659
-0.0196
 0.0061
-0.0586
 0.0173
 0.0419
-0.0894
-0.1533
-0.2411
-0.0551
-0.2329
-0.0793
-0.1506
-0.0932
-0.0346
-0.0669
-0.0169
-0.0648
-0.0944
-0.0388
 0.0490
-0.1098
-0.0281
-0.0698
 0.0242
-0.1326
-0.1563
-0.1202
-0.1215
-0.0205
 0.0585
-0.0662
-0.0557
-0.1211
-0.0433
-0.0776
-0.0196
-0.1099
-0.0125
-0.0118
 0.0713
 0.0312
-0.0848
-0.0285
-0.0930
-0.1811
 0.0056
-0.1187
-0.1604
-0.0468
-0.0873
-0.1416
-0.0729
-0.0248
-0.1224
-0.0555
-0.0775
-0.0915
-0.2334
-0.0852
-0.0967
-0.0524
-0.0236
-0.1041
-0.0395
-0.0378
-0.0748
 0.0027
-0.0729
-0.1010
-0.0883
-0.0129
 0.0029
-0.0616
-0.0109
-0.1122
-0.0732
-0.1368
-0.1080
-0.1118
-0.0088
-0.1058
-0.0149
-0.0720
-0.0560
-0.0901
-0.0408
-0.1170
-0.1106
-0.0039
-0.1033
-0.1685
-0.1181
 0.0540
-0.0674
 0.0411
-0.0045
-0.0447
 0.0872
 0.0144
-0.1125
-0.0119
-0.1414
-0.0183
-0.0348
-0.0699
-0.0519
-0.0588
-0.0496
-0.1232
-0.0235
-0.1452
-0.0747
-0.0737
-0.0180
-0.0709
-0.0222
-0.0781
-0.1059
-0.1303
 0.0125
-0.1016
-0.0187
-0.0484
-0.2141
-0.0108
-0.1376
 0.0428
-0.1497
-0.1150
 0.0180
-0.0783
-0.0127
-0.0675
-0.0710
-0.0281
-0.1076
-0.0806
-0.1865
 0.0022
-0.0692
-0.0442
-0.1488
-0.0713
-0.1026
-0.1299
-0.1426
-0.1377
-0.1282
-0.0109
-0.1291
-0.0440
-0.0475
-0.0274
-0.0560
-0.0344
-0.2000
-0.1119
-0.1297
 0.0004
-0.0375
-0.1451
-0.1252
-0.0269
-0.0786
-0.0291
 0.0236
-0.1498
[torch.FloatTensor of size 256]
), ('layer3.4.bn1.running_var', 
1.00000e-02 *
  1.5587
  1.1115
  1.5589
  2.2311
  1.4606
  0.8779
  1.5434
  1.7033
  1.1646
  1.6613
  1.2817
  1.3302
  1.8995
  2.1639
  1.3179
  1.1834
  2.0312
  2.2118
  1.5174
  1.2525
  1.3896
  0.9094
  2.1986
  1.3775
  1.1663
  1.7446
  0.9916
  1.4552
  1.1305
  1.3882
  2.5805
  1.3457
  1.1829
  1.6872
  1.7551
  1.3038
  1.3390
  1.2120
  1.4453
  1.1517
  1.0283
  1.5275
  2.3340
  1.7629
  1.0325
  1.1993
  1.1993
  1.3243
  1.0306
  1.1225
  1.3618
  1.4653
  1.2787
  1.2239
  1.3028
  1.0970
  3.0271
  2.1823
  1.4954
  1.2061
  1.0387
  1.7470
  1.5878
  1.9118
  1.7989
  1.2018
  1.6930
  2.1152
  1.0652
  1.4462
  1.4096
  1.8183
  1.0961
  1.0107
  1.3979
  1.4812
  1.4134
  1.0631
  2.7615
  1.3991
  1.0958
  1.5329
  1.2538
  1.2314
  0.9472
  1.2176
  1.6669
  1.6059
  1.3171
  1.1535
  1.2807
  1.6171
  1.0319
  1.7333
  1.5981
  1.2286
  2.1789
  1.0806
  1.4983
  1.0912
  1.1372
  1.6345
  1.0728
  1.3680
  1.0944
  1.1630
  1.4084
  1.0682
  1.2445
  1.2404
  1.0408
  1.5747
  2.3322
  1.4960
  1.6955
  2.3934
  1.2353
  1.5893
  1.3193
  2.4019
  2.1792
  1.3983
  0.9410
  1.1903
  1.8383
  1.3092
  2.0579
  1.4227
  3.0920
  1.4435
  1.3000
  1.2691
  1.5366
  1.8206
  1.2747
  1.7071
  2.2023
  2.2724
  1.8910
  2.2712
  1.4057
  0.9825
  1.9937
  1.2259
  1.4630
  1.7999
  1.8332
  1.6809
  1.5684
  1.5401
  1.6141
  1.2590
  1.0974
  1.0172
  1.5440
  1.2785
  1.3521
  1.0928
  1.3119
  1.9691
  1.2130
  1.8148
  1.2847
  1.3018
  1.2516
  1.6315
  1.6747
  1.2273
  1.0089
  1.6040
  1.2840
  1.2080
  1.4500
  2.3440
  1.8006
  2.0413
  1.5258
  1.6523
  1.0907
  1.6582
  1.9745
  1.8425
  1.1240
  1.3287
  1.3029
  1.0889
  2.0176
  1.2442
  1.6380
  1.9694
  1.8828
  0.9354
  1.1470
  1.1791
  1.2426
  1.5789
  1.6674
  1.1502
  1.2689
  1.4390
  0.8324
  1.2543
  1.5888
  0.9997
  1.0031
  1.7512
  2.2960
  1.3398
  1.2791
  1.7120
  1.3203
  1.4970
  1.5508
  1.3350
  1.8164
  1.3089
  1.5492
  1.8388
  1.0032
  1.8777
  1.1348
  1.5433
  1.0337
  1.4537
  2.1910
  2.6022
  1.6638
  1.6215
  1.4031
  2.3692
  2.0369
  0.9046
  1.5460
  1.3385
  1.5492
  1.8634
  1.7231
  1.4114
  1.2535
  1.2235
  1.3880
  1.1901
  1.2200
  1.2788
  2.2337
  1.1771
  1.6846
  1.4075
  2.0849
  1.8286
  2.9878
  1.3816
  1.9738
  1.1194
  0.9204
  1.8417
[torch.FloatTensor of size 256]
), ('layer3.4.conv2.weight', 
( 0 , 0 ,.,.) = 
 -8.8975e-03  8.2681e-03  9.5468e-03
 -2.9289e-02 -8.9024e-03 -5.8257e-03
 -1.3357e-02 -2.4827e-02 -2.0100e-02

( 0 , 1 ,.,.) = 
 -2.0383e-05 -2.2157e-04  2.0560e-03
 -8.4056e-03 -2.0170e-02 -9.2057e-03
  6.3248e-03  2.9406e-03  1.7933e-03

( 0 , 2 ,.,.) = 
 -2.3525e-02 -1.0879e-02  1.3070e-02
 -5.3119e-04 -2.0994e-02 -8.2173e-03
  1.7326e-03 -1.0037e-02 -1.5591e-03
    ... 

( 0 ,253,.,.) = 
  3.6863e-03 -5.2400e-03 -4.0052e-03
 -5.1566e-03  1.3139e-02  1.6186e-02
 -1.3664e-02  6.3976e-03  1.2014e-04

( 0 ,254,.,.) = 
  3.8689e-03 -2.4797e-02 -4.8346e-03
 -1.1029e-03 -7.9848e-03 -1.4444e-02
  4.1494e-03  1.7180e-02  9.5370e-03

( 0 ,255,.,.) = 
 -1.2932e-04 -4.8040e-03 -5.6616e-03
 -1.0475e-02 -9.0452e-03 -1.4027e-02
 -4.2955e-03 -5.5446e-03 -5.4631e-03
      ⋮  

( 1 , 0 ,.,.) = 
 -2.7454e-02 -2.0153e-02 -1.8434e-02
  7.4258e-03  2.4594e-02  1.1151e-02
 -6.7909e-03  7.0744e-03  2.1112e-02

( 1 , 1 ,.,.) = 
 -4.6547e-03  8.3668e-03 -1.7892e-03
  6.8007e-03  1.5259e-02  9.2129e-03
 -9.3272e-03  2.7739e-02  2.1659e-02

( 1 , 2 ,.,.) = 
  9.6779e-03  1.1780e-02  3.2909e-02
 -1.2889e-02 -5.5071e-03  7.3504e-03
  8.8885e-03 -1.7894e-02 -2.8907e-02
    ... 

( 1 ,253,.,.) = 
  1.2088e-02  2.2231e-02  3.1916e-02
 -4.9173e-03  7.6595e-03 -5.7592e-04
 -7.4764e-03 -6.3305e-03  8.6860e-03

( 1 ,254,.,.) = 
  1.1143e-02  2.3442e-02  2.0811e-02
 -4.2744e-03 -8.3543e-03 -9.7011e-03
 -1.4505e-02 -1.7824e-02 -2.2806e-02

( 1 ,255,.,.) = 
 -1.3286e-02 -2.2064e-02 -1.9249e-02
 -3.6862e-03 -1.6956e-02  1.0500e-03
 -7.9056e-03 -1.2488e-02 -3.9062e-03
      ⋮  

( 2 , 0 ,.,.) = 
  1.5650e-02  5.5990e-03  5.8535e-03
  1.9149e-02  1.0672e-02  8.3179e-03
 -5.1511e-03 -2.1840e-02 -1.2597e-02

( 2 , 1 ,.,.) = 
 -1.1448e-02 -5.5772e-03 -4.4170e-03
  1.3190e-02  4.8222e-03  1.1920e-02
  2.1151e-02 -1.6081e-02 -1.6144e-02

( 2 , 2 ,.,.) = 
  1.3584e-02  8.9827e-03 -2.1343e-02
 -1.2230e-02  2.7788e-02  2.3000e-03
 -1.7698e-02 -2.2797e-03 -5.8428e-03
    ... 

( 2 ,253,.,.) = 
 -1.7119e-02 -1.3688e-02 -1.7270e-02
  2.0308e-03  4.9227e-03 -7.2063e-03
  1.2536e-02  4.1750e-03  5.4226e-03

( 2 ,254,.,.) = 
 -9.3082e-03 -7.7180e-03 -1.0361e-03
 -1.5341e-02  7.7777e-03 -1.3469e-02
 -8.7009e-03 -1.2160e-02 -1.3999e-02

( 2 ,255,.,.) = 
  3.0552e-02  1.3847e-02  1.0074e-02
  3.2691e-02  2.2141e-02  2.6602e-02
  2.1128e-02  1.8867e-02  2.3278e-02
...     
      ⋮  

(253, 0 ,.,.) = 
 -5.8947e-03  2.2211e-02  8.4547e-03
 -1.4809e-02  7.6867e-03  5.8644e-03
 -1.1171e-02  3.2800e-02  1.4729e-02

(253, 1 ,.,.) = 
  1.2799e-02  3.3404e-03 -8.0116e-03
  4.8239e-04  2.8425e-03 -2.3057e-03
 -6.2140e-03  3.7630e-03 -2.6030e-02

(253, 2 ,.,.) = 
 -7.2594e-03 -2.1482e-02 -2.5453e-02
  3.2283e-02  2.0096e-03 -7.4505e-03
  1.8465e-02  2.2960e-04 -8.8664e-04
    ... 

(253,253,.,.) = 
 -9.5123e-04  1.4586e-02  7.2230e-03
 -5.7989e-03  5.6743e-03  1.2585e-02
 -8.1923e-03 -3.2287e-02 -1.8241e-02

(253,254,.,.) = 
 -1.7783e-02 -6.6726e-03 -4.9502e-03
 -5.5236e-03  2.3151e-02  1.7009e-02
  3.1129e-03  4.9039e-03 -3.2202e-03

(253,255,.,.) = 
  6.8004e-03  3.6363e-03 -2.0004e-03
 -1.3302e-02  8.8023e-03  9.2807e-04
  4.0554e-03  8.5429e-04 -6.7752e-04
      ⋮  

(254, 0 ,.,.) = 
  2.5518e-03  1.4680e-02  2.6033e-03
 -1.0130e-02  9.3898e-03 -5.2180e-03
  5.4886e-03  2.5363e-03  1.3153e-03

(254, 1 ,.,.) = 
 -1.0852e-03 -1.4529e-02 -6.6705e-03
  2.0345e-03 -6.7807e-04 -4.9220e-03
  3.5024e-03  2.1662e-02  9.4759e-03

(254, 2 ,.,.) = 
  2.8206e-04  1.0020e-02 -2.4434e-02
 -1.1955e-02  1.1725e-02 -5.1801e-04
 -9.4785e-03  9.1012e-03 -3.6745e-03
    ... 

(254,253,.,.) = 
 -6.2103e-03  7.7493e-03 -1.5079e-02
  5.9956e-03  2.1847e-02  7.9742e-03
  8.1026e-03  1.8854e-02  2.2557e-02

(254,254,.,.) = 
 -1.2351e-02 -8.2933e-03 -8.4467e-03
  7.8022e-03 -4.6217e-03 -2.0065e-04
  1.4313e-03 -7.1164e-03 -4.9417e-03

(254,255,.,.) = 
 -1.5546e-02 -1.5347e-02 -2.1985e-02
 -4.6449e-03 -4.8014e-03 -6.0732e-03
  7.9861e-03  1.2879e-02 -6.0797e-03
      ⋮  

(255, 0 ,.,.) = 
 -3.3188e-02 -1.3365e-02  2.2784e-02
  1.1430e-02 -4.2369e-03  3.5036e-03
  3.2804e-02 -1.2503e-02 -1.4610e-02

(255, 1 ,.,.) = 
 -2.0241e-02 -9.5616e-03 -1.7951e-03
 -5.1452e-04  2.3131e-03 -4.7360e-03
  4.2102e-03  3.8810e-03 -3.0991e-03

(255, 2 ,.,.) = 
 -2.3415e-02 -1.2256e-02 -1.6172e-04
 -3.4541e-02 -1.5332e-02  5.2646e-03
 -1.0936e-02  8.1166e-03  1.2481e-02
    ... 

(255,253,.,.) = 
  9.7134e-03  3.2788e-03  1.1143e-02
 -8.3905e-03 -1.3490e-02  9.7034e-03
  9.1477e-03  1.8615e-03  2.3738e-02

(255,254,.,.) = 
 -6.7199e-03  5.7582e-03 -1.5496e-03
 -9.9920e-03  1.1892e-02  6.0360e-03
 -1.0405e-02  2.1521e-03 -1.0671e-02

(255,255,.,.) = 
  7.0692e-03 -7.7802e-03 -1.0909e-02
  1.5003e-02 -1.6051e-02 -2.7195e-03
  2.0354e-02  7.5280e-03  5.8080e-03
[torch.FloatTensor of size 256x256x3x3]
), ('layer3.4.bn2.weight', 
 0.2008
 0.1521
 0.1259
 0.1868
 0.1921
 0.2461
 0.1508
 0.2064
 0.1900
 0.1617
 0.2026
 0.2085
 0.2002
 0.1725
 0.1931
 0.2046
 0.1522
 0.1449
 0.1675
 0.1770
 0.1903
 0.1664
 0.1221
 0.1943
 0.1635
 0.1830
 0.1265
 0.1798
 0.2328
 0.1720
 0.1510
 0.1793
 0.2229
 0.1740
 0.1833
 0.2031
 0.1971
 0.1704
 0.1564
 0.1749
 0.2048
 0.1699
 0.1695
 0.1954
 0.1749
 0.1589
 0.2077
 0.1882
 0.1825
 0.1804
 0.1969
 0.1586
 0.1490
 0.2586
 0.2114
 0.1672
 0.1810
 0.1974
 0.1204
 0.1673
 0.1668
 0.0994
 0.1337
 0.2054
 0.1776
 0.1594
 0.1904
 0.2033
 0.1793
 0.1668
 0.1542
 0.2019
 0.1837
 0.1943
 0.1338
 0.1116
 0.1929
 0.1416
 0.1832
 0.1873
 0.1796
 0.2255
 0.2024
 0.1473
 0.1653
 0.1743
 0.1756
 0.1637
 0.1556
 0.1973
 0.1373
 0.2583
 0.1816
 0.1601
 0.1976
 0.2355
 0.1635
 0.1721
 0.1581
 0.1209
 0.1857
 0.1934
 0.1604
 0.1342
 0.1788
 0.1542
 0.1111
 0.2232
 0.2222
 0.1729
 0.2066
 0.2073
 0.1874
 0.1871
 0.1798
 0.1855
 0.1945
 0.1648
 0.1276
 0.2362
 0.1991
 0.1864
 0.1613
 0.1551
 0.1677
 0.2030
 0.1779
 0.2047
 0.2305
 0.1639
 0.1974
 0.1774
 0.1617
 0.2104
 0.1987
 0.1616
 0.1128
 0.1850
 0.1840
 0.1340
 0.1449
 0.1235
 0.1881
 0.1881
 0.2245
 0.2144
 0.1606
 0.1581
 0.1136
 0.2017
 0.1562
 0.1934
 0.1712
 0.1811
 0.2129
 0.1608
 0.1805
 0.1978
 0.1429
 0.2109
 0.1914
 0.2316
 0.1665
 0.1306
 0.1375
 0.2008
 0.1785
 0.2253
 0.1964
 0.1796
 0.1488
 0.1971
 0.1903
 0.1901
 0.1814
 0.2048
 0.1558
 0.1998
 0.1498
 0.1917
 0.2003
 0.1697
 0.2062
 0.1638
 0.1716
 0.1496
 0.2007
 0.1685
 0.2033
 0.1396
 0.2072
 0.2134
 0.1713
 0.1970
 0.1853
 0.1743
 0.1968
 0.1930
 0.1577
 0.1861
 0.1883
 0.1416
 0.2113
 0.1806
 0.1282
 0.1883
 0.1662
 0.1733
 0.1812
 0.1679
 0.1543
 0.2277
 0.2178
 0.1982
 0.1462
 0.1987
 0.1771
 0.1761
 0.1810
 0.1655
 0.1544
 0.1721
 0.2057
 0.1552
 0.1237
 0.1702
 0.1612
 0.1703
 0.1313
 0.1579
 0.1547
 0.2015
 0.1264
 0.1486
 0.1426
 0.1809
 0.1765
 0.1920
 0.1950
 0.1873
 0.1197
 0.1519
 0.2036
 0.1775
 0.1829
 0.1793
 0.1870
 0.2008
 0.1836
 0.1925
 0.1903
 0.1819
 0.2041
 0.1113
 0.1589
 0.2006
[torch.FloatTensor of size 256]
), ('layer3.4.bn2.bias', 
-0.1534
 0.0185
 0.0063
-0.1076
-0.1880
-0.1379
-0.0456
-0.1717
-0.1251
 0.0141
-0.1812
-0.1499
-0.1637
-0.0188
-0.1679
-0.1444
-0.0716
-0.0381
-0.0721
-0.1751
-0.1522
-0.0850
 0.0063
-0.0695
-0.0497
-0.1304
 0.0265
-0.0965
-0.1659
-0.0703
-0.0357
-0.0827
-0.1077
-0.1052
-0.0943
-0.1322
-0.0909
-0.0434
-0.0348
-0.0832
-0.0884
-0.0801
-0.0949
-0.1403
-0.0660
-0.0954
-0.1571
-0.1181
-0.1045
-0.0982
-0.1383
-0.0774
-0.0425
-0.3299
-0.1648
-0.0514
-0.0858
-0.1613
 0.0429
-0.0169
-0.0669
 0.1415
-0.0169
-0.1406
-0.1162
-0.0315
-0.0810
-0.1463
-0.1556
-0.0816
-0.0693
-0.1055
-0.1247
-0.1693
-0.0105
 0.2091
-0.1337
 0.0016
-0.1023
-0.1028
-0.0606
-0.1522
-0.1760
 0.0498
-0.0562
-0.1344
-0.1116
-0.0575
-0.0552
-0.1528
-0.0358
-0.1000
-0.1304
-0.0047
-0.1353
-0.1303
-0.0915
-0.0978
-0.0104
 0.0226
-0.0793
-0.1432
-0.0402
-0.0381
-0.1285
-0.0798
 0.0532
-0.0730
-0.2105
-0.0254
-0.1120
-0.1362
-0.1390
-0.0606
-0.1243
-0.1169
-0.1203
-0.1249
 0.0253
-0.1063
-0.0950
-0.0806
-0.0616
-0.0574
-0.0497
-0.1602
-0.1168
-0.1691
-0.1608
-0.0488
-0.1329
-0.1264
-0.0928
-0.0536
-0.1786
-0.0642
 0.0652
-0.0765
-0.0856
-0.0073
-0.0301
 0.0740
-0.0872
-0.1576
-0.1412
-0.1435
-0.0439
-0.0509
 0.0339
-0.0990
-0.1011
-0.1460
-0.0592
-0.1022
-0.1250
-0.0839
-0.1574
-0.1794
-0.0208
-0.0967
-0.0815
-0.1702
-0.0819
 0.0412
-0.0051
-0.2341
-0.1041
-0.1077
-0.1686
-0.1207
-0.0321
-0.1361
-0.1332
-0.1180
-0.1132
-0.1542
-0.0331
-0.1595
-0.0496
-0.1201
-0.1465
-0.0696
-0.1338
-0.0479
-0.0996
-0.0330
-0.0948
-0.0846
-0.1685
 0.0679
-0.0419
-0.0863
-0.0898
-0.1544
-0.1005
-0.0553
-0.1426
-0.1411
-0.0633
-0.1525
-0.0995
 0.0178
-0.1673
-0.0620
 0.0531
-0.1369
-0.0632
-0.1179
-0.1617
-0.1062
-0.0803
-0.1628
-0.1020
-0.1734
-0.0217
-0.1389
-0.1538
-0.0584
-0.1068
-0.0700
-0.0484
-0.1114
-0.1421
-0.0315
 0.0264
-0.0764
-0.0019
-0.0895
 0.0459
-0.1228
-0.0460
-0.1303
 0.0304
-0.0031
-0.0399
-0.1261
-0.0755
-0.0842
-0.1189
-0.1167
 0.0110
-0.0680
-0.1387
-0.1016
-0.1808
-0.1114
-0.1655
-0.1242
-0.0829
-0.1266
-0.1015
-0.1227
-0.1013
 0.0410
-0.0963
-0.1434
[torch.FloatTensor of size 256]
), ('layer3.4.bn2.running_mean', 
-0.1062
 0.0821
-0.0961
-0.0871
-0.0776
-0.1187
-0.0408
-0.0681
-0.0693
 0.0426
-0.1137
-0.0776
-0.0682
-0.1038
-0.0584
-0.0872
-0.0636
-0.0358
-0.0463
-0.0244
-0.0408
-0.0767
-0.1122
-0.1549
-0.0711
-0.0662
 0.0545
-0.0552
 0.0388
-0.0383
-0.0534
-0.0724
-0.1162
-0.0765
-0.0634
-0.0792
-0.0724
 0.0299
-0.0657
-0.0512
-0.1099
-0.0643
 0.0042
-0.0577
-0.0429
-0.1392
-0.0516
-0.1126
-0.1069
-0.0554
-0.0214
-0.0965
 0.0392
 0.0178
-0.0450
-0.0627
-0.0716
-0.1362
-0.0536
-0.1255
-0.0139
-0.0545
-0.1371
-0.0702
-0.0957
-0.0742
-0.0817
-0.0216
-0.0674
-0.0930
-0.0307
-0.0837
-0.1026
-0.1163
-0.1128
 0.1119
-0.0312
 0.0584
-0.1059
-0.0691
-0.1265
-0.1262
-0.0681
 0.0700
 0.1118
-0.1072
-0.1259
-0.1073
-0.0458
-0.0551
-0.0077
-0.0883
-0.0164
-0.1678
-0.0939
-0.0175
 0.1141
-0.1120
 0.0238
 0.0537
-0.1300
-0.0682
-0.0226
-0.0776
-0.0601
-0.0174
 0.0799
-0.1607
-0.1194
 0.0590
-0.1460
-0.0476
-0.1192
-0.0801
-0.1015
-0.0684
-0.0406
-0.0744
-0.0538
-0.1830
-0.1477
-0.0585
-0.0892
-0.0149
-0.0462
-0.1070
 0.2887
-0.1220
-0.1162
-0.0645
-0.0532
-0.0693
-0.1495
-0.0989
-0.0458
-0.0548
 0.0701
-0.0314
 0.0072
-0.0168
-0.0468
 0.0317
-0.1055
-0.0999
 0.1720
-0.0740
-0.0555
 0.0314
 0.0033
 0.0437
 0.0842
-0.1129
-0.1114
-0.1133
-0.1288
-0.1200
-0.0381
-0.0983
-0.0830
-0.0484
-0.0985
-0.0938
-0.1269
-0.0077
-0.0203
-0.0203
-0.1277
-0.1024
-0.1115
 0.0762
-0.0013
-0.0904
-0.2145
-0.1202
-0.0983
-0.0722
-0.0138
-0.0661
-0.0617
-0.0326
-0.1232
-0.1099
-0.1070
-0.1247
-0.0583
-0.0994
-0.0411
-0.0272
-0.0781
 0.0244
-0.0884
-0.1168
-0.0265
-0.0825
-0.0991
-0.0665
-0.1081
-0.1534
-0.0735
-0.1369
 0.0208
-0.1076
-0.1228
-0.0997
-0.0841
-0.0718
-0.0067
-0.0285
-0.1128
-0.0755
 0.0422
-0.0706
 0.0768
-0.1308
-0.0445
-0.0796
-0.1320
-0.0804
-0.0930
-0.1086
-0.0565
-0.0958
-0.0765
-0.1126
-0.0288
-0.0822
-0.0242
-0.1200
-0.0721
-0.0901
-0.0997
-0.0478
 0.0630
-0.0620
-0.0546
-0.0661
-0.0557
-0.0677
-0.1287
-0.0809
-0.0712
-0.0195
 0.0864
-0.1244
 0.1621
-0.1095
-0.1330
-0.0836
-0.0834
-0.1221
-0.1626
-0.0538
 0.0435
-0.0048
-0.0422
-0.0617
[torch.FloatTensor of size 256]
), ('layer3.4.bn2.running_var', 
1.00000e-02 *
  0.7093
  1.4399
  1.0110
  0.6659
  0.6931
  1.2161
  0.7871
  0.5581
  0.8782
  1.8943
  0.8105
  1.1080
  0.8002
  0.8760
  0.6081
  1.1145
  0.7185
  0.6550
  1.0409
  1.1818
  0.7993
  0.5614
  0.7604
  0.9539
  0.8024
  0.9727
  0.6919
  0.7600
  0.9215
  0.7517
  0.6323
  1.4999
  1.2999
  0.6198
  0.6651
  1.0666
  1.3212
  0.8284
  0.5883
  0.5819
  1.3450
  0.7074
  0.5554
  0.7821
  0.6610
  0.8246
  0.7147
  0.6866
  1.3135
  0.6925
  0.8955
  0.7722
  0.6625
  2.6739
  0.6793
  0.7496
  0.5960
  0.7681
  0.9143
  1.0875
  0.7138
  0.9810
  0.8069
  0.7263
  0.6332
  0.5430
  1.5741
  1.5772
  0.7125
  1.0571
  0.6135
  0.9407
  0.8230
  0.9584
  1.1768
  1.3270
  0.6641
  1.7270
  0.7720
  0.8342
  1.0382
  0.7106
  0.8387
  1.3702
  0.7085
  0.8422
  1.0857
  0.6815
  0.8861
  0.6328
  0.6310
  1.7039
  0.7192
  0.9814
  0.7075
  1.7386
  0.8980
  0.6129
  1.7189
  0.8907
  0.8783
  0.9132
  0.6799
  0.6226
  0.5898
  0.7034
  0.8722
  1.0820
  0.7159
  1.8304
  1.0010
  0.8034
  0.7522
  1.0692
  0.7648
  0.6873
  0.8660
  0.6140
  1.0178
  1.1833
  1.1069
  1.7900
  0.9096
  0.5322
  0.8669
  0.6663
  1.7419
  0.9139
  1.3150
  0.8319
  0.7685
  0.6776
  0.6577
  1.3165
  0.6996
  0.6529
  0.6610
  0.8026
  0.9331
  0.6319
  0.6902
  1.2370
  0.9864
  0.6632
  1.3991
  1.3061
  0.8618
  0.9307
  0.9959
  1.0044
  0.7443
  0.8459
  1.0360
  0.9388
  1.2310
  0.6424
  0.5804
  0.6128
  0.6981
  1.0620
  0.6917
  0.7664
  0.9246
  1.3838
  0.9506
  0.9046
  0.7194
  1.3983
  0.6345
  0.9874
  0.7634
  1.0204
  0.7857
  0.7947
  0.8903
  0.6756
  0.8573
  0.8294
  0.7741
  0.7640
  0.8012
  0.7878
  1.1527
  0.8135
  0.5933
  0.5427
  1.3901
  0.7265
  0.7736
  1.4690
  2.6832
  2.0378
  0.5354
  0.7972
  1.0227
  1.0614
  0.8048
  1.0031
  0.5246
  0.8802
  0.6075
  0.7237
  0.8039
  0.9032
  0.8021
  0.5895
  0.6389
  0.5724
  0.6189
  0.6243
  0.6247
  0.8427
  1.0766
  0.7732
  0.6067
  0.9121
  0.5756
  0.6959
  0.8115
  0.7000
  0.5530
  0.7730
  0.8382
  0.5674
  0.9155
  0.6772
  1.1471
  0.8509
  0.9285
  0.6016
  0.5879
  0.8060
  0.7156
  0.8025
  0.8133
  1.1539
  0.7270
  1.1033
  0.8326
  0.7110
  0.7695
  0.5836
  1.4028
  0.9392
  1.0893
  1.0633
  0.7497
  1.3496
  1.1011
  0.9535
  1.2260
  0.7138
  1.4667
  0.8705
  0.5595
  0.8997
[torch.FloatTensor of size 256]
), ('layer3.4.conv3.weight', 
( 0  , 0  ,.,.) = 
 -5.9331e-04

( 0  , 1  ,.,.) = 
  1.8079e-03

( 0  , 2  ,.,.) = 
  2.0339e-03
      ... 

( 0  ,253 ,.,.) = 
 -4.1875e-03

( 0  ,254 ,.,.) = 
  9.9313e-04

( 0  ,255 ,.,.) = 
  1.1720e-02
        ⋮  

( 1  , 0  ,.,.) = 
 -2.6283e-02

( 1  , 1  ,.,.) = 
 -3.2600e-03

( 1  , 2  ,.,.) = 
 -2.4356e-02
      ... 

( 1  ,253 ,.,.) = 
 -1.3290e-02

( 1  ,254 ,.,.) = 
 -3.0868e-02

( 1  ,255 ,.,.) = 
  3.9922e-03
        ⋮  

( 2  , 0  ,.,.) = 
 -1.2431e-02

( 2  , 1  ,.,.) = 
  2.8370e-02

( 2  , 2  ,.,.) = 
 -1.7048e-03
      ... 

( 2  ,253 ,.,.) = 
  6.5235e-03

( 2  ,254 ,.,.) = 
 -6.0346e-03

( 2  ,255 ,.,.) = 
 -2.8201e-02
 ...      
        ⋮  

(1021, 0  ,.,.) = 
 -9.0503e-04

(1021, 1  ,.,.) = 
  1.3985e-02

(1021, 2  ,.,.) = 
  2.7736e-02
      ... 

(1021,253 ,.,.) = 
  2.9050e-02

(1021,254 ,.,.) = 
 -5.3157e-03

(1021,255 ,.,.) = 
  2.8942e-02
        ⋮  

(1022, 0  ,.,.) = 
  1.5963e-02

(1022, 1  ,.,.) = 
 -1.8018e-02

(1022, 2  ,.,.) = 
  9.1698e-03
      ... 

(1022,253 ,.,.) = 
  2.8396e-03

(1022,254 ,.,.) = 
  6.5481e-03

(1022,255 ,.,.) = 
  1.1409e-02
        ⋮  

(1023, 0  ,.,.) = 
 -2.2462e-02

(1023, 1  ,.,.) = 
  3.6912e-04

(1023, 2  ,.,.) = 
  1.0604e-02
      ... 

(1023,253 ,.,.) = 
  3.9585e-03

(1023,254 ,.,.) = 
 -6.8926e-03

(1023,255 ,.,.) = 
 -3.9098e-03
[torch.FloatTensor of size 1024x256x1x1]
), ('layer3.4.bn3.weight', 
 0.0009
 0.1826
 0.1513
   ⋮   
 0.1095
 0.0555
 0.0358
[torch.FloatTensor of size 1024]
), ('layer3.4.bn3.bias', 
-0.0169
-0.2128
-0.1429
   ⋮   
-0.0969
-0.0237
-0.0253
[torch.FloatTensor of size 1024]
), ('layer3.4.bn3.running_mean', 
 0.0095
-0.0122
-0.0056
   ⋮   
-0.0219
-0.0243
 0.0019
[torch.FloatTensor of size 1024]
), ('layer3.4.bn3.running_var', 
1.00000e-03 *
  0.1839
  1.1367
  1.0510
    ⋮   
  0.7920
  0.5985
  0.2961
[torch.FloatTensor of size 1024]
), ('layer3.5.conv1.weight', 
( 0  , 0  ,.,.) = 
 -6.0310e-04

( 0  , 1  ,.,.) = 
  1.7761e-02

( 0  , 2  ,.,.) = 
  4.6879e-02
      ... 

( 0  ,1021,.,.) = 
  2.6565e-03

( 0  ,1022,.,.) = 
 -3.1731e-02

( 0  ,1023,.,.) = 
 -3.1961e-02
        ⋮  

( 1  , 0  ,.,.) = 
  2.8313e-02

( 1  , 1  ,.,.) = 
 -2.9960e-02

( 1  , 2  ,.,.) = 
  6.8789e-03
      ... 

( 1  ,1021,.,.) = 
  9.8169e-04

( 1  ,1022,.,.) = 
  1.3060e-02

( 1  ,1023,.,.) = 
  3.7413e-03
        ⋮  

( 2  , 0  ,.,.) = 
  2.9517e-02

( 2  , 1  ,.,.) = 
 -3.3596e-03

( 2  , 2  ,.,.) = 
  1.2223e-02
      ... 

( 2  ,1021,.,.) = 
 -2.2092e-02

( 2  ,1022,.,.) = 
  1.6818e-02

( 2  ,1023,.,.) = 
  6.9128e-03
 ...      
        ⋮  

(253 , 0  ,.,.) = 
  2.6911e-03

(253 , 1  ,.,.) = 
 -1.8400e-03

(253 , 2  ,.,.) = 
  1.2463e-02
      ... 

(253 ,1021,.,.) = 
  1.5551e-03

(253 ,1022,.,.) = 
 -5.0797e-03

(253 ,1023,.,.) = 
  1.7978e-02
        ⋮  

(254 , 0  ,.,.) = 
  1.7029e-02

(254 , 1  ,.,.) = 
 -2.0254e-03

(254 , 2  ,.,.) = 
 -1.6941e-03
      ... 

(254 ,1021,.,.) = 
 -1.4131e-02

(254 ,1022,.,.) = 
  1.2477e-02

(254 ,1023,.,.) = 
 -2.8510e-02
        ⋮  

(255 , 0  ,.,.) = 
 -2.7551e-02

(255 , 1  ,.,.) = 
  5.7712e-02

(255 , 2  ,.,.) = 
 -1.1912e-02
      ... 

(255 ,1021,.,.) = 
  1.9937e-02

(255 ,1022,.,.) = 
 -1.9331e-02

(255 ,1023,.,.) = 
 -7.3146e-03
[torch.FloatTensor of size 256x1024x1x1]
), ('layer3.5.bn1.weight', 
 0.1948
 0.2103
 0.1212
 0.2151
 0.2013
 0.2338
 0.2944
 0.1917
 0.1829
 0.1640
 0.1770
 0.2103
 0.1740
 0.1739
 0.1975
 0.1661
 0.1908
 0.1852
 0.2386
 0.2910
 0.2078
 0.1500
 0.1946
 0.1704
 0.1506
 0.1702
 0.1703
 0.1529
 0.1867
 0.1885
 0.1863
 0.1446
 0.1716
 0.1494
 0.1802
 0.1402
 0.1357
 0.1655
 0.1934
 0.1497
 0.2556
 0.1460
 0.1724
 0.2584
 0.2165
 0.1947
 0.1798
 0.2354
 0.1708
 0.1231
 0.1407
 0.2105
 0.2117
 0.1465
 0.1701
 0.1808
 0.1243
 0.1922
 0.1521
 0.1621
 0.2064
 0.1426
 0.1856
 0.1543
 0.2007
 0.1708
 0.1919
 0.1408
 0.1831
 0.1552
 0.2117
 0.2639
 0.1480
 0.1831
 0.2081
 0.2090
 0.1943
 0.2180
 0.1667
 0.1511
 0.1736
 0.2208
 0.2237
 0.1889
 0.1799
 0.0897
 0.1993
 0.1611
 0.1971
 0.2159
 0.1882
 0.1870
 0.2000
 0.1844
 0.1307
 0.1710
 0.1956
 0.1528
 0.1681
 0.2044
 0.1870
 0.1935
 0.1741
 0.1656
 0.2113
 0.1651
 0.1835
 0.2180
 0.2107
 0.2069
 0.1844
 0.2376
 0.1798
 0.1882
 0.2032
 0.2480
 0.1715
 0.2192
 0.1886
 0.1920
 0.1843
 0.1610
 0.2126
 0.2105
 0.1596
 0.2083
 0.1651
 0.2035
 0.1743
 0.0949
 0.1796
 0.1529
 0.2042
 0.1798
 0.1606
 0.1607
 0.1746
 0.2506
 0.1223
 0.1925
 0.1924
 0.2121
 0.1935
 0.1857
 0.1669
 0.2394
 0.2276
 0.2252
 0.1508
 0.1717
 0.1808
 0.1765
 0.1416
 0.2039
 0.1914
 0.1832
 0.2311
 0.2254
 0.2168
 0.1956
 0.1924
 0.2333
 0.1844
 0.1900
 0.1785
 0.1613
 0.1865
 0.1906
 0.2555
 0.1293
 0.2190
 0.1616
 0.0984
 0.2218
 0.1602
 0.2044
 0.1798
 0.1376
 0.1238
 0.2010
 0.1835
 0.1742
 0.1905
 0.1863
 0.2193
 0.1538
 0.1164
 0.1184
 0.1739
 0.1561
 0.1640
 0.1818
 0.1947
 0.1763
 0.1744
 0.1801
 0.1632
 0.1439
 0.1758
 0.1852
 0.2062
 0.1236
 0.2004
 0.1913
 0.1859
 0.2046
 0.1666
 0.2145
 0.2149
 0.2011
 0.2430
 0.2123
 0.1989
 0.1607
 0.1499
 0.2037
 0.1780
 0.1772
 0.1461
 0.1630
 0.1615
 0.1403
 0.1770
 0.1623
 0.2794
 0.2106
 0.2188
 0.1711
 0.1859
 0.2209
 0.1645
 0.1755
 0.1464
 0.1461
 0.1962
 0.1848
 0.1843
 0.1541
 0.2093
 0.1778
 0.1801
 0.1604
 0.1644
 0.1697
 0.1581
 0.1750
 0.1816
 0.2000
 0.2390
 0.2000
 0.1911
 0.1910
 0.2021
 0.1601
 0.1581
 0.2218
[torch.FloatTensor of size 256]
), ('layer3.5.bn1.bias', 
-0.1785
-0.2099
 0.0802
-0.1658
-0.1965
-0.2649
-0.3144
-0.0887
-0.1429
-0.0583
-0.1153
-0.2108
-0.0852
-0.1223
-0.1581
-0.1357
-0.1542
-0.1896
-0.2577
-0.2571
-0.1749
 0.0310
-0.1794
-0.0743
-0.0657
-0.0592
-0.0611
-0.1266
-0.1670
-0.1209
-0.1311
-0.0373
-0.0707
-0.0690
-0.0851
-0.0012
 0.0047
-0.1133
-0.0909
-0.1065
-0.2593
 0.0044
-0.0931
-0.2399
-0.1464
-0.1072
-0.1299
-0.1336
-0.1267
 0.0212
-0.0075
-0.1517
-0.1415
 0.0184
-0.0890
-0.1225
 0.0779
-0.1476
-0.0708
-0.0566
-0.1248
-0.0549
-0.1264
-0.0319
-0.1575
-0.0975
-0.1478
-0.0235
-0.1250
-0.0867
-0.1991
-0.1774
-0.0249
-0.0916
-0.1465
-0.2073
-0.1081
-0.1618
-0.0178
-0.0159
-0.1207
-0.2073
-0.2235
-0.1227
-0.1143
 0.1364
-0.1229
-0.0937
-0.1521
-0.2278
-0.1377
-0.1255
-0.1556
-0.0900
-0.0239
-0.1024
-0.2163
-0.0313
-0.0678
-0.1471
-0.1437
-0.1314
-0.1286
-0.0424
-0.1573
-0.0780
-0.0815
-0.1611
-0.1554
-0.1146
-0.0302
-0.1876
-0.1179
-0.1574
-0.1549
-0.2130
-0.0870
-0.1054
-0.0972
-0.1181
-0.0949
-0.0361
-0.1453
-0.1136
-0.0825
-0.1286
-0.1012
-0.1371
-0.1124
 0.1322
-0.0724
-0.0367
-0.1939
-0.1150
-0.0140
-0.0929
-0.1021
-0.1714
 0.1359
-0.1186
-0.0458
-0.1254
-0.1112
-0.0745
-0.0571
-0.2073
-0.1167
-0.1942
-0.0252
-0.1083
-0.1104
-0.1240
 0.0227
-0.2183
-0.1357
-0.1065
-0.1628
-0.2177
-0.1620
-0.1555
-0.1785
-0.1714
-0.1276
-0.1518
-0.1752
-0.0056
-0.1766
-0.0879
-0.2702
 0.0020
-0.2040
-0.0641
 0.0797
-0.1826
-0.0647
-0.1214
-0.0979
 0.0018
 0.0326
-0.1189
-0.0942
-0.0361
-0.1732
-0.1679
-0.1091
-0.0713
 0.0521
 0.0196
-0.1274
-0.0747
-0.1058
-0.0559
-0.1106
-0.1207
-0.1368
-0.0935
-0.0353
-0.0506
-0.0538
-0.0936
-0.1119
 0.0077
-0.1345
-0.1528
-0.1444
-0.1633
-0.1036
-0.2081
-0.0842
-0.1674
-0.1317
-0.1487
-0.1678
-0.0215
-0.0322
-0.1122
-0.1295
-0.0814
-0.0010
-0.0139
-0.1405
 0.0197
-0.1477
-0.0987
-0.0793
-0.1658
-0.1859
-0.1489
-0.1157
-0.1785
-0.0796
-0.1393
-0.0174
-0.0157
-0.1570
-0.1530
-0.1477
-0.0691
-0.0899
-0.1629
-0.1087
-0.0536
-0.0294
-0.0746
-0.0863
-0.1033
-0.1157
-0.1933
-0.2118
-0.1378
-0.1393
-0.1381
-0.1260
 0.0618
-0.0584
-0.1395
[torch.FloatTensor of size 256]
), ('layer3.5.bn1.running_mean', 
-0.0568
-0.0939
 0.0116
-0.1836
 0.0337
-0.1707
-0.1542
-0.0395
-0.0057
-0.0256
-0.0135
-0.0707
-0.0444
-0.0274
-0.0293
-0.0906
-0.0631
-0.0663
-0.1745
-0.1742
-0.0979
-0.0939
-0.0557
-0.0390
-0.0409
-0.0803
 0.0341
-0.0810
-0.0635
 0.0201
-0.0489
-0.0205
-0.0472
-0.1139
-0.0479
 0.0400
-0.0890
 0.0116
-0.1147
 0.0481
-0.1160
 0.1828
-0.0277
-0.0782
-0.0791
-0.0890
-0.0630
-0.1658
-0.0569
-0.0671
-0.0477
-0.0669
-0.1231
-0.0803
-0.0667
-0.0369
-0.1150
 0.0147
-0.0188
-0.1250
-0.1702
-0.0061
-0.0954
-0.0398
 0.0002
-0.0151
-0.0942
-0.0468
-0.0893
-0.0066
-0.0110
-0.0921
-0.0499
-0.0720
-0.1289
-0.0890
 0.0166
-0.1174
-0.1157
-0.1002
-0.0606
-0.0187
-0.0237
-0.0035
-0.1105
-0.0679
-0.0745
-0.0174
-0.0610
-0.0994
-0.0156
 0.0142
-0.0570
-0.0875
-0.0037
 0.0144
-0.0601
-0.0549
-0.0067
-0.0200
-0.0949
-0.0648
-0.1184
-0.0120
-0.0756
-0.0018
-0.1428
-0.0874
-0.0734
-0.0840
 0.0473
-0.1010
-0.1072
-0.0449
 0.0459
-0.0998
-0.0306
-0.1279
-0.1006
-0.0484
 0.0522
-0.0189
-0.0421
-0.0574
-0.0471
-0.0174
-0.0144
-0.0882
-0.1020
-0.0387
-0.0588
-0.0107
-0.0932
-0.0215
-0.0550
-0.1058
-0.0482
-0.0365
 0.0997
-0.0720
-0.0785
-0.0385
-0.0365
-0.0039
-0.0380
-0.0769
 0.0011
-0.1404
-0.0312
-0.0083
-0.0312
-0.0469
-0.0648
-0.1263
-0.0575
-0.0491
-0.1216
-0.1039
-0.0416
-0.0521
-0.0484
-0.0505
 0.0216
-0.0281
-0.1157
-0.0681
-0.0199
 0.0142
-0.1272
-0.0712
-0.0616
 0.0131
-0.0312
-0.1029
-0.1011
-0.1296
-0.0494
-0.0873
 0.0012
-0.0256
-0.0224
-0.1417
-0.0128
-0.0558
 0.0145
 0.0259
-0.0492
-0.0236
 0.0105
-0.0322
-0.0030
-0.1471
-0.0587
-0.0137
-0.0189
-0.0299
-0.1210
 0.0240
-0.1807
-0.0817
-0.1126
-0.1425
-0.0861
-0.0575
-0.1099
-0.0465
-0.0310
-0.0765
-0.0609
-0.1153
-0.1609
-0.0438
-0.0179
-0.1095
 0.0076
-0.1141
-0.1394
-0.0606
-0.0818
-0.0680
-0.0428
-0.0621
-0.0623
-0.1138
-0.1251
 0.0016
-0.0591
-0.0538
-0.0545
-0.1293
-0.0435
 0.0289
-0.1083
-0.0232
 0.0014
-0.0290
-0.0626
-0.0659
-0.0522
-0.0467
-0.0745
 0.0323
-0.1183
-0.0698
-0.0890
-0.0850
-0.0110
-0.0309
-0.0197
-0.1140
-0.0425
-0.0258
-0.0515
-0.0272
 0.0101
-0.0357
[torch.FloatTensor of size 256]
), ('layer3.5.bn1.running_var', 
1.00000e-02 *
  1.3312
  1.1212
  1.7119
  1.2479
  1.2543
  0.9665
  1.8714
  2.4534
  1.0225
  0.9182
  1.1130
  1.1003
  1.3025
  1.2519
  1.4518
  0.9185
  1.0357
  0.8711
  1.3672
  2.3564
  1.3172
  2.8303
  0.9657
  1.2143
  1.0759
  1.2995
  1.3905
  1.1915
  1.0149
  1.1779
  1.3028
  1.4138
  1.5683
  1.0937
  1.1747
  1.7917
  1.3094
  1.1497
  1.6156
  1.0735
  1.1662
  2.5682
  1.1714
  1.3999
  1.1349
  1.8099
  1.4030
  1.6664
  0.9281
  1.4078
  1.2527
  1.4316
  1.8757
  1.6897
  1.3896
  1.2881
  2.4904
  1.2634
  1.1164
  1.2715
  1.4292
  1.3231
  1.7810
  1.7351
  1.0071
  1.2546
  1.0513
  1.2129
  1.7598
  1.2300
  1.4348
  2.2223
  1.9264
  1.4057
  1.2211
  1.0282
  1.0787
  1.1675
  1.9111
  1.2583
  1.2877
  1.3956
  1.2487
  1.4795
  1.1003
  1.1631
  1.1082
  0.9724
  1.2717
  1.1912
  0.9524
  1.3380
  1.2604
  1.1720
  0.9826
  0.8504
  0.8237
  1.8665
  1.3707
  1.7382
  1.1284
  1.4489
  1.2581
  1.5068
  1.2449
  1.3885
  1.6917
  1.5499
  1.5044
  1.3080
  2.5316
  1.9684
  1.2104
  1.1281
  1.2061
  1.5857
  1.3252
  2.8520
  1.2792
  1.4004
  1.5629
  1.4790
  1.6387
  1.5755
  0.9234
  1.9114
  1.3082
  1.2270
  1.2926
  1.5642
  1.3537
  1.4127
  1.1997
  1.1544
  1.9621
  1.4815
  1.3418
  1.9672
  3.1282
  1.8308
  1.9066
  2.0281
  1.4967
  1.8611
  1.5572
  1.5011
  1.5021
  1.2170
  1.3450
  1.1605
  1.4714
  1.1355
  1.8258
  0.9913
  1.4020
  1.3788
  1.9765
  1.6827
  1.2225
  0.8004
  1.1351
  1.5360
  1.0272
  1.0303
  1.1083
  2.9348
  0.9289
  1.9371
  1.2572
  1.9674
  1.1469
  1.2013
  1.3985
  1.6185
  1.2002
  1.4473
  1.7932
  1.2983
  1.6892
  1.1992
  1.6526
  2.3007
  1.0140
  0.9137
  1.7618
  1.1847
  1.5294
  1.4431
  0.9445
  1.0912
  1.0317
  2.2234
  1.7879
  0.9938
  1.0334
  1.3270
  1.2199
  1.1551
  1.9553
  1.2352
  1.4546
  1.5963
  1.5623
  1.0939
  0.8713
  1.0336
  0.9521
  0.9468
  1.7995
  0.9295
  2.0945
  1.1553
  0.9022
  1.5299
  1.3601
  1.1923
  1.1785
  1.7295
  1.8099
  1.7941
  1.0227
  1.9622
  1.0020
  1.0639
  3.3366
  1.2807
  1.3630
  1.2391
  1.5352
  1.2230
  1.1331
  1.2264
  1.4715
  1.4367
  1.0318
  1.3018
  1.2973
  1.4144
  2.1743
  1.0608
  1.3100
  1.3878
  1.7206
  1.6656
  1.1589
  1.1196
  1.6186
  1.1360
  1.3670
  1.3537
  1.2171
  1.1075
  1.3863
  4.4336
  1.2461
  1.7438
[torch.FloatTensor of size 256]
), ('layer3.5.conv2.weight', 
( 0 , 0 ,.,.) = 
 -1.8232e-02 -3.7203e-02 -2.3618e-02
  3.0867e-03  2.6153e-02 -6.7986e-03
  9.4285e-04  2.7014e-04 -7.5859e-03

( 0 , 1 ,.,.) = 
  1.6146e-02  3.4563e-02  1.9232e-02
  3.4687e-02  6.0032e-02  3.0663e-02
 -9.6985e-03  8.1054e-03 -5.2946e-03

( 0 , 2 ,.,.) = 
 -1.4234e-02 -4.3377e-03 -4.3874e-03
 -2.9364e-03 -1.6176e-02  1.6481e-03
  7.3642e-03  4.9375e-03  1.0650e-02
    ... 

( 0 ,253,.,.) = 
  2.2973e-03  1.2850e-02  7.2108e-03
 -1.6075e-02 -9.9125e-04 -8.4743e-03
 -2.4514e-02 -5.8070e-03  6.4657e-04

( 0 ,254,.,.) = 
 -5.7512e-03  5.9755e-03  7.1584e-04
 -1.9273e-03 -1.6972e-03  1.2136e-02
 -5.5451e-03  4.0342e-03 -7.4075e-04

( 0 ,255,.,.) = 
 -2.2081e-03 -4.3551e-03  1.6138e-02
 -1.9957e-04 -5.3369e-03  8.8725e-03
  1.7258e-03  2.5699e-03  2.1026e-03
      ⋮  

( 1 , 0 ,.,.) = 
  1.2546e-02 -5.8992e-04  1.1483e-02
  8.9914e-03 -1.7501e-02  2.2883e-02
 -1.0734e-03  2.9461e-03  1.4120e-02

( 1 , 1 ,.,.) = 
 -2.1133e-02 -2.2209e-02 -1.1183e-02
 -1.7590e-02 -1.1415e-02  8.1762e-03
  1.4293e-02  1.0924e-02  1.3077e-02

( 1 , 2 ,.,.) = 
  4.5858e-03  1.4184e-02  3.5841e-03
  5.9022e-03  2.7697e-03 -5.5906e-03
  3.5660e-03  6.7106e-04 -2.4308e-02
    ... 

( 1 ,253,.,.) = 
  4.1768e-03 -5.8112e-03 -1.4056e-02
  7.0462e-03 -7.4015e-03 -2.3515e-02
  7.2506e-03 -2.1789e-02 -2.8072e-02

( 1 ,254,.,.) = 
 -2.9742e-02 -1.8196e-02 -2.6209e-02
 -1.0129e-02  5.7325e-03 -2.9642e-03
 -1.6088e-02 -1.6614e-02 -1.1019e-02

( 1 ,255,.,.) = 
 -8.2624e-03 -1.6960e-02 -3.3549e-03
 -1.7180e-03  6.2526e-03 -3.3032e-03
 -1.8702e-02 -1.8744e-02 -2.3722e-02
      ⋮  

( 2 , 0 ,.,.) = 
 -1.9137e-02 -2.7025e-02 -2.1552e-02
 -2.2563e-02 -2.3228e-02 -2.6779e-02
 -8.2968e-03 -6.4610e-03 -1.5783e-02

( 2 , 1 ,.,.) = 
  9.5157e-03  1.2182e-02  1.2291e-02
 -2.8040e-03 -3.4145e-02 -4.8298e-03
  2.2046e-02  4.7675e-02  2.1649e-02

( 2 , 2 ,.,.) = 
 -2.0278e-03  8.4803e-04  6.6914e-03
  1.9825e-03  3.2314e-02  4.6029e-03
 -4.1501e-02 -2.1296e-02 -3.4869e-02
    ... 

( 2 ,253,.,.) = 
  4.7098e-04  8.1674e-03  2.3782e-02
 -1.6303e-02  7.0586e-04  4.3863e-03
 -1.2259e-02 -4.9076e-05 -8.3181e-03

( 2 ,254,.,.) = 
  1.4525e-02  2.8155e-02  1.7697e-02
 -5.9662e-03  6.7936e-03 -8.2686e-03
  6.9453e-04 -4.3367e-03 -8.2057e-03

( 2 ,255,.,.) = 
 -1.8842e-02 -2.1245e-02 -2.3877e-02
 -1.7169e-02 -2.7942e-02 -1.0753e-02
 -8.7937e-03 -1.1407e-02  5.4173e-03
...     
      ⋮  

(253, 0 ,.,.) = 
 -1.9409e-02 -5.9409e-03  5.9442e-03
 -1.3004e-02 -3.1706e-02 -5.4351e-03
 -1.8653e-02 -3.2165e-02 -1.7299e-02

(253, 1 ,.,.) = 
 -2.1062e-02 -1.3807e-02 -6.0481e-03
 -3.0783e-03  7.8395e-04  2.6406e-03
  1.6623e-02  3.9630e-02  1.7395e-02

(253, 2 ,.,.) = 
  7.0678e-03 -1.0764e-02 -7.9355e-03
  1.7459e-02 -1.3725e-02  1.7816e-02
  3.9210e-02  1.7379e-03  3.2920e-02
    ... 

(253,253,.,.) = 
 -1.7285e-02  4.9075e-03  3.6420e-02
 -1.3927e-02 -2.3735e-02  2.0495e-02
  6.6044e-04 -8.5418e-03  3.0780e-02

(253,254,.,.) = 
 -1.5714e-03 -7.0358e-06  1.0681e-02
  5.5914e-03 -1.3363e-02  8.8432e-03
  2.1575e-03  2.5597e-02  1.6927e-02

(253,255,.,.) = 
 -2.8245e-02 -2.2754e-02 -2.1844e-02
 -1.8798e-02 -1.2453e-02 -3.1665e-02
 -1.6448e-02 -1.8883e-02 -9.7382e-03
      ⋮  

(254, 0 ,.,.) = 
  1.4527e-02  9.7653e-03  1.8607e-02
  1.7390e-02 -1.9561e-03 -5.7478e-04
  1.6686e-02  2.3338e-02  1.6452e-02

(254, 1 ,.,.) = 
 -4.0944e-04 -1.8717e-02 -1.3497e-02
 -2.3124e-02 -1.6939e-02 -2.5140e-02
 -1.3033e-02 -8.9127e-03 -1.2479e-02

(254, 2 ,.,.) = 
 -3.5714e-03  4.4862e-03 -6.0032e-03
 -1.0248e-02  9.9325e-03  5.7604e-04
 -1.6890e-02 -1.7979e-02 -1.3478e-02
    ... 

(254,253,.,.) = 
 -1.4564e-02 -2.6356e-03 -4.5432e-03
 -1.6651e-02  4.7062e-03  4.6228e-03
 -4.2661e-02 -5.1634e-03  2.0140e-02

(254,254,.,.) = 
 -2.2336e-02 -6.3959e-03 -1.4273e-02
 -1.2428e-02 -3.4726e-04 -5.6012e-03
 -1.1977e-03  7.1808e-03  1.0123e-02

(254,255,.,.) = 
 -1.3417e-02 -8.7234e-03 -3.6504e-03
 -1.3646e-02  1.1942e-03 -1.1993e-02
 -1.0109e-02 -2.3825e-03  4.6304e-05
      ⋮  

(255, 0 ,.,.) = 
 -1.8207e-02 -1.4984e-02 -5.5921e-03
 -5.2332e-03 -9.4499e-03  5.2049e-03
 -1.6134e-02 -2.5237e-02  3.4571e-03

(255, 1 ,.,.) = 
  6.7473e-04 -1.6203e-02 -7.4866e-03
 -3.3327e-03 -1.3687e-02 -6.5016e-03
 -1.1313e-03 -1.4965e-02 -7.7389e-03

(255, 2 ,.,.) = 
  6.2745e-03 -2.6325e-03  9.0637e-03
 -1.1776e-02 -2.6573e-02 -2.3888e-03
  4.6322e-05 -1.2211e-02  1.0205e-02
    ... 

(255,253,.,.) = 
 -2.3483e-02 -1.4903e-02  1.8685e-02
 -1.4492e-02 -1.0298e-02 -5.6091e-03
 -2.9446e-02 -1.0531e-02  3.6425e-02

(255,254,.,.) = 
 -3.9796e-03 -3.1082e-03  5.5262e-03
  7.4617e-03 -3.9119e-03  2.0406e-02
  2.9055e-03  1.1029e-02  3.7552e-03

(255,255,.,.) = 
 -2.2511e-02 -2.0920e-02 -7.0648e-03
 -7.2441e-03  3.7443e-03  6.4946e-03
 -2.3425e-02 -2.0029e-02 -1.6306e-02
[torch.FloatTensor of size 256x256x3x3]
), ('layer3.5.bn2.weight', 
 0.1799
 0.1605
 0.1783
 0.2058
 0.1696
 0.1842
 0.1896
 0.1804
 0.1934
 0.1578
 0.2080
 0.1694
 0.1916
 0.1874
 0.1915
 0.1741
 0.1861
 0.1983
 0.1571
 0.1738
 0.1989
 0.1918
 0.2507
 0.1985
 0.2673
 0.2015
 0.1865
 0.1705
 0.1490
 0.1483
 0.2136
 0.1862
 0.2224
 0.1555
 0.2088
 0.2230
 0.1968
 0.2047
 0.1893
 0.2631
 0.2627
 0.1734
 0.1242
 0.2153
 0.1686
 0.1458
 0.1808
 0.1640
 0.2165
 0.2037
 0.1304
 0.2112
 0.2024
 0.2059
 0.1898
 0.2017
 0.2588
 0.2188
 0.2253
 0.1665
 0.1293
 0.1915
 0.1621
 0.1938
 0.2011
 0.2077
 0.1975
 0.1982
 0.1697
 0.1678
 0.2291
 0.1479
 0.1789
 0.1988
 0.1555
 0.2127
 0.1735
 0.1964
 0.1736
 0.2242
 0.1795
 0.1817
 0.1719
 0.1855
 0.1770
 0.1576
 0.1541
 0.1984
 0.2344
 0.2019
 0.2195
 0.1836
 0.1987
 0.1910
 0.2012
 0.1697
 0.1727
 0.1793
 0.2071
 0.1914
 0.2501
 0.1884
 0.1887
 0.1951
 0.2129
 0.2244
 0.1791
 0.1999
 0.2044
 0.1941
 0.1926
 0.1959
 0.1806
 0.1630
 0.2154
 0.1732
 0.2303
 0.2440
 0.1516
 0.1596
 0.1556
 0.1869
 0.2629
 0.2113
 0.1619
 0.1587
 0.1845
 0.1732
 0.1779
 0.1950
 0.2191
 0.1859
 0.1843
 0.1634
 0.1895
 0.2074
 0.1900
 0.1853
 0.2092
 0.1943
 0.1716
 0.2214
 0.1908
 0.1776
 0.1515
 0.2557
 0.1530
 0.1435
 0.1909
 0.1779
 0.1919
 0.1913
 0.1885
 0.2086
 0.1791
 0.1657
 0.2026
 0.1309
 0.1823
 0.1708
 0.1615
 0.2154
 0.1943
 0.1827
 0.1934
 0.1686
 0.1740
 0.1768
 0.1884
 0.1395
 0.1694
 0.1249
 0.1984
 0.2558
 0.1818
 0.1458
 0.2029
 0.2144
 0.1988
 0.1616
 0.1437
 0.2796
 0.2465
 0.1809
 0.2152
 0.2142
 0.2673
 0.1915
 0.1817
 0.2041
 0.1874
 0.1996
 0.1443
 0.1502
 0.1848
 0.1803
 0.2029
 0.2030
 0.2153
 0.4324
 0.1349
 0.2063
 0.2048
 0.2260
 0.1979
 0.1902
 0.2182
 0.2398
 0.1996
 0.1711
 0.1908
 0.2316
 0.1845
 0.2276
 0.1680
 0.2185
 0.2005
 0.1392
 0.1781
 0.1988
 0.2038
 0.1930
 0.1915
 0.2092
 0.1931
 0.1841
 0.1957
 0.1809
 0.2423
 0.1645
 0.1514
 0.2635
 0.2045
 0.1309
 0.2280
 0.5254
 0.1907
 0.1380
 0.1668
 0.1868
 0.1888
 0.1843
 0.1799
 0.1814
 0.2375
 0.1715
 0.2021
 0.1671
 0.1971
 0.2151
 0.1966
 0.1898
 0.1715
 0.2070
 0.1768
 0.2672
[torch.FloatTensor of size 256]
), ('layer3.5.bn2.bias', 
-0.0950
-0.0429
-0.0648
-0.1112
-0.1368
-0.0984
-0.1285
-0.0326
-0.1890
-0.0512
-0.1933
-0.0564
-0.1662
-0.1181
-0.1060
-0.0185
-0.0933
-0.0582
-0.0318
-0.1021
-0.1560
-0.0770
-0.1286
-0.0659
-0.1850
-0.0679
-0.0762
-0.0659
-0.0736
-0.0134
-0.1070
-0.0684
-0.0896
-0.0029
-0.1198
-0.0119
-0.1063
-0.0686
-0.0612
-0.1319
-0.1781
-0.0765
 0.1662
-0.1244
-0.0856
-0.0270
-0.1248
-0.0530
-0.2007
-0.0635
 0.1194
-0.1497
-0.0671
-0.1742
-0.1035
-0.1167
-0.1681
-0.1027
-0.1438
-0.0405
 0.0730
-0.0770
-0.0578
-0.0834
-0.2014
-0.1179
-0.0407
-0.0703
 0.0021
-0.0678
-0.1495
-0.0342
-0.0983
-0.1059
-0.0215
-0.0554
-0.0783
-0.0742
-0.0601
-0.0918
-0.0507
-0.1486
-0.0749
-0.0901
-0.0635
-0.0740
-0.0870
-0.0976
-0.2599
-0.0890
-0.0899
-0.1529
-0.0424
-0.1403
-0.1159
-0.0523
-0.0813
-0.1198
-0.1376
-0.0852
-0.0677
-0.1657
-0.1012
-0.1307
-0.0798
-0.2427
-0.0658
-0.0631
-0.1021
-0.1274
-0.0958
-0.0602
-0.1203
-0.0622
-0.0727
-0.1410
-0.0888
-0.0385
 0.0194
-0.0481
-0.0208
-0.0979
-0.1236
-0.1103
-0.0105
-0.0171
-0.0954
-0.1581
-0.1261
-0.0830
-0.1141
-0.0670
-0.0860
-0.0589
-0.1430
-0.1792
-0.1794
-0.1547
-0.1523
-0.0766
-0.0451
-0.2206
-0.0898
-0.0892
 0.0237
-0.0974
-0.0226
 0.0019
-0.1125
-0.0307
-0.0494
-0.0957
-0.1024
-0.0940
-0.0093
-0.0868
-0.1182
 0.0850
-0.0677
-0.0792
-0.0409
-0.0909
-0.1315
-0.0968
-0.0748
-0.0583
-0.1061
-0.1413
-0.1347
-0.0359
-0.0432
 0.0624
-0.0906
-0.0642
-0.0414
 0.0111
-0.1396
-0.0983
-0.1266
-0.0408
-0.0016
-0.2592
-0.0847
-0.0986
-0.0259
-0.0874
-0.1198
-0.1240
-0.0968
-0.1471
-0.0967
-0.1273
 0.0119
-0.0201
-0.0717
-0.0867
-0.0837
-0.1474
-0.1657
-0.1853
 0.0859
-0.1579
-0.0908
-0.1340
-0.1316
-0.0701
-0.1214
-0.1153
-0.1148
-0.0618
-0.0698
-0.1352
-0.0835
-0.1696
-0.0338
-0.1497
-0.0935
 0.0735
-0.0710
-0.1276
-0.0829
-0.1407
-0.1013
-0.1726
-0.0950
-0.0895
-0.1470
-0.0467
-0.0776
-0.0146
 0.0136
-0.2587
-0.0910
 0.1044
-0.1138
-0.3993
-0.0544
 0.0161
-0.0538
-0.1187
-0.0699
-0.0692
-0.0611
-0.0670
-0.2354
-0.0573
-0.1812
-0.0524
-0.1779
-0.1660
-0.1221
-0.0657
-0.0900
-0.0665
-0.0885
-0.0772
[torch.FloatTensor of size 256]
), ('layer3.5.bn2.running_mean', 
-0.1190
-0.0680
-0.0405
 0.0105
-0.0615
-0.0273
-0.0822
-0.0283
-0.0505
-0.0188
-0.0873
-0.0744
-0.1176
-0.0725
-0.1012
-0.0991
 0.2366
-0.0888
-0.0793
 0.0598
-0.0902
-0.1586
 0.0859
-0.0309
 0.0194
-0.1046
-0.0982
-0.1998
-0.0417
-0.0794
-0.0633
-0.0903
-0.1061
-0.0950
-0.0742
-0.0513
-0.1391
-0.1032
-0.0750
-0.0823
-0.0795
 0.0062
-0.0118
-0.1539
-0.0603
-0.0471
 0.0504
-0.0810
-0.1315
-0.0739
 0.0347
-0.0797
-0.0305
-0.0405
-0.0195
-0.0415
-0.0955
-0.1272
-0.0688
-0.0668
-0.0022
-0.0568
-0.0715
-0.0508
-0.1614
-0.0722
-0.1584
-0.1032
-0.0558
-0.0125
 0.0237
-0.1208
-0.0509
-0.0455
-0.0881
-0.1163
 0.0030
-0.0471
-0.0676
-0.0604
-0.0375
-0.0732
-0.0673
-0.0837
-0.0974
-0.0309
-0.0161
-0.0964
-0.0885
 0.0205
-0.1267
 0.0301
-0.0646
-0.0482
 0.1000
-0.0801
-0.0459
-0.0871
-0.0340
-0.0980
-0.0990
-0.1079
-0.0532
-0.0312
-0.0675
 0.1884
-0.0749
-0.0284
-0.0359
-0.0395
-0.1025
-0.0799
-0.0891
 0.0109
-0.0521
-0.0682
-0.0451
-0.0545
-0.0060
-0.0603
-0.1265
-0.0850
-0.0946
-0.0698
-0.0189
-0.0995
-0.0607
-0.0706
-0.0774
-0.0877
-0.0044
-0.0532
-0.0500
-0.1197
-0.1535
-0.0942
-0.1032
-0.0451
-0.0858
-0.0826
-0.1034
-0.0032
 0.0640
 0.0791
-0.0658
-0.1041
 0.0067
-0.0697
-0.0641
-0.1103
-0.0406
 0.0026
-0.0549
-0.0933
-0.0378
-0.0659
-0.0681
-0.1166
-0.1443
-0.0577
-0.1023
-0.1006
-0.0870
-0.0030
 0.0047
-0.0549
-0.0313
-0.0563
-0.0135
-0.0529
-0.1002
 0.0191
-0.0775
-0.1105
-0.1338
-0.0498
 0.0088
-0.1388
-0.0525
-0.0404
-0.0885
-0.1318
-0.0682
 0.0594
-0.0441
-0.0967
 0.0375
-0.0896
-0.0489
 0.0156
-0.0757
-0.0193
-0.0553
-0.0455
 0.0019
-0.0742
-0.0693
-0.0347
-0.0252
-0.1069
 0.0383
-0.0441
-0.0619
 0.1426
-0.0828
-0.0741
-0.0161
-0.0682
 0.0172
-0.0716
-0.1241
 0.0029
-0.1268
 0.0397
-0.0564
-0.0474
-0.0282
-0.1347
-0.1370
-0.0157
-0.1291
 0.0961
 0.0035
-0.0653
-0.0809
-0.0842
-0.0697
-0.0241
-0.0853
-0.1776
-0.0170
-0.2015
-0.1110
 0.0435
-0.0585
-0.2239
-0.0967
-0.0188
-0.0779
-0.0437
-0.0090
-0.0234
-0.0493
-0.0602
-0.0687
-0.0474
-0.1764
-0.0889
 0.1536
-0.0324
-0.0289
-0.1210
-0.0145
-0.0760
-0.0976
-0.1151
[torch.FloatTensor of size 256]
), ('layer3.5.bn2.running_var', 
1.00000e-02 *
  0.7674
  0.7330
  0.9424
  0.9354
  0.5737
  0.9118
  0.6688
  1.1100
  0.7307
  0.9758
  0.8974
  1.1533
  0.8474
  0.7944
  1.0763
  1.6191
  2.1495
  1.2954
  0.7939
  1.5128
  0.8365
  0.9848
  1.7537
  1.2920
  1.1368
  1.5869
  1.2227
  1.6354
  0.7843
  0.8291
  1.0569
  1.1099
  1.3745
  0.9768
  0.9169
  5.6528
  1.0380
  1.4584
  1.0922
  1.6969
  1.3612
  0.9827
  1.2334
  0.9437
  0.8017
  1.0462
  0.7097
  0.8696
  0.9120
  2.1427
  1.1322
  0.9604
  1.0165
  0.7838
  1.2461
  0.9379
  1.1853
  1.2734
  0.9867
  1.1007
  1.4859
  0.9360
  0.9352
  0.8641
  0.7602
  0.9930
  1.6696
  1.4915
  1.6254
  0.7988
  1.2000
  0.9161
  0.7747
  0.9153
  0.8594
  1.1929
  0.9917
  1.6512
  1.2301
  1.0837
  1.0513
  0.6662
  0.8519
  1.0058
  0.9581
  0.7560
  1.2023
  0.9607
  1.1804
  1.3675
  1.8040
  0.8357
  1.0719
  0.8420
  0.9942
  0.9471
  0.7506
  0.8670
  0.8191
  1.3972
  1.4458
  0.9381
  1.1062
  0.7752
  1.5403
  1.5288
  1.1084
  1.2042
  0.9902
  0.8331
  1.3412
  1.8829
  0.6450
  0.9159
  2.5302
  0.6443
  1.1245
  4.6281
  1.2130
  0.9738
  1.0395
  1.1330
  3.5293
  1.8346
  0.9764
  1.3964
  0.7557
  0.6105
  0.7827
  1.1991
  1.5495
  1.0424
  0.8951
  0.7198
  0.8046
  0.7547
  1.2669
  0.7278
  0.7235
  1.1272
  0.7940
  1.1472
  1.0301
  1.0765
  1.3192
  2.0995
  1.0252
  1.0962
  1.0474
  1.8130
  0.9406
  0.9650
  0.8627
  1.2120
  1.3021
  0.9089
  1.1816
  1.2409
  1.0835
  0.7967
  1.0993
  1.4187
  0.7428
  0.7520
  1.2555
  0.8963
  0.7190
  1.2827
  0.9123
  1.1984
  0.9835
  1.5152
  1.2045
  1.2247
  1.0672
  0.9366
  0.9195
  1.3097
  0.7647
  0.7826
  1.3258
  1.1378
  1.8913
  0.9618
  1.4691
  1.0494
  1.6400
  0.7177
  0.7181
  1.1707
  1.2424
  0.8039
  1.1063
  1.2753
  1.1108
  0.7709
  1.5903
  0.8851
  1.5259
  4.5615
  1.8791
  0.6297
  1.8127
  1.6095
  1.1161
  0.8170
  1.3335
  1.4440
  0.9837
  1.3684
  1.4479
  1.1372
  0.8494
  1.2360
  1.3190
  0.8197
  0.7751
  1.2768
  0.8733
  0.8962
  1.2332
  1.0192
  0.9046
  1.3219
  0.8345
  0.9128
  0.7406
  1.1760
  2.0217
  1.5369
  0.8208
  1.6726
  0.9484
  1.7975
  0.9319
  5.3226
  1.1823
  1.0993
  0.7086
  0.7375
  1.5630
  1.3996
  0.9678
  1.0632
  0.7716
  1.1465
  0.9723
  0.6916
  1.2613
  0.7662
  0.8981
  1.4287
  0.7468
  1.3244
  0.8698
  2.6827
[torch.FloatTensor of size 256]
), ('layer3.5.conv3.weight', 
( 0  , 0  ,.,.) = 
 -9.7815e-04

( 0  , 1  ,.,.) = 
  2.4410e-03

( 0  , 2  ,.,.) = 
  6.4362e-04
      ... 

( 0  ,253 ,.,.) = 
  4.5518e-04

( 0  ,254 ,.,.) = 
  3.1839e-03

( 0  ,255 ,.,.) = 
  1.5780e-04
        ⋮  

( 1  , 0  ,.,.) = 
  1.2811e-02

( 1  , 1  ,.,.) = 
  1.2113e-02

( 1  , 2  ,.,.) = 
  5.8799e-02
      ... 

( 1  ,253 ,.,.) = 
 -2.8383e-02

( 1  ,254 ,.,.) = 
 -2.0166e-03

( 1  ,255 ,.,.) = 
  5.0809e-03
        ⋮  

( 2  , 0  ,.,.) = 
 -4.1260e-02

( 2  , 1  ,.,.) = 
 -6.2723e-03

( 2  , 2  ,.,.) = 
 -2.2622e-02
      ... 

( 2  ,253 ,.,.) = 
 -1.7216e-03

( 2  ,254 ,.,.) = 
  1.5788e-02

( 2  ,255 ,.,.) = 
 -3.2657e-02
 ...      
        ⋮  

(1021, 0  ,.,.) = 
 -3.1535e-02

(1021, 1  ,.,.) = 
  5.1557e-03

(1021, 2  ,.,.) = 
 -1.2088e-02
      ... 

(1021,253 ,.,.) = 
  3.0828e-02

(1021,254 ,.,.) = 
  1.3775e-02

(1021,255 ,.,.) = 
 -9.8853e-04
        ⋮  

(1022, 0  ,.,.) = 
 -1.1204e-02

(1022, 1  ,.,.) = 
 -7.0668e-03

(1022, 2  ,.,.) = 
  6.0029e-04
      ... 

(1022,253 ,.,.) = 
  1.0027e-02

(1022,254 ,.,.) = 
 -4.5193e-03

(1022,255 ,.,.) = 
 -2.3091e-02
        ⋮  

(1023, 0  ,.,.) = 
  1.1157e-02

(1023, 1  ,.,.) = 
 -1.1145e-02

(1023, 2  ,.,.) = 
  2.1084e-03
      ... 

(1023,253 ,.,.) = 
 -7.7713e-04

(1023,254 ,.,.) = 
 -1.5691e-02

(1023,255 ,.,.) = 
 -1.2437e-02
[torch.FloatTensor of size 1024x256x1x1]
), ('layer3.5.bn3.weight', 
 0.0096
 0.2409
 0.1293
   ⋮   
 0.1258
 0.1065
 0.0620
[torch.FloatTensor of size 1024]
), ('layer3.5.bn3.bias', 
-0.0098
-0.2781
-0.1677
   ⋮   
-0.1630
-0.0894
-0.0560
[torch.FloatTensor of size 1024]
), ('layer3.5.bn3.running_mean', 
-0.0106
-0.0633
-0.0473
   ⋮   
-0.0087
-0.0513
-0.0187
[torch.FloatTensor of size 1024]
), ('layer3.5.bn3.running_var', 
1.00000e-02 *
  0.0502
  0.1606
  0.0796
    ⋮   
  0.0987
  0.2087
  0.1059
[torch.FloatTensor of size 1024]
), ('layer4.0.conv1.weight', 
( 0  , 0  ,.,.) = 
 -1.1374e-02

( 0  , 1  ,.,.) = 
  5.5807e-03

( 0  , 2  ,.,.) = 
  9.1282e-03
      ... 

( 0  ,1021,.,.) = 
 -1.4862e-02

( 0  ,1022,.,.) = 
 -5.4489e-03

( 0  ,1023,.,.) = 
  2.1227e-02
        ⋮  

( 1  , 0  ,.,.) = 
  3.6502e-02

( 1  , 1  ,.,.) = 
  3.3715e-02

( 1  , 2  ,.,.) = 
  1.3829e-02
      ... 

( 1  ,1021,.,.) = 
 -1.0736e-02

( 1  ,1022,.,.) = 
 -3.6041e-03

( 1  ,1023,.,.) = 
  1.4193e-02
        ⋮  

( 2  , 0  ,.,.) = 
 -6.0909e-03

( 2  , 1  ,.,.) = 
  1.0740e-02

( 2  , 2  ,.,.) = 
 -9.8404e-03
      ... 

( 2  ,1021,.,.) = 
 -1.9329e-02

( 2  ,1022,.,.) = 
  3.3954e-02

( 2  ,1023,.,.) = 
 -6.5343e-03
 ...      
        ⋮  

(509 , 0  ,.,.) = 
 -1.1574e-02

(509 , 1  ,.,.) = 
 -7.4281e-03

(509 , 2  ,.,.) = 
  4.3363e-02
      ... 

(509 ,1021,.,.) = 
 -6.7327e-03

(509 ,1022,.,.) = 
  1.5979e-02

(509 ,1023,.,.) = 
  1.4803e-02
        ⋮  

(510 , 0  ,.,.) = 
 -1.0136e-02

(510 , 1  ,.,.) = 
 -2.6504e-03

(510 , 2  ,.,.) = 
  3.3763e-02
      ... 

(510 ,1021,.,.) = 
 -1.2074e-02

(510 ,1022,.,.) = 
  2.4048e-02

(510 ,1023,.,.) = 
 -9.6961e-03
        ⋮  

(511 , 0  ,.,.) = 
  2.2228e-02

(511 , 1  ,.,.) = 
  3.4054e-02

(511 , 2  ,.,.) = 
 -1.0361e-02
      ... 

(511 ,1021,.,.) = 
 -2.7223e-02

(511 ,1022,.,.) = 
 -2.2601e-02

(511 ,1023,.,.) = 
 -2.8185e-02
[torch.FloatTensor of size 512x1024x1x1]
), ('layer4.0.bn1.weight', 
 0.2462
 0.2034
 0.2143
 0.1938
 0.2510
 0.1240
 0.2001
 0.2137
 0.2534
 0.2067
 0.2093
 0.2244
 0.2066
 0.2306
 0.2302
 0.2152
 0.2407
 0.2676
 0.2223
 0.1754
 0.2101
 0.2194
 0.1837
 0.1776
 0.2045
 0.2021
 0.2422
 0.2532
 0.2059
 0.2350
 0.1950
 0.1903
 0.2258
 0.1925
 0.2111
 0.2013
 0.2637
 0.2237
 0.2357
 0.2412
 0.2189
 0.2197
 0.2078
 0.2263
 0.2604
 0.2077
 0.2054
 0.2379
 0.2354
 0.1077
 0.2260
 0.2293
 0.1899
 0.2251
 0.2309
 0.2795
 0.2172
 0.1836
 0.1883
 0.2571
 0.1856
 0.2182
 0.2303
 0.2086
 0.2472
 0.1762
 0.2093
 0.1905
 0.2524
 0.2211
 0.2556
 0.1940
 0.2342
 0.2055
 0.2206
 0.1930
 0.2021
 0.2092
 0.2336
 0.2286
 0.2379
 0.2490
 0.2149
 0.1973
 0.2099
 0.2344
 0.1830
 0.2326
 0.1574
 0.2438
 0.2254
 0.1911
 0.2204
 0.2274
 0.1969
 0.2182
 0.1931
 0.2509
 0.2521
 0.1925
 0.2377
 0.2324
 0.2670
 0.1988
 0.2084
 0.2450
 0.2055
 0.2119
 0.2360
 0.2452
 0.2045
 0.2478
 0.1893
 0.2108
 0.2084
 0.2339
 0.2185
 0.2205
 0.2294
 0.2188
 0.2381
 0.2082
 0.2058
 0.2083
 0.2336
 0.2164
 0.2470
 0.2366
 0.2203
 0.2240
 0.2288
 0.2165
 0.2007
 0.2195
 0.2266
 0.2039
 0.2281
 0.2222
 0.2251
 0.2182
 0.2243
 0.2152
 0.2941
 0.2194
 0.2381
 0.2093
 0.2087
 0.2220
 0.2543
 0.2170
 0.2280
 0.1535
 0.2196
 0.2148
 0.1781
 0.2426
 0.2034
 0.1962
 0.2318
 0.2469
 0.2318
 0.2657
 0.2220
 0.2584
 0.1986
 0.2339
 0.2226
 0.2071
 0.2190
 0.2091
 0.2354
 0.2533
 0.2204
 0.2393
 0.2066
 0.2310
 0.2180
 0.2134
 0.2427
 0.2502
 0.2549
 0.2276
 0.1926
 0.2079
 0.2423
 0.2285
 0.2370
 0.2187
 0.2238
 0.1998
 0.2426
 0.2139
 0.2280
 0.2132
 0.2214
 0.1837
 0.2234
 0.2111
 0.2580
 0.2246
 0.2045
 0.2103
 0.2105
 0.2207
 0.1960
 0.2650
 0.2113
 0.1490
 0.2160
 0.2174
 0.2180
 0.2107
 0.2024
 0.2144
 0.1979
 0.2168
 0.1921
 0.2279
 0.2415
 0.2067
 0.2305
 0.2180
 0.2022
 0.2137
 0.2339
 0.1968
 0.2246
 0.2299
 0.2766
 0.2430
 0.2241
 0.2287
 0.2635
 0.2429
 0.2172
 0.2222
 0.1796
 0.2586
 0.2350
 0.2124
 0.1291
 0.2378
 0.2201
 0.2236
 0.2223
 0.2275
 0.2061
 0.2239
 0.2502
 0.1724
 0.2392
 0.2549
 0.2235
 0.2618
 0.2144
 0.2525
 0.2069
 0.2182
 0.2177
 0.1917
 0.1990
 0.2437
 0.2024
 0.1732
 0.2206
 0.2077
 0.2178
 0.2046
 0.2107
 0.1953
 0.2171
 0.2137
 0.2223
 0.2423
 0.2529
 0.2113
 0.2146
 0.2098
 0.1863
 0.2249
 0.2054
 0.2136
 0.2328
 0.1940
 0.2633
 0.2296
 0.2346
 0.2395
 0.2532
 0.2267
 0.1727
 0.2451
 0.2514
 0.2407
 0.2097
 0.1829
 0.2201
 0.2144
 0.1827
 0.2006
 0.1995
 0.2090
 0.2308
 0.2142
 0.2260
 0.2410
 0.2440
 0.1859
 0.2416
 0.2207
 0.2547
 0.2300
 0.1791
 0.2534
 0.2217
 0.2240
 0.2580
 0.2448
 0.2728
 0.2306
 0.2326
 0.2160
 0.1869
 0.2149
 0.2141
 0.2293
 0.2138
 0.1821
 0.2306
 0.2142
 0.2170
 0.2173
 0.2244
 0.2011
 0.2069
 0.2077
 0.2210
 0.1640
 0.2039
 0.2013
 0.2598
 0.2139
 0.2268
 0.2212
 0.2453
 0.1991
 0.2113
 0.2228
 0.2406
 0.2506
 0.1811
 0.2256
 0.2445
 0.2114
 0.2182
 0.2173
 0.2317
 0.1942
 0.2193
 0.2188
 0.2504
 0.2156
 0.1970
 0.1823
 0.1798
 0.2012
 0.2475
 0.2468
 0.2174
 0.1958
 0.2931
 0.2224
 0.2318
 0.1906
 0.2206
 0.2219
 0.2306
 0.2346
 0.1847
 0.2283
 0.2394
 0.1965
 0.2445
 0.2298
 0.2200
 0.2318
 0.1988
 0.2006
 0.2072
 0.2190
 0.1855
 0.2399
 0.2132
 0.2293
 0.2554
 0.1858
 0.2029
 0.2477
 0.2313
 0.2533
 0.2162
 0.2603
 0.2497
 0.2342
 0.2239
 0.2425
 0.2529
 0.2127
 0.2413
 0.2322
 0.2488
 0.2064
 0.2104
 0.2315
 0.2319
 0.1946
 0.2229
 0.2540
 0.2150
 0.2343
 0.2455
 0.2339
 0.1965
 0.1373
 0.2323
 0.2465
 0.2311
 0.2184
 0.2125
 0.2531
 0.2685
 0.2479
 0.2182
 0.2476
 0.2109
 0.2456
 0.2219
 0.2264
 0.1067
 0.2136
 0.2517
 0.2152
 0.2209
 0.2371
 0.2470
 0.2249
 0.2587
 0.2195
 0.2219
 0.2402
 0.2096
 0.2145
 0.1919
 0.2218
 0.2520
 0.2175
 0.2018
 0.2076
 0.1905
 0.1864
 0.2519
 0.2386
 0.2543
 0.2127
 0.2372
 0.2022
 0.2238
 0.2053
 0.2376
 0.2253
 0.2090
 0.2125
 0.2132
 0.2138
 0.2118
 0.2333
 0.2116
 0.2168
 0.2299
 0.2243
 0.2510
 0.2361
 0.2444
 0.2322
 0.2225
 0.2222
 0.2889
 0.2260
 0.2320
 0.2274
 0.1987
 0.2059
 0.1884
 0.2110
 0.2356
 0.1963
 0.2456
 0.2088
 0.2272
 0.2144
 0.1880
 0.2790
 0.2074
 0.2409
 0.2274
 0.1923
 0.2130
 0.1960
 0.2030
 0.2224
 0.2564
 0.2266
[torch.FloatTensor of size 512]
), ('layer4.0.bn1.bias', 
-0.2308
-0.1761
-0.0937
-0.1531
-0.2710
 0.0623
-0.1275
-0.1807
-0.2341
-0.1118
-0.2037
-0.2124
-0.1777
-0.2047
-0.2234
-0.1634
-0.2535
-0.2236
-0.1890
-0.0604
-0.1507
-0.1673
-0.1038
-0.0869
-0.1487
-0.1072
-0.2277
-0.2578
-0.1459
-0.2438
-0.0822
-0.0670
-0.2275
-0.1194
-0.1724
-0.1303
-0.1969
-0.1824
-0.1828
-0.2419
-0.1568
-0.2165
-0.1494
-0.1369
-0.2610
-0.1064
-0.1072
-0.2473
-0.2436
 0.0821
-0.2224
-0.1274
-0.1208
-0.2416
-0.2440
-0.2850
-0.1709
-0.0610
-0.1020
-0.2847
-0.1033
-0.1511
-0.2111
-0.1366
-0.1880
-0.1200
-0.1406
-0.0585
-0.2489
-0.1252
-0.2507
-0.1156
-0.2484
-0.1786
-0.1779
-0.0934
-0.1713
-0.1485
-0.2675
-0.2188
-0.2363
-0.1913
-0.1279
-0.0951
-0.1292
-0.2095
-0.0947
-0.1581
-0.0322
-0.2477
-0.2326
-0.1597
-0.1734
-0.1965
-0.1505
-0.1968
-0.1474
-0.2446
-0.1911
-0.1056
-0.2804
-0.1589
-0.2636
-0.1227
-0.1381
-0.2229
-0.1304
-0.1155
-0.2480
-0.2459
-0.1495
-0.2223
-0.1299
-0.1368
-0.1800
-0.1983
-0.1409
-0.1889
-0.2367
-0.1810
-0.2503
-0.1134
-0.1526
-0.1088
-0.2068
-0.1979
-0.3152
-0.1867
-0.2190
-0.1651
-0.1980
-0.1649
-0.1655
-0.2005
-0.1904
-0.1375
-0.1580
-0.1180
-0.2084
-0.1959
-0.1726
-0.1365
-0.2819
-0.2339
-0.1224
-0.1820
-0.1551
-0.2123
-0.2434
-0.2113
-0.1955
-0.0400
-0.2069
-0.1053
-0.0640
-0.1653
-0.1333
-0.1650
-0.1765
-0.2277
-0.2257
-0.3134
-0.1804
-0.2576
-0.1218
-0.1448
-0.1630
-0.1979
-0.2166
-0.1266
-0.2090
-0.2591
-0.1744
-0.2015
-0.1987
-0.2108
-0.1445
-0.1957
-0.2194
-0.1616
-0.2196
-0.2030
-0.1548
-0.1879
-0.2119
-0.2266
-0.2398
-0.1248
-0.1703
-0.1851
-0.2178
-0.2146
-0.1445
-0.1997
-0.2087
-0.1335
-0.2209
-0.1668
-0.2400
-0.1854
-0.1425
-0.1872
-0.1327
-0.1915
-0.1509
-0.2687
-0.1463
 0.0642
-0.1317
-0.1294
-0.2200
-0.1736
-0.0859
-0.1758
-0.1168
-0.1331
-0.1143
-0.2396
-0.2387
-0.1612
-0.1937
-0.2153
-0.1131
-0.2001
-0.2297
-0.1337
-0.1681
-0.1954
-0.2385
-0.2743
-0.1668
-0.2160
-0.2624
-0.0336
-0.1844
-0.1711
-0.0712
-0.2424
-0.1956
-0.1081
 0.0251
-0.2100
-0.2119
-0.1683
-0.1855
-0.1292
-0.1379
-0.1589
-0.2316
-0.0971
-0.1738
-0.3015
-0.2092
-0.2583
-0.1207
-0.1675
-0.1414
-0.1788
-0.1578
-0.1389
-0.1479
-0.2305
-0.1171
-0.0974
-0.1838
-0.1314
-0.1654
-0.1416
-0.2133
-0.1350
-0.1867
-0.1387
-0.1258
-0.2143
-0.2643
-0.1093
-0.1990
-0.0902
-0.1015
-0.1762
-0.1492
-0.1645
-0.2281
-0.0933
-0.2688
-0.1438
-0.2457
-0.2086
-0.1512
-0.1650
-0.0634
-0.2289
-0.2759
-0.2881
-0.1929
-0.0951
-0.1687
-0.1828
-0.0931
-0.1432
-0.0879
-0.1999
-0.2027
-0.1946
-0.2239
-0.1918
-0.2296
-0.0841
-0.1543
-0.1537
-0.2310
-0.1915
-0.1400
-0.2135
-0.1951
-0.1890
-0.2463
-0.2388
-0.2843
-0.2063
-0.2206
-0.1795
-0.0844
-0.1268
-0.1717
-0.1666
-0.2106
-0.1249
-0.1600
-0.1590
-0.2129
-0.1590
-0.1076
-0.1489
-0.1598
-0.1725
-0.2238
-0.0797
-0.1211
-0.1014
-0.3295
-0.2025
-0.2122
-0.1465
-0.2333
-0.1234
-0.1399
-0.1313
-0.2006
-0.2161
-0.0881
-0.1829
-0.2658
-0.1843
-0.2071
-0.2781
-0.1495
-0.1349
-0.1869
-0.1729
-0.2520
-0.1773
-0.1660
-0.1046
-0.0922
-0.1342
-0.2262
-0.2800
-0.2520
-0.1484
-0.3502
-0.1395
-0.1667
-0.1266
-0.1416
-0.1684
-0.1808
-0.1973
-0.0871
-0.1645
-0.2660
-0.0910
-0.2093
-0.1474
-0.2139
-0.2306
-0.1006
-0.1725
-0.1280
-0.1845
-0.1330
-0.2659
-0.1924
-0.2043
-0.2196
-0.0951
-0.1187
-0.2538
-0.1601
-0.2201
-0.1537
-0.3002
-0.2502
-0.2787
-0.1976
-0.2943
-0.2462
-0.1518
-0.2764
-0.1788
-0.2599
-0.1789
-0.1199
-0.1739
-0.1968
-0.1160
-0.1700
-0.2527
-0.1620
-0.1617
-0.2226
-0.2406
-0.1234
 0.1028
-0.2205
-0.2132
-0.2048
-0.1483
-0.1198
-0.3155
-0.2804
-0.2631
-0.1542
-0.2127
-0.1408
-0.2889
-0.1590
-0.2144
 0.1268
-0.1653
-0.2186
-0.1706
-0.1840
-0.2003
-0.1963
-0.2261
-0.2132
-0.1472
-0.2435
-0.2382
-0.1526
-0.1516
-0.1387
-0.1780
-0.2600
-0.1514
-0.1096
-0.1209
-0.1138
-0.1090
-0.2111
-0.1948
-0.2490
-0.1558
-0.2552
-0.1097
-0.1932
-0.2010
-0.2124
-0.2218
-0.1598
-0.1624
-0.1398
-0.1577
-0.1947
-0.2213
-0.2049
-0.1922
-0.2600
-0.1811
-0.2457
-0.1937
-0.1836
-0.2131
-0.1730
-0.1829
-0.2494
-0.1952
-0.2291
-0.2097
-0.1549
-0.1311
-0.1169
-0.1512
-0.2128
-0.1026
-0.1979
-0.1322
-0.1768
-0.0914
-0.1160
-0.3000
-0.1729
-0.1652
-0.2310
-0.1146
-0.1912
-0.0916
-0.1530
-0.1449
-0.2726
-0.1986
[torch.FloatTensor of size 512]
), ('layer4.0.bn1.running_mean', 
-0.0912
-0.0533
-0.0311
-0.0377
-0.1038
 0.1002
-0.0716
-0.0175
-0.1376
-0.0591
-0.0424
-0.1047
-0.0323
-0.0752
-0.0669
-0.0981
-0.0466
-0.1938
-0.0397
-0.0600
-0.0516
-0.1327
-0.0844
-0.0703
-0.0699
-0.0619
-0.0482
-0.0683
-0.0420
-0.0981
-0.0825
 0.0096
-0.0538
-0.0531
 0.0024
-0.0362
-0.1095
-0.0482
-0.1140
-0.0465
-0.0752
-0.0859
-0.0754
-0.0868
-0.0804
-0.0404
-0.0612
-0.0720
-0.0775
-0.0107
-0.0609
-0.1103
-0.0558
-0.0487
-0.1003
-0.1435
-0.0682
-0.0319
-0.0194
-0.1473
-0.0475
-0.1532
-0.0851
-0.0815
-0.1305
-0.0033
-0.1087
-0.0310
-0.0462
-0.0538
-0.0464
-0.0298
-0.0751
-0.0839
-0.0710
-0.0107
-0.0528
-0.0802
-0.0515
-0.1086
-0.1148
-0.1626
-0.0174
-0.0865
-0.0163
-0.1078
-0.0321
-0.0751
-0.0785
-0.1490
-0.0860
-0.0927
-0.0048
-0.1020
-0.0337
-0.0426
-0.1054
-0.1026
-0.0908
-0.0695
-0.0950
-0.0571
-0.0750
-0.0832
-0.0689
-0.0514
-0.0571
-0.1330
-0.0938
-0.0334
-0.0942
-0.1160
-0.0380
-0.1106
-0.0505
-0.0989
-0.0556
-0.1144
-0.0938
-0.0546
-0.0832
-0.0474
-0.0949
-0.0512
-0.0787
-0.0472
-0.0836
-0.1279
-0.0835
-0.1467
-0.0632
-0.0654
-0.0864
-0.0828
-0.0453
-0.0893
-0.0377
-0.0199
-0.0321
-0.1220
-0.1034
-0.1166
-0.2086
-0.0326
-0.0966
-0.1073
-0.1267
-0.0764
-0.1002
-0.0059
-0.0733
 0.0190
-0.0399
-0.1364
-0.0449
-0.1357
-0.0863
-0.0048
-0.1247
-0.0403
-0.0460
-0.1116
-0.0831
-0.1233
-0.0405
-0.0806
-0.0406
-0.0864
-0.0824
-0.0689
-0.0940
-0.1091
-0.0343
-0.1006
-0.0466
-0.0362
-0.0648
-0.0214
-0.0656
-0.0833
-0.1082
-0.0509
-0.0319
-0.1004
-0.0249
-0.0223
-0.0791
-0.0236
-0.1035
-0.0484
-0.1338
-0.0760
-0.0496
-0.0652
-0.0409
 0.0162
-0.0782
-0.0296
-0.0898
-0.1196
-0.0884
-0.0343
-0.0611
-0.0168
-0.0845
-0.0504
-0.0513
-0.0452
-0.1654
-0.0854
-0.0298
-0.0831
-0.0749
-0.0581
-0.0495
-0.0697
-0.0618
-0.0124
-0.1237
-0.0694
-0.0385
-0.0740
-0.1069
-0.0551
-0.0668
 0.0162
-0.0729
-0.0625
-0.1447
-0.1059
-0.1135
-0.0427
-0.0849
 0.1810
-0.0336
-0.1219
-0.0229
-0.0696
-0.0849
-0.0864
-0.0627
-0.0364
-0.1276
-0.0829
-0.0549
-0.0995
-0.0807
-0.1207
-0.1700
-0.0135
-0.1033
-0.0881
-0.0502
-0.1070
-0.0858
-0.1201
-0.0273
-0.1140
-0.0930
-0.0290
-0.1168
-0.1031
-0.0624
-0.0752
-0.0594
-0.0644
-0.0622
-0.0478
-0.1030
-0.0101
-0.0580
-0.0716
-0.0706
-0.0822
-0.0745
-0.0656
-0.0708
-0.0390
-0.0440
-0.0847
-0.0650
-0.0732
-0.0828
 0.0081
-0.0678
-0.0676
-0.0839
-0.0938
-0.1250
-0.0580
-0.0651
-0.0858
-0.1744
-0.0827
-0.0688
-0.0557
-0.1411
-0.0324
-0.0747
-0.0049
-0.0579
-0.0620
-0.0974
-0.0177
-0.0785
-0.0493
-0.0799
-0.0927
-0.0536
-0.0749
-0.0510
-0.0325
-0.0377
-0.0908
-0.0533
-0.0663
-0.0347
-0.1148
-0.1023
-0.0757
-0.0788
-0.0727
 0.0137
-0.0361
-0.0376
-0.0031
-0.0413
-0.0371
-0.1046
-0.0675
-0.0133
-0.0268
-0.1168
-0.0339
-0.0337
-0.0610
-0.0750
-0.0483
-0.1256
-0.0764
-0.1013
-0.0350
-0.0795
-0.1056
-0.0591
-0.1226
-0.0511
-0.1085
-0.0804
-0.1470
-0.0247
-0.0577
-0.0553
-0.0841
-0.0858
-0.0269
-0.0661
-0.0532
-0.0017
-0.0803
-0.1071
-0.1024
-0.0748
-0.0915
-0.0708
-0.0510
-0.1761
-0.0855
-0.0320
-0.0414
-0.1144
-0.0160
-0.0911
-0.0893
-0.1041
-0.0818
 0.0064
-0.0311
-0.0876
-0.1169
-0.0619
-0.0297
-0.1720
-0.0280
-0.0411
-0.0634
-0.0672
-0.0662
-0.0534
-0.0751
-0.0434
-0.0856
-0.0615
-0.1190
-0.0380
-0.0401
-0.0677
-0.0403
-0.1016
-0.0825
-0.0719
-0.0328
-0.0552
-0.0606
-0.0969
-0.1048
-0.1382
-0.1430
-0.1117
-0.1306
-0.1336
-0.0349
 0.0183
-0.0883
-0.0755
-0.0790
-0.0587
-0.0936
-0.0535
-0.0492
-0.0797
-0.0537
-0.0224
-0.0556
-0.0826
-0.1516
-0.1276
-0.0698
-0.0390
-0.1156
-0.0605
-0.0990
-0.0490
-0.0928
-0.0465
-0.0902
-0.0789
-0.0665
-0.0673
-0.1139
 0.0065
-0.0045
-0.0955
-0.1642
-0.1654
-0.0767
-0.0634
-0.1365
-0.0689
-0.0660
-0.0246
-0.0855
-0.0316
-0.0108
-0.0757
-0.0890
-0.0743
-0.0472
-0.0931
-0.0499
-0.0962
-0.0959
-0.0729
-0.1101
-0.0762
-0.0611
-0.0859
-0.0877
-0.0694
-0.0359
-0.0380
-0.0665
-0.0587
-0.0883
-0.0634
-0.0821
-0.0603
-0.0503
-0.0530
-0.0643
-0.0988
-0.0783
-0.1407
-0.0907
-0.0684
-0.0511
-0.1883
-0.0669
-0.1097
-0.1224
-0.0528
-0.0788
-0.0327
-0.0859
-0.0612
-0.0258
-0.0471
-0.1316
-0.0444
-0.0081
-0.0258
-0.0571
-0.0491
-0.0910
-0.0147
-0.0172
-0.0770
-0.0349
-0.0221
-0.0761
-0.0705
-0.0968
[torch.FloatTensor of size 512]
), ('layer4.0.bn1.running_var', 
1.00000e-02 *
  1.2554
  0.9705
  1.7260
  1.2110
  1.3709
  1.9027
  1.5536
  1.0881
  1.5535
  1.5466
  1.2842
  1.2892
  1.3238
  1.3719
  1.1412
  1.2892
  1.2344
  2.3482
  1.3682
  1.3383
  1.2345
  1.6889
  1.0840
  1.2115
  1.0833
  1.1258
  1.1302
  1.3591
  1.4170
  1.0069
  1.5809
  1.3008
  1.0894
  1.2942
  1.4473
  1.0746
  1.8634
  1.4523
  1.8066
  1.3481
  1.4880
  1.3246
  1.1909
  1.4982
  1.6408
  1.9365
  1.7145
  1.4163
  1.0315
  1.9662
  1.1451
  1.6269
  1.2152
  0.9877
  1.2149
  1.7821
  1.3166
  1.3701
  1.0278
  1.3338
  1.2260
  2.1786
  1.0929
  1.5256
  1.5825
  1.1734
  1.6908
  1.8128
  1.1322
  1.5524
  1.3210
  0.9590
  1.2446
  1.2291
  1.4763
  1.1675
  1.0750
  1.6376
  1.1680
  1.4622
  1.2770
  1.5485
  1.4455
  1.5807
  1.5990
  1.3507
  1.0296
  1.2849
  1.3519
  1.4191
  1.2965
  1.2891
  1.2761
  1.2072
  1.0982
  1.0432
  1.1689
  1.3915
  2.7086
  1.4248
  1.0512
  1.4939
  1.7444
  1.1405
  1.4354
  1.5232
  1.4767
  1.8394
  1.1354
  1.1977
  1.2269
  1.2893
  1.1225
  1.6270
  1.0948
  1.3205
  1.3690
  1.5521
  1.3364
  1.3565
  1.1743
  1.6560
  1.4282
  1.3998
  1.4110
  1.0843
  1.0467
  1.8549
  0.9421
  1.4867
  1.2338
  1.5929
  1.1873
  1.4191
  1.2454
  1.3012
  1.1758
  1.5342
  1.1902
  1.1873
  1.6530
  1.4691
  2.2607
  1.0559
  2.0366
  1.2028
  1.3325
  1.2770
  1.2007
  1.0903
  1.3321
  1.1941
  1.4095
  2.3838
  1.7055
  1.4155
  1.4004
  1.0931
  1.7781
  1.7719
  1.2404
  1.4344
  1.2545
  1.6132
  1.3327
  1.5196
  1.3128
  1.2211
  1.0767
  1.4683
  1.3753
  1.4061
  1.1876
  1.6255
  1.0576
  1.2344
  1.4226
  1.1028
  1.5934
  2.1853
  1.7110
  1.0429
  0.9627
  1.4102
  1.5855
  1.0465
  1.0822
  1.5937
  1.1364
  1.1980
  1.8625
  1.1894
  2.0341
  1.0710
  1.1107
  1.1602
  1.0019
  1.1492
  1.7048
  1.3239
  1.1569
  1.0548
  1.2683
  1.3352
  1.0837
  1.2155
  1.3081
  1.9542
  1.8063
  1.2510
  1.0361
  1.3281
  1.8172
  1.0097
  1.2689
  1.6019
  1.1385
  0.9667
  1.8653
  1.1609
  1.4736
  0.9180
  1.4310
  1.0286
  1.1739
  1.2381
  1.4240
  1.1972
  1.6521
  1.1259
  1.6829
  1.1954
  1.3095
  3.3034
  1.2895
  1.4510
  1.2103
  1.4370
  1.7303
  1.6495
  1.0403
  1.3926
  1.4882
  1.5088
  1.2884
  1.9437
  1.3036
  1.3958
  1.4873
  1.0403
  1.8995
  1.1349
  1.2432
  1.5968
  1.3851
  1.6119
  1.2911
  1.9896
  2.0499
  1.3485
  1.3616
  1.4996
  1.1421
  1.2525
  1.2485
  1.4759
  1.0808
  1.0535
  1.3018
  1.0897
  1.4009
  1.3215
  1.6919
  1.2845
  1.3138
  1.4904
  1.0326
  1.7061
  1.1932
  1.1845
  1.5583
  1.4373
  1.3169
  1.4262
  1.3673
  1.9451
  1.3209
  1.3126
  2.0413
  1.4405
  1.4063
  1.3343
  1.3696
  1.0416
  1.1986
  1.3158
  1.5044
  1.1568
  1.3592
  1.1140
  1.1958
  1.0825
  1.4392
  0.9980
  1.2273
  1.4902
  1.5057
  1.3064
  1.5941
  1.5058
  1.4491
  1.2546
  0.8113
  1.7515
  1.5189
  1.5029
  1.6489
  1.2658
  1.5577
  1.1788
  1.4776
  1.0931
  1.4385
  1.5278
  1.5784
  1.0449
  1.1708
  1.0974
  1.3341
  1.2435
  1.1254
  1.7504
  1.9539
  1.0557
  1.2760
  1.1937
  1.1038
  1.1069
  1.5807
  1.4209
  1.0751
  1.1047
  1.3838
  1.6360
  1.2898
  0.9540
  1.4960
  1.6380
  1.6647
  1.4261
  1.4743
  1.5782
  1.1447
  1.3022
  1.0738
  0.9593
  1.3691
  1.2790
  1.3117
  1.2166
  1.4405
  1.5081
  1.1784
  1.1128
  1.4035
  1.3357
  1.8756
  1.2507
  0.9598
  1.3156
  1.7030
  1.2631
  1.5085
  1.2073
  1.8727
  1.5434
  1.4963
  1.3970
  1.4555
  1.5307
  1.0550
  1.4074
  1.4755
  1.7221
  1.4399
  1.3620
  1.7533
  1.1155
  1.3742
  1.0169
  1.2341
  1.2842
  1.1830
  1.5485
  1.8679
  1.5084
  1.4319
  1.3928
  1.4799
  1.2820
  1.3346
  1.0837
  1.4301
  1.1897
  1.1620
  1.1585
  1.2805
  1.5908
  1.0436
  1.7097
  1.2082
  1.1769
  1.3914
  1.4525
  1.7499
  1.0338
  1.3653
  1.1982
  1.0918
  1.7259
  1.4122
  1.3467
  1.3817
  2.0042
  0.9670
  1.3376
  1.3753
  1.5245
  1.6171
  1.0295
  1.3441
  1.2331
  1.8540
  1.1925
  1.5890
  1.3429
  1.1429
  1.4616
  2.2963
  1.4850
  1.3224
  1.1822
  1.2042
  1.3257
  1.9121
  1.0237
  1.4832
  1.9687
  1.1656
  1.2234
  1.5836
  1.4104
  1.2201
  1.4747
  1.5269
  1.3544
  1.3997
  1.4495
  1.1121
  1.2300
  2.1821
  1.6841
  1.4201
  1.5997
  0.9786
  1.4444
  1.3035
  1.1334
  1.2411
  1.2504
  1.3434
  1.5537
  1.5218
  1.5008
  1.1456
  1.2297
  1.1953
  1.2803
  1.0602
  1.1731
  1.6397
  1.5250
  1.8784
  1.5593
  1.4450
  1.3314
  2.2991
  1.5075
  1.2705
  1.2134
  1.0134
  1.3955
  1.0993
  1.4828
  1.7073
  1.4014
  1.4960
  2.0358
  1.8693
  1.6144
  1.4351
  1.4555
  1.2048
  1.7600
  1.2564
  1.3967
  1.0625
  1.4943
  1.1984
  1.1786
  1.3930
  1.2928
[torch.FloatTensor of size 512]
), ('layer4.0.conv2.weight', 
( 0 , 0 ,.,.) = 
 -1.8932e-05 -7.4859e-03  9.6491e-03
 -2.2479e-03 -2.0006e-04 -3.6864e-05
 -1.1834e-02 -1.1647e-02 -1.5366e-02

( 0 , 1 ,.,.) = 
  1.2991e-02  1.5447e-02  5.3347e-03
  1.7168e-02  6.0779e-03  1.7853e-02
  1.1593e-02  1.0402e-02  2.2161e-02

( 0 , 2 ,.,.) = 
  1.4707e-02  8.4722e-03  1.4035e-02
  6.1920e-03 -7.7390e-04  9.9112e-03
  2.2165e-03  3.9418e-04  5.2819e-03
    ... 

( 0 ,509,.,.) = 
 -6.9770e-03 -1.1489e-02 -3.1116e-03
  7.9997e-05 -1.0091e-02 -1.3120e-02
 -5.6402e-03 -1.8282e-02 -1.5200e-02

( 0 ,510,.,.) = 
 -1.0741e-03  1.0623e-02  6.1738e-03
  2.5879e-03  2.2386e-02  1.5238e-02
  2.3290e-02  1.9626e-02  1.9223e-02

( 0 ,511,.,.) = 
 -4.5040e-03 -1.9257e-02 -1.3205e-02
  2.3460e-03 -3.5540e-03  1.2624e-03
 -4.6241e-04  1.1147e-03  4.4033e-03
      ⋮  

( 1 , 0 ,.,.) = 
 -6.6627e-03 -5.2057e-03 -1.1148e-02
  1.2389e-03 -3.5889e-03 -4.3800e-03
  9.3645e-04  2.8129e-03 -1.2770e-04

( 1 , 1 ,.,.) = 
  4.9192e-03 -1.9213e-02 -1.4320e-02
 -9.6087e-05 -7.0832e-03 -1.3274e-02
 -2.3235e-04 -1.7716e-03 -3.6681e-03

( 1 , 2 ,.,.) = 
 -7.1228e-03 -6.3466e-03 -4.5244e-03
 -1.3066e-02 -1.0045e-02 -7.2065e-03
 -1.6555e-03  2.6594e-03 -1.3689e-02
    ... 

( 1 ,509,.,.) = 
  2.0937e-03 -1.0113e-02 -7.7149e-03
 -1.3948e-03 -2.9934e-03 -2.8338e-03
  4.5504e-04  1.9351e-03  8.2757e-03

( 1 ,510,.,.) = 
  6.9076e-03  1.1807e-02  1.3122e-02
  6.9059e-03  2.1202e-02  9.4469e-03
  1.2430e-02  2.7566e-02  2.2332e-02

( 1 ,511,.,.) = 
  9.2173e-03  1.9151e-02  1.6164e-02
  1.4868e-02  2.8880e-03  2.1927e-02
  1.2321e-02  1.9790e-02  4.2284e-03
      ⋮  

( 2 , 0 ,.,.) = 
  2.3412e-04 -2.0671e-03 -6.8237e-03
 -4.6504e-03 -2.1855e-03 -1.0251e-02
 -2.9401e-04  1.3175e-03 -1.1234e-02

( 2 , 1 ,.,.) = 
 -4.9503e-03  8.9317e-03  2.6918e-03
  1.0705e-02  1.1473e-02  9.2489e-03
  2.8794e-03  1.1532e-02  4.8235e-03

( 2 , 2 ,.,.) = 
  1.1369e-02  3.0000e-03  1.0115e-02
 -8.0448e-03  4.2242e-03  2.8538e-03
  1.7742e-02  1.0463e-02  1.3950e-02
    ... 

( 2 ,509,.,.) = 
 -1.3608e-02 -7.1644e-03 -2.7831e-03
 -1.3388e-02 -3.1697e-03 -9.3962e-03
 -1.3446e-02 -4.3622e-03 -5.1630e-03

( 2 ,510,.,.) = 
  4.5147e-03 -6.0227e-04  1.3493e-03
  5.1841e-03 -2.9237e-03  8.9890e-03
  9.9917e-04  9.1767e-03  7.3072e-03

( 2 ,511,.,.) = 
  1.6313e-03 -1.0309e-02 -1.9694e-03
 -4.1622e-03 -1.0549e-02 -1.2063e-03
 -8.0016e-03 -1.9246e-02 -2.2881e-03
...     
      ⋮  

(509, 0 ,.,.) = 
  5.3197e-03  1.0739e-02  7.5438e-03
  1.8130e-03 -9.9511e-04 -1.0913e-02
 -1.7927e-03  2.6350e-03 -9.5398e-03

(509, 1 ,.,.) = 
 -5.3403e-03 -1.1985e-02 -6.8201e-03
 -1.6533e-02 -2.1918e-02 -2.8128e-02
 -1.3284e-02 -1.5445e-02 -1.0317e-02

(509, 2 ,.,.) = 
  1.1441e-02  1.4437e-02  1.3181e-02
  1.2144e-02  1.1447e-02  2.8634e-02
  1.0510e-02  1.7770e-02  1.6992e-02
    ... 

(509,509,.,.) = 
 -2.1599e-03  7.8671e-03 -6.1923e-04
  7.5586e-03  1.3950e-02  1.1614e-02
  1.4030e-02  5.9642e-03  1.1040e-02

(509,510,.,.) = 
  4.5254e-03 -2.3593e-04 -5.5669e-04
 -4.1367e-03 -7.9505e-03 -4.5104e-04
 -5.8288e-03 -1.9287e-03 -5.5973e-03

(509,511,.,.) = 
  2.7503e-03  1.2145e-02  1.1896e-02
  7.3540e-03 -8.4273e-04  3.8247e-03
  7.6577e-03 -1.1592e-04 -1.3644e-02
      ⋮  

(510, 0 ,.,.) = 
 -1.0538e-02  5.3589e-03 -8.1633e-03
 -2.0017e-02 -2.0131e-03  8.4960e-05
 -1.0978e-02 -2.2514e-03 -1.1561e-02

(510, 1 ,.,.) = 
 -9.3425e-03 -5.5640e-03 -6.8126e-03
 -1.1227e-02 -9.0562e-03 -1.1572e-02
 -2.5548e-02 -1.0502e-02 -1.0963e-02

(510, 2 ,.,.) = 
 -7.0718e-03 -3.7960e-03 -4.4435e-03
 -3.1759e-03 -5.2850e-03  1.0829e-02
 -1.3921e-02 -3.5413e-03  3.8238e-03
    ... 

(510,509,.,.) = 
 -5.9729e-03 -2.4035e-03 -3.1776e-04
 -4.6231e-03  1.0382e-02  7.6540e-03
 -2.0714e-04  5.5136e-03  1.1262e-02

(510,510,.,.) = 
  1.4789e-02  3.4354e-02  2.9374e-02
  1.5814e-02  5.2303e-03  2.4696e-02
  1.5806e-02  2.5775e-02  3.2018e-02

(510,511,.,.) = 
 -8.7909e-03 -4.0719e-04  6.5272e-03
  8.6746e-03  1.5274e-02  1.8845e-02
  1.0660e-02 -2.4729e-03  7.9977e-03
      ⋮  

(511, 0 ,.,.) = 
 -1.2894e-02 -1.0477e-02 -1.0273e-02
 -7.3072e-03 -1.7766e-02 -1.4866e-02
 -1.4896e-02 -1.6391e-02 -1.1351e-02

(511, 1 ,.,.) = 
  4.2452e-03  9.7323e-03  6.3848e-03
  2.8216e-03  1.4437e-02  6.7594e-03
 -5.7585e-03 -2.9353e-03  6.1200e-03

(511, 2 ,.,.) = 
 -1.1477e-02 -2.0302e-02 -1.5273e-02
 -1.0698e-02 -1.8577e-02 -1.4859e-02
 -1.1331e-02 -2.4588e-02 -2.6877e-02
    ... 

(511,509,.,.) = 
 -9.2459e-03 -8.6223e-03 -1.0176e-02
 -2.1788e-02 -1.7749e-02 -1.8284e-02
 -1.3576e-02 -7.9282e-03 -1.4381e-02

(511,510,.,.) = 
  1.1836e-02 -2.5031e-03  7.5345e-03
 -4.6892e-03 -6.5944e-03 -5.5750e-03
  3.7375e-03 -6.6268e-03  8.4169e-03

(511,511,.,.) = 
  5.9674e-03  1.2748e-02  8.5630e-03
  1.2223e-02  3.4018e-04  5.7421e-03
  2.1898e-03 -9.1239e-03 -4.3494e-03
[torch.FloatTensor of size 512x512x3x3]
), ('layer4.0.bn2.weight', 
 0.2305
 0.2149
 0.2113
 0.2504
 0.1993
 0.2356
 0.2335
 0.2072
 0.2152
 0.2138
 0.1926
 0.1850
 0.2060
 0.2176
 0.2039
 0.2212
 0.1863
 0.1873
 0.1493
 0.1916
 0.1705
 0.2303
 0.1862
 0.1900
 0.1700
 0.2087
 0.1958
 0.2270
 0.1818
 0.1843
 0.1867
 0.1674
 0.1857
 0.1507
 0.2051
 0.2016
 0.1635
 0.2090
 0.2137
 0.1697
 0.1658
 0.1667
 0.1646
 0.1953
 0.1907
 0.1708
 0.1973
 0.1910
 0.2409
 0.2094
 0.1890
 0.1803
 0.1537
 0.2171
 0.2022
 0.2190
 0.2084
 0.2063
 0.1961
 0.2263
 0.2096
 0.1797
 0.1928
 0.2004
 0.2098
 0.1724
 0.1908
 0.1635
 0.2154
 0.1940
 0.1532
 0.2299
 0.2036
 0.1993
 0.1817
 0.1758
 0.1739
 0.1898
 0.1723
 0.1854
 0.2060
 0.2025
 0.2072
 0.1955
 0.1988
 0.1879
 0.2443
 0.1874
 0.1690
 0.2082
 0.2248
 0.2016
 0.2220
 0.2259
 0.2958
 0.2071
 0.1808
 0.2133
 0.1863
 0.1748
 0.2262
 0.1895
 0.1894
 0.2384
 0.2254
 0.1897
 0.2026
 0.1965
 0.1802
 0.1952
 0.1813
 0.1589
 0.1982
 0.1864
 0.1672
 0.1707
 0.1760
 0.2035
 0.1570
 0.2007
 0.1765
 0.1876
 0.2342
 0.1865
 0.2147
 0.1536
 0.2116
 0.1822
 0.2341
 0.1717
 0.2063
 0.2262
 0.1724
 0.1806
 0.1653
 0.2156
 0.1748
 0.2158
 0.2337
 0.2297
 0.1854
 0.1810
 0.1692
 0.1804
 0.1746
 0.1662
 0.1493
 0.1964
 0.2126
 0.2184
 0.1747
 0.1895
 0.2041
 0.2171
 0.1821
 0.1945
 0.1935
 0.1638
 0.1799
 0.2162
 0.1858
 0.1889
 0.1872
 0.1739
 0.1865
 0.2291
 0.1788
 0.1600
 0.1865
 0.1779
 0.1956
 0.1996
 0.2372
 0.1826
 0.2348
 0.2068
 0.1677
 0.1771
 0.2023
 0.1754
 0.1874
 0.2169
 0.1879
 0.1615
 0.1805
 0.2040
 0.1772
 0.2004
 0.1646
 0.1637
 0.1891
 0.2016
 0.1769
 0.1661
 0.1812
 0.2009
 0.1647
 0.1879
 0.1904
 0.1855
 0.1999
 0.1690
 0.1609
 0.1811
 0.1904
 0.1988
 0.2032
 0.2188
 0.1967
 0.1962
 0.2084
 0.1966
 0.1501
 0.1659
 0.1896
 0.2027
 0.2389
 0.1970
 0.2117
 0.1696
 0.2104
 0.2026
 0.1986
 0.1631
 0.2146
 0.1875
 0.1753
 0.1845
 0.2147
 0.1824
 0.2123
 0.1903
 0.1967
 0.2304
 0.2226
 0.1775
 0.1932
 0.2176
 0.2162
 0.2272
 0.2025
 0.2087
 0.2339
 0.1899
 0.1701
 0.2032
 0.1907
 0.2193
 0.1623
 0.1796
 0.1751
 0.1995
 0.1729
 0.2064
 0.1893
 0.2141
 0.2307
 0.1708
 0.2110
 0.2116
 0.1991
 0.1828
 0.2030
 0.1919
 0.2161
 0.1755
 0.1646
 0.1895
 0.1973
 0.1941
 0.1695
 0.1704
 0.1703
 0.2145
 0.1842
 0.2075
 0.1891
 0.2088
 0.2047
 0.1872
 0.1859
 0.2269
 0.1749
 0.1997
 0.2139
 0.1763
 0.1623
 0.1824
 0.1923
 0.1595
 0.1911
 0.2251
 0.1903
 0.1984
 0.1985
 0.2090
 0.1969
 0.1730
 0.1787
 0.1914
 0.2028
 0.1932
 0.1906
 0.1597
 0.1972
 0.1879
 0.1934
 0.1956
 0.1957
 0.2221
 0.2029
 0.2008
 0.1982
 0.2019
 0.1695
 0.1831
 0.1885
 0.1647
 0.2512
 0.1835
 0.1837
 0.1601
 0.1919
 0.2029
 0.1897
 0.1828
 0.2103
 0.2356
 0.2173
 0.2193
 0.1878
 0.2295
 0.1871
 0.2093
 0.1741
 0.2000
 0.1921
 0.2364
 0.1980
 0.1866
 0.2378
 0.2181
 0.1913
 0.1983
 0.1934
 0.1931
 0.1722
 0.2273
 0.1615
 0.1908
 0.2212
 0.1972
 0.2161
 0.1996
 0.1579
 0.2063
 0.2309
 0.2028
 0.1862
 0.1598
 0.1887
 0.2195
 0.1666
 0.1697
 0.1596
 0.2042
 0.1968
 0.1889
 0.1885
 0.2129
 0.2014
 0.1905
 0.1782
 0.1718
 0.2078
 0.2096
 0.1831
 0.1875
 0.2041
 0.1548
 0.2152
 0.1869
 0.2047
 0.2215
 0.2216
 0.1562
 0.2020
 0.2031
 0.2029
 0.1854
 0.2547
 0.1899
 0.2076
 0.2047
 0.1777
 0.1835
 0.1803
 0.2210
 0.1841
 0.1639
 0.2232
 0.1949
 0.2214
 0.1776
 0.1766
 0.2020
 0.1795
 0.1773
 0.1935
 0.2271
 0.2240
 0.1920
 0.1956
 0.2152
 0.2191
 0.2241
 0.1991
 0.1989
 0.1710
 0.2015
 0.1692
 0.2305
 0.1947
 0.1998
 0.2084
 0.1661
 0.2133
 0.2205
 0.2272
 0.2074
 0.2052
 0.2016
 0.1843
 0.2327
 0.1834
 0.1947
 0.1872
 0.2205
 0.1833
 0.1971
 0.2128
 0.1950
 0.1970
 0.2586
 0.1950
 0.2016
 0.1925
 0.1830
 0.2521
 0.1816
 0.2167
 0.2020
 0.1841
 0.1686
 0.1702
 0.2297
 0.2118
 0.1780
 0.1762
 0.1848
 0.1886
 0.2115
 0.1706
 0.1859
 0.1870
 0.1786
 0.1864
 0.2016
 0.2209
 0.2564
 0.2433
 0.2004
 0.2107
 0.1850
 0.2278
 0.1843
 0.2140
 0.1943
 0.1779
 0.1881
 0.1833
 0.2342
 0.1804
 0.1929
 0.1840
 0.2287
 0.2168
 0.2022
 0.1960
 0.2238
 0.2029
 0.1857
 0.1682
 0.1811
 0.1620
 0.1618
 0.1965
 0.2047
 0.2026
 0.1740
 0.2286
 0.2168
 0.1971
 0.1870
 0.1993
 0.1862
 0.1752
 0.1608
 0.1918
 0.1864
 0.1858
 0.2140
[torch.FloatTensor of size 512]
), ('layer4.0.bn2.bias', 
-0.0920
-0.1411
-0.0523
-0.1000
-0.0655
-0.0419
-0.1133
-0.1290
-0.0927
-0.0645
-0.1192
-0.0788
-0.0687
-0.1556
-0.0255
-0.0789
 0.0948
-0.0681
-0.0261
-0.0788
-0.0278
-0.0883
-0.0057
-0.0421
-0.0658
-0.1372
-0.0525
-0.1194
-0.0540
-0.0300
-0.0428
-0.0118
-0.0833
-0.0122
-0.0490
-0.1302
-0.0240
-0.0574
-0.1482
-0.0501
-0.0729
 0.0158
-0.0015
-0.0640
-0.0950
-0.0278
-0.1279
-0.1025
-0.1357
-0.0999
-0.1115
-0.0461
 0.0211
-0.0588
-0.0943
-0.0604
-0.0678
-0.0874
-0.1174
-0.1086
-0.1377
-0.1077
-0.0324
-0.0861
-0.1247
-0.0451
-0.0815
-0.0504
-0.1273
-0.1229
 0.0236
-0.0837
-0.1405
-0.0841
-0.0814
-0.0070
-0.0227
-0.1178
-0.0563
-0.0657
-0.0765
-0.0352
-0.1057
-0.0809
-0.0972
-0.0249
-0.1255
-0.0635
-0.0048
-0.0715
-0.0836
-0.0836
-0.0979
-0.1462
 0.1644
-0.0837
-0.0521
-0.1234
-0.0658
-0.0507
-0.1238
-0.0623
-0.0378
-0.1392
-0.0761
-0.0807
-0.0691
-0.0864
-0.0855
-0.0811
-0.0410
-0.0344
-0.1100
-0.0622
-0.0542
-0.0276
-0.0495
-0.1398
 0.0192
-0.0646
-0.0371
-0.1021
-0.1268
-0.0757
-0.0842
 0.0272
-0.1082
-0.0564
-0.0999
-0.0314
-0.1207
-0.1000
-0.0443
-0.0269
-0.0157
-0.0984
-0.0686
-0.0944
-0.1282
-0.1197
-0.0288
-0.0699
-0.0645
-0.0793
-0.0071
 0.0143
-0.0077
-0.0424
-0.0866
-0.0759
-0.0394
-0.0554
-0.0826
-0.1193
-0.0756
-0.1268
-0.1065
-0.0184
-0.0582
-0.1538
-0.0839
-0.0419
-0.0611
-0.0987
-0.0625
-0.1325
-0.0745
-0.0129
-0.0712
-0.0540
-0.0978
-0.0934
-0.1396
-0.0202
-0.1340
-0.1441
-0.0187
-0.0233
-0.0898
-0.0757
-0.0705
-0.0869
-0.0323
-0.0032
-0.0387
-0.0660
-0.0546
-0.0747
-0.0288
-0.0242
-0.0237
-0.0854
-0.0164
-0.0403
-0.0312
-0.0937
 0.0083
-0.0876
-0.0751
-0.0616
-0.0942
 0.0030
-0.0197
-0.0916
-0.0770
-0.0762
-0.1011
-0.1555
-0.0717
-0.0655
-0.0798
-0.0913
 0.0551
-0.0367
-0.0476
-0.1010
-0.1732
-0.0893
-0.0949
-0.0406
-0.1215
-0.0659
-0.0475
-0.0127
-0.1182
-0.0670
-0.0296
-0.0488
-0.0878
-0.0531
-0.1500
-0.0672
-0.0498
-0.1030
-0.1057
-0.0329
-0.0739
-0.1282
-0.0883
-0.0943
-0.1215
-0.1259
-0.0920
-0.0817
-0.0514
-0.0981
-0.0437
-0.0480
-0.0360
-0.0168
-0.0270
-0.0403
 0.0360
-0.0803
-0.0643
-0.0936
-0.1147
-0.0406
-0.0845
-0.0918
-0.0862
-0.0447
-0.0856
-0.0856
-0.0730
-0.0556
-0.0097
-0.1006
-0.0404
-0.0959
-0.0164
-0.0347
-0.0679
-0.1526
-0.0696
-0.0992
-0.0355
-0.0879
-0.1195
-0.0695
-0.0712
-0.1060
-0.0404
-0.0859
-0.1243
-0.0703
-0.0242
-0.0749
-0.0749
-0.0370
-0.0636
-0.1228
-0.0362
-0.0893
-0.0773
-0.0952
-0.0758
-0.0296
-0.0277
-0.0231
-0.0857
-0.0686
-0.0757
-0.0032
-0.1003
-0.0969
-0.1314
-0.0920
-0.0502
-0.0404
-0.0885
-0.0732
-0.0898
-0.0381
-0.0278
-0.0769
-0.0987
-0.0394
-0.0657
-0.0722
-0.0677
-0.0231
-0.0517
-0.1037
-0.1019
-0.0786
-0.1079
-0.1017
-0.1091
-0.0757
-0.0643
-0.1064
 0.0049
-0.0878
-0.0316
-0.1030
-0.0307
-0.0887
-0.0491
-0.0868
-0.1664
-0.0781
-0.1024
-0.0784
-0.0856
-0.0620
-0.0185
-0.1327
 0.1189
-0.0366
-0.1077
-0.0728
-0.1745
-0.0796
-0.0099
-0.0921
-0.1571
-0.0776
-0.0894
 0.0031
-0.0316
-0.0923
-0.0630
-0.0335
 0.0053
-0.1365
-0.0495
-0.0483
-0.0799
-0.0961
-0.0891
-0.0895
-0.0404
-0.0077
-0.0855
-0.1146
-0.0017
-0.0684
-0.1385
-0.0034
-0.0727
-0.0797
-0.0410
-0.1159
-0.0847
 0.0431
-0.1066
-0.1185
-0.1068
-0.0188
-0.1109
-0.1102
-0.0771
-0.0912
-0.0125
-0.0542
-0.0775
-0.0888
-0.0149
 0.0045
-0.1373
-0.0681
-0.1069
-0.0021
-0.0243
-0.1225
-0.0420
-0.0223
-0.0701
-0.1700
-0.0927
-0.0235
-0.0778
-0.0536
-0.1118
-0.1384
-0.0801
-0.0604
-0.0305
-0.0559
-0.0430
-0.1875
-0.1258
-0.1089
-0.1129
-0.0064
-0.0996
-0.0769
-0.0957
-0.1080
-0.0845
-0.0717
-0.0649
-0.0916
-0.0466
-0.0266
-0.0842
-0.1211
-0.0492
-0.1211
-0.0475
-0.0963
-0.0814
-0.1344
-0.0734
-0.0633
-0.0490
-0.0544
-0.0992
-0.0239
-0.0544
-0.0797
-0.1091
-0.0313
-0.0757
-0.1236
-0.0523
-0.0285
-0.0144
-0.0110
-0.0588
-0.1018
-0.0468
-0.0478
-0.0548
-0.0536
-0.0439
-0.0712
-0.1086
-0.0954
-0.1090
-0.1015
-0.0882
-0.1089
-0.0655
-0.0356
-0.1559
-0.0522
-0.0405
-0.1131
-0.0420
-0.1802
-0.0664
-0.0831
-0.0385
-0.1161
-0.0675
-0.1112
-0.1245
-0.1202
-0.0801
-0.0174
 0.0027
-0.0580
-0.0321
-0.0002
-0.0598
-0.0986
-0.0915
-0.0513
-0.0970
-0.0760
-0.0715
-0.0224
-0.0913
-0.0334
-0.0487
 0.0372
-0.0371
-0.0260
-0.0177
-0.1683
[torch.FloatTensor of size 512]
), ('layer4.0.bn2.running_mean', 
-0.0966
-0.0860
-0.1041
-0.0383
-0.0867
 0.0135
-0.0510
-0.1047
-0.0428
-0.0823
-0.0585
-0.0710
-0.0837
-0.0846
-0.0373
-0.0892
-0.0905
-0.0551
-0.0455
-0.0474
-0.0561
-0.0665
-0.0469
-0.1182
-0.0181
-0.0515
-0.0558
-0.0689
-0.0715
-0.0673
 0.0042
-0.0619
-0.0550
-0.0139
-0.0511
-0.0530
-0.1087
-0.0515
-0.0626
-0.0348
-0.0352
-0.1458
-0.0453
-0.0645
-0.0453
-0.0670
-0.0658
-0.1058
-0.0645
-0.0352
-0.0245
-0.0813
-0.0130
-0.0378
-0.0705
-0.0490
 0.0047
-0.0492
-0.0848
-0.0575
-0.0734
-0.0071
-0.0590
-0.0180
-0.1237
-0.0203
-0.0624
-0.0279
-0.0537
-0.0898
-0.0210
-0.0861
-0.0185
-0.1260
-0.0650
-0.0158
-0.0517
-0.0741
-0.0806
-0.0638
-0.0542
-0.0616
-0.0910
-0.0776
-0.0929
-0.0617
-0.0655
-0.0826
-0.0732
-0.0923
-0.0223
-0.0212
-0.0661
-0.0195
 0.3134
-0.1162
-0.0296
-0.0985
-0.0585
-0.0360
-0.0857
-0.0042
-0.0511
-0.0749
-0.0710
-0.0396
-0.0852
-0.0760
-0.0458
-0.1039
-0.0280
-0.0449
-0.0733
-0.0107
-0.0641
-0.0236
-0.1356
-0.0791
-0.1107
-0.0617
-0.0340
-0.0761
-0.0482
-0.0515
-0.0271
-0.1111
-0.0635
 0.0074
-0.0503
-0.0461
-0.0434
-0.0752
-0.0545
-0.0421
-0.0394
-0.0447
 0.0337
-0.0931
-0.0677
-0.0717
-0.0190
-0.0458
-0.0681
-0.0227
-0.0321
-0.0210
-0.0124
-0.0029
-0.0731
-0.0642
 0.0240
-0.0737
-0.1149
-0.0863
-0.0464
-0.0299
-0.1046
-0.0057
-0.0399
-0.0798
-0.0610
-0.0392
-0.0831
-0.0710
-0.0422
-0.0655
-0.0631
-0.0090
-0.0488
-0.0289
-0.0683
-0.0274
-0.1405
-0.0729
-0.0957
-0.1094
-0.0935
-0.0734
-0.0655
-0.0806
-0.1074
-0.0808
-0.0616
-0.0669
-0.0933
-0.0240
-0.0594
-0.0505
-0.0367
-0.0464
-0.0573
-0.0659
-0.0169
-0.0380
-0.0221
-0.0817
-0.0924
-0.0529
-0.0624
-0.0572
-0.0371
-0.0826
-0.0629
-0.0071
-0.0514
-0.0544
-0.0502
-0.1198
-0.0431
-0.0652
-0.1201
-0.0670
-0.0653
-0.0496
-0.0808
-0.0443
-0.0690
-0.0293
-0.0686
-0.0478
-0.0648
-0.0778
-0.0525
-0.0369
-0.1237
-0.0750
-0.0524
-0.0373
-0.0868
-0.0620
-0.0912
-0.0771
-0.0675
-0.0528
-0.0390
-0.0251
-0.0679
-0.0975
-0.0938
-0.0856
-0.0618
-0.0713
-0.0767
-0.0495
-0.0559
-0.0340
-0.0701
-0.0629
-0.0444
-0.0603
-0.0193
-0.0658
-0.0420
-0.0659
-0.0154
-0.0827
-0.0506
-0.0537
-0.0644
-0.0569
-0.0593
-0.0653
-0.0062
-0.1323
-0.0694
-0.0497
-0.0102
-0.0880
-0.0833
-0.0746
-0.0647
-0.0106
-0.0143
-0.1003
-0.0583
-0.0653
-0.1251
-0.0674
-0.1058
-0.0520
-0.0731
-0.1058
-0.0663
-0.0812
-0.0956
-0.0958
-0.0495
-0.0852
-0.0334
-0.0250
-0.0446
-0.0728
-0.0798
-0.0879
-0.0644
-0.0463
-0.0380
-0.0444
-0.0354
-0.1011
-0.0835
-0.0429
-0.0693
-0.0560
-0.0765
-0.0406
-0.0976
-0.0319
-0.0161
-0.0324
-0.0442
-0.0912
-0.0852
-0.0290
-0.0723
-0.0695
-0.0834
-0.0466
-0.0493
-0.1004
-0.0496
 0.0202
-0.1087
-0.0588
-0.0724
-0.0304
-0.0675
-0.1218
-0.0856
-0.0725
-0.0551
-0.0598
-0.0749
-0.0612
-0.0330
-0.0821
-0.1236
-0.0857
-0.0962
-0.0529
-0.0603
-0.0374
-0.0632
-0.0990
-0.0760
-0.0436
-0.0765
-0.1079
 0.1009
-0.0346
-0.0646
-0.0692
-0.0789
-0.1139
-0.0270
-0.1271
-0.0586
-0.0936
-0.0899
-0.0483
-0.0561
-0.0420
-0.0974
-0.0358
-0.0369
-0.0505
-0.0373
-0.0670
-0.0347
-0.0741
-0.0251
-0.0398
-0.0793
-0.0562
-0.0457
-0.1093
-0.0969
-0.0336
-0.0370
 0.0518
-0.0661
 0.0066
-0.0518
-0.0939
-0.1032
-0.0174
-0.0583
-0.0847
-0.0542
-0.0911
-0.0217
-0.0916
-0.0752
-0.0605
-0.0859
-0.0062
-0.0063
-0.0882
-0.0177
-0.0481
-0.0547
-0.0978
-0.0640
 0.0020
-0.0438
-0.0892
-0.0601
-0.0323
-0.1144
-0.0666
-0.0751
-0.0991
-0.0385
-0.0802
-0.0847
-0.0715
-0.0941
-0.0849
-0.0304
-0.0211
-0.0551
-0.1100
-0.0662
-0.1011
-0.0746
-0.0319
-0.0522
-0.1238
-0.0416
-0.0615
-0.0370
-0.0640
-0.0218
-0.0600
-0.1228
-0.0839
-0.0136
-0.0502
-0.0610
-0.0722
-0.0483
-0.0313
-0.0663
-0.1113
-0.0851
-0.0805
-0.0295
-0.0681
-0.0805
-0.0548
-0.0527
-0.0743
-0.0724
-0.0295
-0.0445
-0.0499
-0.0754
-0.0552
 0.0158
-0.0192
-0.0779
-0.0245
 0.0063
-0.0715
-0.0902
-0.1228
-0.0515
-0.1005
-0.1090
-0.0849
-0.0672
-0.0308
-0.0835
-0.0087
-0.0841
-0.0462
-0.0926
-0.0165
-0.0549
-0.0795
-0.0281
-0.0854
-0.0125
-0.0166
-0.0358
-0.0574
-0.0484
-0.0251
-0.0827
-0.1131
-0.0709
-0.0518
-0.0122
-0.0803
-0.0117
-0.0715
-0.0652
-0.0747
-0.0596
-0.0557
-0.0352
-0.0753
-0.0504
-0.0428
-0.1150
-0.0242
 0.0136
-0.0699
-0.0696
-0.0578
-0.0804
-0.0839
[torch.FloatTensor of size 512]
), ('layer4.0.bn2.running_var', 
 0.0135
 0.0128
 0.0154
 0.0185
 0.0140
 0.0175
 0.0150
 0.0118
 0.0130
 0.0179
 0.0104
 0.0110
 0.0160
 0.0122
 0.0145
 0.0144
 0.0241
 0.0126
 0.0110
 0.0090
 0.0122
 0.0202
 0.0157
 0.0132
 0.0116
 0.0102
 0.0161
 0.0119
 0.0118
 0.0135
 0.0195
 0.0125
 0.0103
 0.0100
 0.0160
 0.0136
 0.0116
 0.0137
 0.0116
 0.0121
 0.0107
 0.0173
 0.0098
 0.0132
 0.0098
 0.0127
 0.0113
 0.0128
 0.0122
 0.0107
 0.0098
 0.0138
 0.0114
 0.0160
 0.0133
 0.0148
 0.0132
 0.0114
 0.0112
 0.0122
 0.0088
 0.0109
 0.0128
 0.0124
 0.0166
 0.0105
 0.0131
 0.0114
 0.0147
 0.0114
 0.0121
 0.0218
 0.0114
 0.0154
 0.0105
 0.0125
 0.0117
 0.0108
 0.0133
 0.0133
 0.0138
 0.0152
 0.0112
 0.0124
 0.0139
 0.0153
 0.0141
 0.0133
 0.0146
 0.0132
 0.0155
 0.0135
 0.0152
 0.0124
 0.2141
 0.0176
 0.0117
 0.0153
 0.0100
 0.0107
 0.0132
 0.0132
 0.0131
 0.0127
 0.0151
 0.0150
 0.0130
 0.0131
 0.0140
 0.0134
 0.0123
 0.0107
 0.0113
 0.0117
 0.0111
 0.0111
 0.0151
 0.0123
 0.0133
 0.0140
 0.0138
 0.0115
 0.0118
 0.0103
 0.0139
 0.0162
 0.0137
 0.0107
 0.0135
 0.0101
 0.0104
 0.0141
 0.0110
 0.0102
 0.0124
 0.0108
 0.0124
 0.0141
 0.0179
 0.0137
 0.0125
 0.0121
 0.0121
 0.0107
 0.0100
 0.0146
 0.0097
 0.0130
 0.0127
 0.0178
 0.0109
 0.0109
 0.0118
 0.0122
 0.0134
 0.0099
 0.0124
 0.0126
 0.0103
 0.0129
 0.0093
 0.0143
 0.0144
 0.0101
 0.0106
 0.0147
 0.0134
 0.0119
 0.0147
 0.0118
 0.0105
 0.0120
 0.0122
 0.0153
 0.0135
 0.0136
 0.0140
 0.0117
 0.0138
 0.0130
 0.0113
 0.0130
 0.0141
 0.0116
 0.0117
 0.0104
 0.0111
 0.0117
 0.0114
 0.0112
 0.0154
 0.0122
 0.0121
 0.0107
 0.0125
 0.0107
 0.0138
 0.0111
 0.0109
 0.0116
 0.0090
 0.0146
 0.0118
 0.0105
 0.0114
 0.0126
 0.0104
 0.0132
 0.0132
 0.0119
 0.0133
 0.0131
 0.0143
 0.0097
 0.0170
 0.0110
 0.0102
 0.0116
 0.0122
 0.0108
 0.0117
 0.0150
 0.0198
 0.0107
 0.0114
 0.0139
 0.0112
 0.0099
 0.0136
 0.0113
 0.0136
 0.0123
 0.0126
 0.0130
 0.0123
 0.0134
 0.0127
 0.0106
 0.0115
 0.0114
 0.0106
 0.0145
 0.0156
 0.0113
 0.0117
 0.0104
 0.0109
 0.0170
 0.0098
 0.0135
 0.0111
 0.0186
 0.0202
 0.0134
 0.0121
 0.0144
 0.0121
 0.0115
 0.0126
 0.0138
 0.0136
 0.0125
 0.0128
 0.0124
 0.0152
 0.0101
 0.0117
 0.0117
 0.0151
 0.0125
 0.0123
 0.0105
 0.0127
 0.0118
 0.0126
 0.0110
 0.0159
 0.0106
 0.0105
 0.0140
 0.0123
 0.0194
 0.0151
 0.0163
 0.0134
 0.0120
 0.0117
 0.0103
 0.0112
 0.0124
 0.0129
 0.0117
 0.0174
 0.0151
 0.0119
 0.0117
 0.0095
 0.0122
 0.0140
 0.0147
 0.0134
 0.0124
 0.0127
 0.0131
 0.0122
 0.0108
 0.0126
 0.0119
 0.0135
 0.0171
 0.0122
 0.0127
 0.0141
 0.0152
 0.0113
 0.0115
 0.0108
 0.0105
 0.0261
 0.0104
 0.0140
 0.0104
 0.0149
 0.0130
 0.0114
 0.0099
 0.0134
 0.0154
 0.0154
 0.0146
 0.0099
 0.0160
 0.0192
 0.0139
 0.0129
 0.0121
 0.0148
 0.0177
 0.0155
 0.0106
 0.0123
 0.0144
 0.0098
 0.0124
 0.0119
 0.0143
 0.0126
 0.0120
 0.0236
 0.0168
 0.0128
 0.0129
 0.0122
 0.0114
 0.0148
 0.0154
 0.0139
 0.0159
 0.0114
 0.0132
 0.0137
 0.0139
 0.0113
 0.0150
 0.0128
 0.0107
 0.0152
 0.0114
 0.0120
 0.0134
 0.0102
 0.0121
 0.0119
 0.0134
 0.0142
 0.0158
 0.0152
 0.0118
 0.0099
 0.0111
 0.0173
 0.0110
 0.0168
 0.0172
 0.0167
 0.0126
 0.0118
 0.0113
 0.0146
 0.0177
 0.0179
 0.0107
 0.0125
 0.0149
 0.0154
 0.0118
 0.0117
 0.0167
 0.0132
 0.0143
 0.0123
 0.0140
 0.0132
 0.0119
 0.0127
 0.0133
 0.0135
 0.0131
 0.0134
 0.0101
 0.0176
 0.0157
 0.0106
 0.0193
 0.0135
 0.0106
 0.0130
 0.0130
 0.0119
 0.0139
 0.0133
 0.0115
 0.0121
 0.0107
 0.0111
 0.0129
 0.0126
 0.0173
 0.0162
 0.0141
 0.0137
 0.0144
 0.0128
 0.0169
 0.0132
 0.0187
 0.0123
 0.0124
 0.0136
 0.0106
 0.0189
 0.0110
 0.0111
 0.0190
 0.0124
 0.0185
 0.0118
 0.0124
 0.0242
 0.0133
 0.0162
 0.0123
 0.0085
 0.0128
 0.0115
 0.0199
 0.0177
 0.0129
 0.0126
 0.0150
 0.0132
 0.0125
 0.0107
 0.0117
 0.0172
 0.0151
 0.0153
 0.0161
 0.0158
 0.0182
 0.0132
 0.0107
 0.0152
 0.0110
 0.0162
 0.0143
 0.0119
 0.0103
 0.0150
 0.0090
 0.0139
 0.0112
 0.0105
 0.0112
 0.0124
 0.0178
 0.0123
 0.0113
 0.0104
 0.0153
 0.0121
 0.0151
 0.0142
 0.0129
 0.0098
 0.0135
 0.0114
 0.0106
 0.0143
 0.0125
 0.0112
 0.0165
 0.0147
 0.0167
 0.0112
 0.0133
 0.0105
 0.0131
 0.0120
 0.0125
 0.0184
 0.0114
[torch.FloatTensor of size 512]
), ('layer4.0.conv3.weight', 
( 0  , 0  ,.,.) = 
 -9.6159e-04

( 0  , 1  ,.,.) = 
  3.2276e-03

( 0  , 2  ,.,.) = 
 -1.1370e-02
      ... 

( 0  ,509 ,.,.) = 
 -5.3855e-03

( 0  ,510 ,.,.) = 
 -3.4504e-03

( 0  ,511 ,.,.) = 
 -1.1038e-02
        ⋮  

( 1  , 0  ,.,.) = 
 -5.0824e-03

( 1  , 1  ,.,.) = 
  4.1970e-02

( 1  , 2  ,.,.) = 
 -9.7838e-03
      ... 

( 1  ,509 ,.,.) = 
 -5.5636e-03

( 1  ,510 ,.,.) = 
 -2.6047e-02

( 1  ,511 ,.,.) = 
 -1.5763e-03
        ⋮  

( 2  , 0  ,.,.) = 
  1.3788e-02

( 2  , 1  ,.,.) = 
 -1.7034e-02

( 2  , 2  ,.,.) = 
 -8.1966e-03
      ... 

( 2  ,509 ,.,.) = 
  9.9102e-03

( 2  ,510 ,.,.) = 
 -1.8799e-03

( 2  ,511 ,.,.) = 
 -1.4468e-02
 ...      
        ⋮  

(2045, 0  ,.,.) = 
  1.3086e-02

(2045, 1  ,.,.) = 
 -1.2238e-02

(2045, 2  ,.,.) = 
 -1.1528e-02
      ... 

(2045,509 ,.,.) = 
 -1.1848e-02

(2045,510 ,.,.) = 
  2.5280e-03

(2045,511 ,.,.) = 
  1.7351e-02
        ⋮  

(2046, 0  ,.,.) = 
  6.5744e-03

(2046, 1  ,.,.) = 
  1.6127e-03

(2046, 2  ,.,.) = 
 -1.4042e-02
      ... 

(2046,509 ,.,.) = 
  1.8197e-02

(2046,510 ,.,.) = 
 -9.0295e-03

(2046,511 ,.,.) = 
  2.5450e-05
        ⋮  

(2047, 0  ,.,.) = 
  1.8406e-02

(2047, 1  ,.,.) = 
 -2.0255e-03

(2047, 2  ,.,.) = 
  1.2983e-02
      ... 

(2047,509 ,.,.) = 
  8.1636e-03

(2047,510 ,.,.) = 
  1.4235e-02

(2047,511 ,.,.) = 
 -1.1777e-02
[torch.FloatTensor of size 2048x512x1x1]
), ('layer4.0.bn3.weight', 
 0.3144
 0.4253
 0.3966
   ⋮   
 0.3195
 0.3653
 0.3153
[torch.FloatTensor of size 2048]
), ('layer4.0.bn3.bias', 
-0.0592
-0.0920
-0.0703
   ⋮   
-0.0619
-0.0642
-0.0663
[torch.FloatTensor of size 2048]
), ('layer4.0.bn3.running_mean', 
-0.0036
-0.0296
-0.0169
   ⋮   
-0.0236
-0.0182
-0.0095
[torch.FloatTensor of size 2048]
), ('layer4.0.bn3.running_var', 
1.00000e-02 *
  0.1044
  0.1561
  0.1707
    ⋮   
  0.0927
  0.1417
  0.1030
[torch.FloatTensor of size 2048]
), ('layer4.0.downsample.0.weight', 
( 0  , 0  ,.,.) = 
 -8.6648e-04

( 0  , 1  ,.,.) = 
 -4.8517e-03

( 0  , 2  ,.,.) = 
  1.6167e-03
      ... 

( 0  ,1021,.,.) = 
 -5.7033e-03

( 0  ,1022,.,.) = 
 -7.9826e-03

( 0  ,1023,.,.) = 
  4.8046e-03
        ⋮  

( 1  , 0  ,.,.) = 
  1.3921e-03

( 1  , 1  ,.,.) = 
  4.0305e-03

( 1  , 2  ,.,.) = 
  2.1422e-03
      ... 

( 1  ,1021,.,.) = 
 -3.4758e-03

( 1  ,1022,.,.) = 
 -5.4846e-03

( 1  ,1023,.,.) = 
 -1.5740e-03
        ⋮  

( 2  , 0  ,.,.) = 
 -9.9421e-03

( 2  , 1  ,.,.) = 
 -7.5823e-03

( 2  , 2  ,.,.) = 
  1.1618e-03
      ... 

( 2  ,1021,.,.) = 
 -1.5797e-02

( 2  ,1022,.,.) = 
 -3.1329e-03

( 2  ,1023,.,.) = 
  6.4237e-04
 ...      
        ⋮  

(2045, 0  ,.,.) = 
 -3.5696e-03

(2045, 1  ,.,.) = 
  1.1944e-02

(2045, 2  ,.,.) = 
  5.7097e-03
      ... 

(2045,1021,.,.) = 
  3.6094e-03

(2045,1022,.,.) = 
  1.2801e-02

(2045,1023,.,.) = 
 -9.6219e-03
        ⋮  

(2046, 0  ,.,.) = 
 -2.9107e-03

(2046, 1  ,.,.) = 
  2.7539e-03

(2046, 2  ,.,.) = 
 -9.8598e-03
      ... 

(2046,1021,.,.) = 
 -7.4393e-03

(2046,1022,.,.) = 
 -3.4664e-03

(2046,1023,.,.) = 
 -5.0499e-04
        ⋮  

(2047, 0  ,.,.) = 
  2.6623e-03

(2047, 1  ,.,.) = 
 -7.9957e-05

(2047, 2  ,.,.) = 
 -6.7350e-03
      ... 

(2047,1021,.,.) = 
  1.8782e-02

(2047,1022,.,.) = 
 -1.1642e-02

(2047,1023,.,.) = 
  1.7035e-02
[torch.FloatTensor of size 2048x1024x1x1]
), ('layer4.0.downsample.1.weight', 
 0.2392
 0.2747
 0.2593
   ⋮   
 0.1803
 0.2701
 0.2526
[torch.FloatTensor of size 2048]
), ('layer4.0.downsample.1.bias', 
-0.0592
-0.0920
-0.0703
   ⋮   
-0.0619
-0.0642
-0.0663
[torch.FloatTensor of size 2048]
), ('layer4.0.downsample.1.running_mean', 
1.00000e-02 *
 -0.7887
  0.3358
 -3.3562
    ⋮   
  0.1231
 -2.4153
 -0.2799
[torch.FloatTensor of size 2048]
), ('layer4.0.downsample.1.running_var', 
1.00000e-02 *
  0.1204
  0.1699
  0.1594
    ⋮   
  0.1037
  0.1664
  0.1263
[torch.FloatTensor of size 2048]
), ('layer4.1.conv1.weight', 
( 0  , 0  ,.,.) = 
 -2.3042e-02

( 0  , 1  ,.,.) = 
 -1.0654e-03

( 0  , 2  ,.,.) = 
 -1.4822e-02
      ... 

( 0  ,2045,.,.) = 
 -1.0086e-02

( 0  ,2046,.,.) = 
 -1.9499e-02

( 0  ,2047,.,.) = 
 -7.8702e-03
        ⋮  

( 1  , 0  ,.,.) = 
 -9.9767e-04

( 1  , 1  ,.,.) = 
  2.5386e-02

( 1  , 2  ,.,.) = 
 -6.4168e-03
      ... 

( 1  ,2045,.,.) = 
  8.4395e-03

( 1  ,2046,.,.) = 
  1.6303e-02

( 1  ,2047,.,.) = 
 -2.1171e-02
        ⋮  

( 2  , 0  ,.,.) = 
 -2.3977e-02

( 2  , 1  ,.,.) = 
 -9.8452e-03

( 2  , 2  ,.,.) = 
 -1.5662e-02
      ... 

( 2  ,2045,.,.) = 
  1.1590e-03

( 2  ,2046,.,.) = 
  1.5331e-02

( 2  ,2047,.,.) = 
  3.8369e-04
 ...      
        ⋮  

(509 , 0  ,.,.) = 
  1.7750e-02

(509 , 1  ,.,.) = 
 -1.4810e-02

(509 , 2  ,.,.) = 
 -1.6176e-02
      ... 

(509 ,2045,.,.) = 
  5.5513e-03

(509 ,2046,.,.) = 
 -2.5036e-04

(509 ,2047,.,.) = 
  2.6863e-03
        ⋮  

(510 , 0  ,.,.) = 
 -9.6415e-03

(510 , 1  ,.,.) = 
 -2.5846e-02

(510 , 2  ,.,.) = 
 -2.3286e-03
      ... 

(510 ,2045,.,.) = 
 -3.2576e-03

(510 ,2046,.,.) = 
  6.6953e-03

(510 ,2047,.,.) = 
  9.2437e-04
        ⋮  

(511 , 0  ,.,.) = 
 -2.9495e-02

(511 , 1  ,.,.) = 
 -1.3904e-02

(511 , 2  ,.,.) = 
  1.7718e-02
      ... 

(511 ,2045,.,.) = 
  5.8521e-03

(511 ,2046,.,.) = 
 -2.1756e-03

(511 ,2047,.,.) = 
  5.9336e-03
[torch.FloatTensor of size 512x2048x1x1]
), ('layer4.1.bn1.weight', 
 0.1883
 0.1929
 0.1722
 0.1995
 0.2047
 0.1484
 0.2234
 0.2010
 0.2167
 0.1620
 0.2173
 0.2034
 0.1911
 0.1993
 0.1897
 0.2066
 0.1610
 0.2019
 0.1661
 0.2191
 0.1953
 0.2158
 0.1758
 0.1892
 0.1988
 0.1930
 0.1983
 0.1917
 0.1208
 0.1820
 0.2051
 0.1855
 0.2272
 0.1806
 0.1986
 0.1549
 0.1988
 0.1805
 0.1893
 0.2005
 0.1205
 0.2000
 0.2006
 0.1859
 0.1777
 0.2021
 0.1956
 0.1984
 0.1804
 0.1698
 0.1778
 0.2402
 0.1941
 0.1478
 0.2405
 0.1858
 0.1830
 0.1940
 0.1762
 0.1859
 0.1839
 0.1921
 0.1911
 0.1899
 0.2004
 0.2023
 0.2065
 0.1467
 0.2139
 0.1940
 0.1722
 0.2108
 0.2365
 0.1816
 0.1881
 0.1785
 0.2052
 0.1741
 0.1995
 0.1176
 0.2117
 0.2369
 0.1927
 0.1659
 0.2141
 0.1948
 0.2041
 0.1917
 0.1789
 0.1935
 0.2229
 0.2075
 0.2065
 0.2110
 0.2315
 0.1943
 0.2047
 0.1997
 0.1753
 0.1716
 0.1699
 0.2100
 0.2135
 0.1264
 0.2131
 0.1920
 0.2076
 0.1921
 0.2494
 0.1861
 0.1798
 0.2135
 0.1938
 0.1785
 0.1803
 0.1821
 0.2265
 0.2002
 0.2203
 0.1889
 0.1840
 0.2109
 0.1664
 0.2084
 0.1995
 0.2206
 0.1867
 0.2291
 0.1865
 0.2148
 0.1888
 0.1721
 0.1783
 0.2278
 0.1685
 0.1566
 0.1986
 0.1688
 0.2177
 0.1528
 0.2164
 0.1976
 0.1834
 0.1809
 0.1574
 0.2018
 0.1500
 0.2070
 0.1970
 0.2081
 0.1823
 0.1524
 0.1669
 0.1800
 0.2092
 0.2109
 0.2174
 0.1905
 0.1865
 0.2017
 0.1894
 0.1715
 0.1594
 0.1964
 0.1928
 0.2172
 0.1571
 0.1939
 0.2076
 0.1639
 0.1974
 0.1710
 0.1737
 0.1715
 0.1973
 0.1734
 0.2051
 0.1607
 0.2172
 0.1741
 0.1906
 0.1666
 0.2037
 0.1811
 0.2156
 0.1757
 0.2011
 0.2239
 0.1988
 0.1864
 0.1762
 0.2001
 0.2119
 0.1882
 0.1765
 0.2195
 0.1927
 0.1821
 0.2113
 0.1852
 0.1967
 0.1640
 0.2162
 0.1862
 0.2337
 0.2053
 0.2239
 0.1976
 0.1816
 0.1742
 0.1955
 0.1895
 0.1980
 0.2017
 0.1875
 0.2095
 0.1609
 0.2120
 0.1884
 0.1850
 0.1960
 0.1946
 0.2281
 0.2243
 0.1955
 0.1739
 0.1873
 0.2030
 0.2232
 0.2129
 0.2113
 0.1713
 0.1716
 0.2092
 0.1675
 0.1933
 0.2155
 0.1708
 0.2910
 0.2183
 0.2137
 0.1597
 0.1623
 0.1942
 0.1933
 0.1994
 0.2081
 0.1709
 0.2086
 0.1723
 0.1988
 0.2267
 0.1525
 0.2066
 0.1985
 0.1991
 0.2151
 0.1992
 0.1953
 0.2021
 0.1910
 0.1562
 0.1908
 0.1940
 0.2012
 0.1639
 0.1813
 0.1922
 0.2069
 0.2029
 0.1810
 0.2013
 0.2289
 0.1753
 0.1903
 0.2010
 0.1735
 0.1927
 0.1778
 0.1566
 0.2137
 0.2115
 0.2008
 0.1452
 0.2030
 0.1951
 0.2144
 0.1700
 0.1740
 0.1923
 0.2065
 0.1952
 0.1956
 0.1963
 0.2029
 0.1903
 0.1947
 0.1843
 0.1524
 0.2351
 0.2145
 0.1946
 0.2068
 0.2209
 0.1818
 0.1975
 0.2146
 0.1711
 0.1896
 0.2159
 0.1639
 0.1933
 0.1891
 0.2010
 0.1904
 0.2125
 0.2009
 0.2021
 0.1907
 0.2011
 0.2110
 0.2124
 0.2081
 0.2000
 0.1993
 0.1942
 0.1906
 0.1597
 0.1864
 0.2299
 0.1985
 0.1885
 0.2063
 0.2190
 0.2049
 0.1985
 0.2245
 0.1703
 0.1830
 0.2289
 0.2233
 0.2167
 0.2134
 0.2067
 0.1896
 0.2273
 0.2128
 0.1744
 0.1729
 0.1881
 0.1977
 0.1870
 0.2293
 0.2096
 0.2084
 0.1719
 0.1820
 0.2178
 0.1730
 0.2247
 0.2005
 0.1662
 0.1910
 0.2039
 0.2005
 0.2140
 0.1773
 0.2006
 0.1789
 0.2083
 0.2103
 0.2176
 0.2083
 0.1967
 0.2045
 0.1930
 0.2174
 0.2035
 0.2048
 0.2150
 0.1748
 0.2088
 0.1942
 0.2017
 0.2005
 0.2062
 0.1550
 0.2164
 0.2188
 0.2184
 0.1995
 0.2013
 0.1814
 0.2171
 0.1713
 0.1748
 0.2097
 0.2030
 0.1834
 0.1745
 0.2056
 0.1102
 0.2029
 0.1939
 0.1883
 0.1958
 0.2321
 0.1908
 0.1931
 0.1850
 0.1619
 0.1737
 0.1969
 0.2003
 0.2111
 0.2038
 0.2443
 0.1703
 0.1968
 0.1806
 0.1960
 0.1671
 0.1669
 0.1809
 0.2043
 0.1669
 0.2074
 0.2083
 0.1966
 0.2112
 0.1387
 0.1578
 0.1885
 0.1623
 0.2281
 0.1741
 0.2191
 0.1818
 0.1747
 0.2279
 0.1544
 0.1994
 0.1631
 0.2035
 0.2112
 0.2306
 0.2192
 0.1840
 0.1861
 0.1983
 0.2054
 0.1941
 0.1736
 0.1582
 0.1910
 0.1996
 0.2013
 0.1346
 0.2133
 0.1965
 0.2107
 0.2094
 0.2128
 0.1867
 0.2032
 0.1839
 0.2013
 0.1576
 0.1915
 0.0942
 0.1863
 0.2241
 0.1700
 0.2073
 0.2288
 0.1893
 0.1787
 0.2078
 0.1931
 0.1742
 0.2169
 0.2123
 0.1998
 0.2030
 0.2164
 0.2194
 0.1859
 0.1661
 0.1480
 0.2020
 0.1856
 0.2230
 0.1520
 0.1884
 0.2191
 0.2050
 0.1878
 0.2023
 0.1712
 0.1919
 0.1543
 0.2069
 0.1889
 0.1998
 0.1995
 0.1996
 0.1709
 0.1762
 0.2532
 0.2080
 0.2378
 0.2146
[torch.FloatTensor of size 512]
), ('layer4.1.bn1.bias', 
-0.1270
-0.1093
-0.0724
-0.1474
-0.1615
-0.0255
-0.1078
-0.1203
-0.1714
-0.0687
-0.2019
-0.1363
-0.1361
-0.1479
-0.1245
-0.1355
-0.0564
-0.1522
-0.0921
-0.1775
-0.1214
-0.1484
-0.0819
-0.1174
-0.1434
-0.1288
-0.1327
-0.1067
 0.0906
-0.1104
-0.1524
-0.1216
-0.2097
-0.0653
-0.1615
-0.0297
-0.1304
-0.0882
-0.1176
-0.0989
 0.0505
-0.1436
-0.1350
-0.0774
-0.1000
-0.1587
-0.1616
-0.1678
-0.1012
-0.0694
-0.0678
-0.2256
-0.1368
-0.0530
-0.2195
-0.1572
-0.1242
-0.1302
-0.1118
-0.1105
-0.1069
-0.0899
-0.1178
-0.1454
-0.1917
-0.1499
-0.1357
-0.0212
-0.1930
-0.1668
-0.0708
-0.1822
-0.1990
-0.1022
-0.1129
-0.0926
-0.1389
-0.0759
-0.1552
 0.0855
-0.1423
-0.2011
-0.1186
-0.0731
-0.1669
-0.1538
-0.1470
-0.1402
-0.1318
-0.1145
-0.1726
-0.1697
-0.1653
-0.1906
-0.2122
-0.1471
-0.1393
-0.1483
-0.0730
-0.0873
-0.0857
-0.1750
-0.1410
-0.0249
-0.1648
-0.1170
-0.1406
-0.1208
-0.1760
-0.0760
-0.0675
-0.1420
-0.1126
-0.0958
-0.0468
-0.0712
-0.1573
-0.1242
-0.1463
-0.1524
-0.1183
-0.1702
-0.0700
-0.1416
-0.1207
-0.1828
-0.1418
-0.2284
-0.1138
-0.1773
-0.1082
-0.0563
-0.0733
-0.1796
-0.0890
-0.0491
-0.1036
-0.0640
-0.1486
-0.0051
-0.1689
-0.1525
-0.0967
-0.0758
-0.0634
-0.1797
 0.0391
-0.1898
-0.1371
-0.1734
-0.0985
-0.0157
-0.0580
-0.0776
-0.1429
-0.1747
-0.1626
-0.1181
-0.1103
-0.1766
-0.1194
-0.0921
-0.0345
-0.1089
-0.1329
-0.2053
-0.0441
-0.1517
-0.1679
-0.0943
-0.0993
-0.0326
-0.0782
-0.1087
-0.1264
-0.0594
-0.1471
-0.0256
-0.1493
-0.1153
-0.1132
-0.0738
-0.0767
-0.0778
-0.1421
-0.0926
-0.1565
-0.1702
-0.1402
-0.1580
-0.0982
-0.1588
-0.1513
-0.1258
-0.1151
-0.2177
-0.0863
-0.0832
-0.1720
-0.0578
-0.1328
-0.0668
-0.2070
-0.1327
-0.1686
-0.1246
-0.1734
-0.1167
-0.0848
-0.1023
-0.1100
-0.1171
-0.1741
-0.1482
-0.1303
-0.1643
-0.0459
-0.1893
-0.1396
-0.0647
-0.1718
-0.0985
-0.2080
-0.1632
-0.1556
-0.0841
-0.0921
-0.1580
-0.2452
-0.1629
-0.1599
-0.1027
-0.0592
-0.1652
-0.0855
-0.1060
-0.1445
-0.0597
-0.3197
-0.1471
-0.1718
-0.0662
-0.0333
-0.1416
-0.1289
-0.1417
-0.1418
-0.0914
-0.1410
-0.0899
-0.1059
-0.1487
 0.1150
-0.1461
-0.1552
-0.1600
-0.1682
-0.1460
-0.1237
-0.1376
-0.1289
-0.0366
-0.1097
-0.1278
-0.1048
-0.0590
-0.1104
-0.1489
-0.1846
-0.1524
-0.0906
-0.1270
-0.1108
-0.1203
-0.1576
-0.1721
-0.0661
-0.1171
-0.0848
-0.0444
-0.1497
-0.1645
-0.1889
-0.0157
-0.1383
-0.1269
-0.1272
-0.0517
-0.0551
-0.0915
-0.1306
-0.1438
-0.1273
-0.0932
-0.1233
-0.1355
-0.1480
-0.1195
 0.0233
-0.1817
-0.1629
-0.1484
-0.1246
-0.1549
-0.1048
-0.1873
-0.1755
-0.0963
-0.1107
-0.1498
-0.0633
-0.1275
-0.0736
-0.1554
-0.1129
-0.1995
-0.1580
-0.1594
-0.1223
-0.1807
-0.1656
-0.1846
-0.1346
-0.1607
-0.1345
-0.1140
-0.1222
-0.0582
-0.0575
-0.1731
-0.1556
-0.1090
-0.1702
-0.1770
-0.1547
-0.1325
-0.1633
-0.0602
-0.0854
-0.2301
-0.2028
-0.1684
-0.1436
-0.1264
-0.0829
-0.1749
-0.1688
-0.0969
-0.1090
-0.1039
-0.1183
-0.1212
-0.1733
-0.1276
-0.1702
-0.0645
-0.1143
-0.1957
-0.0956
-0.1968
-0.1542
-0.0826
-0.1523
-0.1390
-0.1294
-0.1754
-0.0654
-0.1359
-0.0955
-0.1525
-0.1614
-0.1144
-0.1182
-0.1552
-0.1545
-0.1213
-0.1353
-0.1265
-0.0974
-0.1509
-0.0756
-0.1797
-0.1375
-0.1034
-0.1176
-0.1566
-0.0334
-0.0262
-0.1513
-0.2117
-0.1166
-0.1409
-0.0864
-0.1996
-0.0782
-0.0743
-0.1586
-0.1561
-0.0860
-0.0688
-0.1398
 0.0232
-0.1254
-0.1005
-0.1245
-0.1279
-0.2275
-0.1261
-0.1210
-0.1300
-0.0605
-0.0750
-0.1311
-0.1176
-0.1693
-0.1546
-0.2275
-0.0751
-0.1654
-0.1151
-0.1623
-0.0655
-0.0910
-0.0957
-0.1316
-0.0814
-0.1528
-0.1825
-0.1704
-0.0955
 0.0185
-0.0552
-0.1287
-0.0487
-0.2256
-0.0889
-0.1843
-0.1447
-0.1026
-0.2055
-0.0261
-0.1398
-0.0930
-0.1772
-0.1733
-0.2061
-0.1637
-0.1221
-0.0906
-0.1391
-0.1621
-0.1452
-0.0933
-0.0451
-0.0759
-0.1456
-0.1350
 0.0080
-0.1592
-0.1537
-0.1664
-0.1203
-0.1491
-0.1220
-0.1480
-0.1087
-0.1044
-0.0441
-0.1063
 0.1173
-0.1216
-0.1920
-0.1041
-0.1778
-0.2004
-0.1242
-0.0666
-0.1841
-0.1281
-0.0680
-0.1676
-0.1586
-0.1287
-0.1400
-0.1780
-0.1871
-0.0859
-0.0780
-0.0360
-0.1559
-0.1082
-0.1888
-0.0434
-0.0955
-0.1704
-0.1289
-0.1071
-0.1394
-0.0673
-0.1828
-0.0189
-0.0892
-0.1085
-0.1113
-0.1325
-0.1506
-0.1036
-0.1144
-0.2774
-0.1427
-0.2034
-0.1701
[torch.FloatTensor of size 512]
), ('layer4.1.bn1.running_mean', 
-0.1916
-0.1306
-0.5641
-0.3095
-0.0476
-0.1280
-0.6904
-0.3189
-0.2290
-0.1937
-0.2151
-0.3759
-0.1178
-0.4778
-0.3844
-0.3949
-0.4586
-0.2718
-0.1495
-0.2085
-0.4034
-0.5979
-0.2436
-0.3732
-0.5118
-0.3175
-0.0823
-0.4268
 0.1110
-0.1543
-0.3296
-0.1627
-0.6110
-0.4550
-0.0947
-0.2373
-0.4838
-0.2989
-0.1281
-0.1316
-0.5678
-0.1748
-0.3893
-0.2226
-0.0438
-0.3578
-0.2707
-0.1047
-0.2956
-0.2912
-0.1723
-0.3856
-0.2686
-0.1331
-0.2595
-0.4313
-0.2581
-0.3145
-0.1826
-0.2213
-0.3110
-0.4257
-0.0651
-0.1588
-0.1752
-0.2847
-0.2550
-0.3575
-0.2131
-0.3219
-0.2403
-0.2684
-0.3321
-0.2623
-0.1260
-0.2453
-0.1503
-0.2003
-0.5001
-0.5735
-0.3854
-0.4602
-0.3255
-0.0219
-0.3219
-0.3146
-0.4514
-0.3374
-0.3363
-0.2938
-0.3243
-0.2466
-0.2531
-0.3218
-0.3456
-0.2869
-0.4742
-0.2069
-0.4052
-0.2845
-0.2011
-0.2002
-0.4396
 0.2039
-0.4025
-0.3400
-0.2309
-0.3430
-0.8784
-0.2911
-0.3113
-0.3315
-0.2267
-0.2784
-0.1644
-0.1508
-0.2600
-0.0602
-0.4942
-0.1756
-0.2939
-0.4634
 0.1083
-0.2495
-0.3828
-0.1149
-0.1369
-0.2924
-0.2697
-0.2267
-0.0061
-0.2221
-0.3635
-0.2385
 0.1407
-0.2482
-0.3663
-0.3305
-0.3835
-0.2477
-0.3301
-0.2417
-0.4701
-0.3696
-0.1309
-0.2579
-0.0068
-0.3316
-0.2088
-0.2530
-0.1690
-0.4290
 0.0113
-0.2340
-0.3048
-0.2381
-0.4807
-0.1484
-0.4196
-0.2097
-0.1705
-0.2245
-0.0408
-0.4523
-0.2048
-0.0440
-0.0518
-0.3951
-0.1402
-0.3956
-0.4746
-0.2468
-0.0816
-0.1996
-0.2358
-0.4301
-0.3169
-0.4539
-0.2523
-0.1709
-0.2637
-0.3043
-0.3942
-0.4219
-0.5014
-0.0864
-0.3084
-0.2481
-0.2761
-0.3373
-0.1074
-0.1979
-0.2380
-0.0683
-0.4937
-0.2402
-0.3695
-0.3205
-0.1773
-0.5511
-0.3852
 0.0840
-0.3279
-0.1553
-0.5834
-0.0869
-0.3466
-0.4002
-0.2281
-0.3121
-0.3747
-0.4463
-0.2288
-0.2879
-0.3597
-0.3336
-0.4257
-0.3058
-0.1894
-0.4695
-0.3000
-0.4415
-0.4415
-0.5963
-0.1701
-0.5069
-0.3511
-0.3473
-0.2431
-0.6099
-0.3942
-0.0692
-0.4169
-0.1921
-0.0376
-0.2174
-0.4048
-0.2467
-0.4477
-0.3038
-0.1639
-0.3351
-0.3051
-0.3271
-0.4293
-0.3123
-0.4228
-0.4803
-0.2102
-0.1488
-0.4285
-0.4802
-0.6597
-0.1237
-0.2807
-0.4138
-0.2796
-0.2064
-0.2639
-0.3320
-0.2211
-0.0354
-0.2808
-0.3054
-0.2530
-0.3766
-0.1612
-0.3407
-0.1695
-0.1517
-0.3619
-0.4944
-0.3448
 0.0110
-0.2927
-0.1794
-0.3716
-0.1494
-0.1564
-0.2128
-0.4684
-0.3394
-0.1767
-0.2328
-0.1716
-0.1706
-0.2368
-0.5409
-0.3306
-0.2455
-0.1724
-0.1890
-0.2731
-0.3863
-0.4310
-0.2231
-0.1632
-0.3782
-0.3480
-0.5960
-0.3459
-0.3254
-0.3210
-0.4443
-0.0561
-0.1076
-0.2247
-0.0831
-0.1265
-0.2069
-0.2827
-0.2169
-0.4966
-0.1960
-0.5378
-0.2227
-0.3416
-0.0963
-0.1152
-0.1513
-0.3361
-0.3008
-0.1899
-0.4101
-0.3203
-0.3666
-0.2999
-0.0227
-0.3871
-0.1448
-0.0961
-0.3862
-0.2356
-0.1971
-0.3269
-0.2663
-0.2703
-0.3037
-0.4482
-0.2373
-0.2085
-0.4905
-0.3682
-0.1377
-0.3728
-0.1682
-0.2662
-0.2573
-0.4425
-0.1306
-0.4533
-0.0853
-0.4848
-0.6100
-0.1855
-0.2286
-0.2314
-0.3269
-0.2759
-0.1712
-0.3477
-0.2599
-0.1935
-0.3545
-0.2549
-0.3711
-0.2215
-0.4127
-0.3041
-0.2785
-0.3002
-0.4924
-0.4576
-0.1940
-0.1747
-0.3943
-0.1624
-0.4150
-0.4671
-0.2632
-0.2898
-0.3650
-0.2903
-0.4056
-0.2483
-0.2658
-0.3968
 1.1245
-0.4665
-0.3606
-0.3618
-0.2035
-0.1902
-0.2862
-0.1652
-0.3545
-0.2139
-0.3952
-0.2270
-0.3055
-0.5144
-0.1946
-0.5027
-0.1801
-0.2216
-0.3767
-0.2083
-0.3480
-0.3151
-0.2899
-0.3096
-0.3367
-0.1509
-0.2810
-0.3435
-0.3112
-0.6745
-0.1870
-0.2800
-0.0987
-0.5102
-0.3537
-0.2966
-0.1254
-0.4257
-0.2827
-0.4604
-0.1326
-0.0531
-0.4443
-0.2984
-0.2466
-0.2889
-0.0507
-0.3956
-0.2214
-0.2470
 0.0179
-0.1148
-0.2788
-0.2084
-0.3171
 0.0818
-0.1374
-0.2381
-0.2841
-0.3192
-0.1191
-0.3014
-0.2988
-0.3089
-0.2157
-0.3689
-0.2456
-0.1560
-0.2691
-0.2926
-0.3562
-0.1723
-0.3062
-0.2653
-0.3786
-0.2843
-0.2251
-0.2893
-0.3448
-0.4087
-0.3136
-0.3328
 0.0810
-0.2619
-0.2100
-0.2799
-0.0629
-0.3481
-0.1726
-0.4228
-0.2629
-0.1183
-0.1843
-0.1327
-0.0503
-0.4530
-0.2646
-0.6293
-0.3712
-0.5354
-0.0242
-0.4270
-0.2626
-0.1450
-0.1967
-0.3662
-0.3285
-0.3543
-0.4536
-0.4482
-0.5073
-0.2856
-0.2582
-0.3196
-0.6497
-0.1488
-0.4381
-0.4054
-0.4962
-0.3996
-0.2046
-0.6387
-0.3663
-0.3522
-0.2240
[torch.FloatTensor of size 512]
), ('layer4.1.bn1.running_var', 
 0.1325
 0.1279
 0.2074
 0.1361
 0.1104
 0.1734
 0.2856
 0.1352
 0.1366
 0.1699
 0.1303
 0.1666
 0.1198
 0.1491
 0.1442
 0.1494
 0.1618
 0.1432
 0.1240
 0.1467
 0.1689
 0.2166
 0.1509
 0.1131
 0.1684
 0.1505
 0.1262
 0.1703
 0.3506
 0.1315
 0.1334
 0.1559
 0.1538
 0.1919
 0.1004
 0.1736
 0.1503
 0.1475
 0.1596
 0.1537
 0.3063
 0.1271
 0.1416
 0.1881
 0.1357
 0.1300
 0.1286
 0.1018
 0.1233
 0.1692
 0.1429
 0.1144
 0.1331
 0.1711
 0.1221
 0.1169
 0.1462
 0.1248
 0.1525
 0.1406
 0.1397
 0.2245
 0.1715
 0.1055
 0.1141
 0.1306
 0.1517
 0.1614
 0.1081
 0.1149
 0.1747
 0.1337
 0.1425
 0.1440
 0.1429
 0.1539
 0.1431
 0.1675
 0.1381
 0.2710
 0.1487
 0.1390
 0.1358
 0.1625
 0.1358
 0.1102
 0.1597
 0.1479
 0.1157
 0.1359
 0.1109
 0.1414
 0.1244
 0.1144
 0.1370
 0.1200
 0.1574
 0.1182
 0.1648
 0.1577
 0.1465
 0.1400
 0.1579
 0.1511
 0.1644
 0.1259
 0.1371
 0.1401
 0.2371
 0.1572
 0.1652
 0.1851
 0.1341
 0.1304
 0.1795
 0.1831
 0.1455
 0.1523
 0.1514
 0.1114
 0.1313
 0.1444
 0.1795
 0.1322
 0.1497
 0.1373
 0.1399
 0.1023
 0.1508
 0.1269
 0.1321
 0.1592
 0.1568
 0.1437
 0.1443
 0.1393
 0.1448
 0.1627
 0.1795
 0.2085
 0.1309
 0.1337
 0.1424
 0.1683
 0.1485
 0.1149
 0.3014
 0.1299
 0.1098
 0.1253
 0.1603
 0.1870
 0.1588
 0.1684
 0.1463
 0.1346
 0.1612
 0.1452
 0.1849
 0.1155
 0.1306
 0.1405
 0.1524
 0.1714
 0.1287
 0.1086
 0.1642
 0.1447
 0.1288
 0.1638
 0.1847
 0.1786
 0.1255
 0.1356
 0.1291
 0.1726
 0.1398
 0.1998
 0.1381
 0.1212
 0.1292
 0.1655
 0.1559
 0.1906
 0.1742
 0.1467
 0.1295
 0.1359
 0.1300
 0.1372
 0.1558
 0.1445
 0.1123
 0.1445
 0.1528
 0.1205
 0.1844
 0.1779
 0.1340
 0.2004
 0.1668
 0.1632
 0.1283
 0.1352
 0.1863
 0.1219
 0.1434
 0.1518
 0.1257
 0.1281
 0.1509
 0.1831
 0.1359
 0.1279
 0.1385
 0.1262
 0.1636
 0.1144
 0.1221
 0.1659
 0.1160
 0.1887
 0.1090
 0.1640
 0.1477
 0.1667
 0.1686
 0.1259
 0.0889
 0.1646
 0.1535
 0.1234
 0.1519
 0.1226
 0.1742
 0.1242
 0.1722
 0.1705
 0.1252
 0.1336
 0.1541
 0.1678
 0.1912
 0.1529
 0.1554
 0.1366
 0.1509
 0.1471
 0.1286
 0.1502
 0.1372
 0.1799
 0.4355
 0.1250
 0.1372
 0.1235
 0.1272
 0.1427
 0.1121
 0.1587
 0.1188
 0.1672
 0.1462
 0.1367
 0.1668
 0.1660
 0.1602
 0.1245
 0.1100
 0.1258
 0.1557
 0.1789
 0.2363
 0.1065
 0.1150
 0.1085
 0.1515
 0.1426
 0.1824
 0.1774
 0.1345
 0.1570
 0.1115
 0.1579
 0.1600
 0.1528
 0.1295
 0.1522
 0.1677
 0.1684
 0.1332
 0.1532
 0.1659
 0.1432
 0.1531
 0.1265
 0.1150
 0.1746
 0.1937
 0.1689
 0.1415
 0.1374
 0.1584
 0.2043
 0.1334
 0.1017
 0.1366
 0.1332
 0.1534
 0.1313
 0.1438
 0.1568
 0.1914
 0.1299
 0.2336
 0.1072
 0.1434
 0.1330
 0.1298
 0.1181
 0.1440
 0.1452
 0.1555
 0.1384
 0.1336
 0.1295
 0.1359
 0.1454
 0.2076
 0.1534
 0.1244
 0.1464
 0.1341
 0.1447
 0.1356
 0.1321
 0.1679
 0.1802
 0.1666
 0.0946
 0.1284
 0.1500
 0.1547
 0.1328
 0.1640
 0.1305
 0.1255
 0.1399
 0.1314
 0.1464
 0.1695
 0.1435
 0.1481
 0.1731
 0.1195
 0.1668
 0.1195
 0.1247
 0.1454
 0.1093
 0.1372
 0.1619
 0.1179
 0.1500
 0.1339
 0.1282
 0.1544
 0.1627
 0.1245
 0.1506
 0.1299
 0.2000
 0.1665
 0.1243
 0.1207
 0.1305
 0.1249
 0.2046
 0.1788
 0.1494
 0.1414
 0.1300
 0.1356
 0.1892
 0.1302
 0.1281
 0.1766
 0.2740
 0.1592
 0.1290
 0.1559
 0.1330
 0.1255
 0.1237
 0.1297
 0.1496
 0.1133
 0.1325
 0.1437
 0.1434
 0.1712
 0.1657
 0.1584
 0.1411
 0.1599
 0.1622
 0.1052
 0.1404
 0.1690
 0.1327
 0.1777
 0.1630
 0.1292
 0.1401
 0.1176
 0.1151
 0.1371
 0.1143
 0.1254
 0.1420
 0.1539
 0.1420
 0.1744
 0.1247
 0.1329
 0.1418
 0.1339
 0.1113
 0.1273
 0.1751
 0.2181
 0.1405
 0.1377
 0.1482
 0.1187
 0.1356
 0.1226
 0.1128
 0.1364
 0.1476
 0.1900
 0.1258
 0.1169
 0.1077
 0.1192
 0.1357
 0.1431
 0.1236
 0.1784
 0.1426
 0.1413
 0.1396
 0.1270
 0.1782
 0.1505
 0.1402
 0.1253
 0.1635
 0.1747
 0.1507
 0.1262
 0.1768
 0.1591
 0.1057
 0.1195
 0.1377
 0.1825
 0.1563
 0.1595
 0.2198
 0.1374
 0.1189
 0.1653
 0.1230
 0.1357
 0.1348
 0.1755
 0.1266
 0.1577
 0.1697
 0.1461
 0.1079
 0.1428
 0.1460
 0.1679
 0.1190
 0.1961
 0.1335
 0.1525
 0.1587
 0.1174
 0.1079
 0.1702
 0.1718
 0.1481
 0.1530
 0.1385
 0.1610
 0.1429
 0.1080
 0.1500
 0.2604
 0.1317
 0.1800
 0.1555
 0.1431
 0.1281
 0.1101
 0.1362
 0.1466
 0.1301
 0.1019
[torch.FloatTensor of size 512]
), ('layer4.1.conv2.weight', 
( 0 , 0 ,.,.) = 
 -7.4608e-03 -6.8952e-03 -2.7633e-03
 -1.3707e-02 -1.5581e-02 -4.6684e-03
  4.3251e-03 -3.6882e-03 -1.8153e-02

( 0 , 1 ,.,.) = 
  5.7387e-03 -6.5001e-03 -6.2026e-03
 -1.8367e-02 -4.4026e-03  2.4085e-02
 -6.3548e-03  1.5282e-03  7.7844e-03

( 0 , 2 ,.,.) = 
  2.9963e-02 -2.1762e-03  9.3708e-03
  1.9061e-02 -1.6008e-02 -1.0292e-02
  1.3134e-02 -1.1607e-02 -8.8721e-03
    ... 

( 0 ,509,.,.) = 
  4.6632e-03  3.9309e-03 -2.7805e-03
 -6.1339e-03  1.1966e-02  7.8287e-03
  6.8404e-03  1.7147e-02  3.1882e-03

( 0 ,510,.,.) = 
 -1.4701e-02  4.6946e-03  1.0421e-02
 -1.1623e-02 -7.3134e-03  1.7751e-02
 -7.0204e-03  9.8106e-04  8.4165e-03

( 0 ,511,.,.) = 
 -3.5283e-03 -4.1191e-04  1.6734e-02
  7.7735e-03  6.5134e-03  1.1710e-02
 -4.4011e-03  1.5796e-02  2.0804e-02
      ⋮  

( 1 , 0 ,.,.) = 
  3.8508e-03 -2.2047e-03 -3.4727e-03
  1.3044e-02  4.5113e-03  7.0148e-03
  4.7800e-03  4.4483e-03  8.8154e-04

( 1 , 1 ,.,.) = 
 -7.7351e-03 -8.0144e-03 -1.0601e-02
 -1.6731e-02 -1.0723e-02 -1.3128e-02
 -6.3074e-03 -5.4699e-05 -1.2802e-02

( 1 , 2 ,.,.) = 
  1.2009e-02  1.3652e-02  1.6251e-02
  1.4238e-02  1.3374e-02  1.3322e-02
  1.2850e-02  1.4042e-03  5.7972e-03
    ... 

( 1 ,509,.,.) = 
 -7.0933e-03 -5.6503e-03 -2.4654e-03
 -6.6871e-04 -5.8700e-04  1.1693e-03
 -4.3795e-03 -5.2460e-03 -1.4626e-02

( 1 ,510,.,.) = 
 -6.1227e-03 -1.4198e-02 -4.9775e-03
  3.0169e-04 -1.9382e-02 -5.3630e-03
 -1.0860e-02 -1.6166e-02 -1.3313e-02

( 1 ,511,.,.) = 
 -4.7030e-03 -3.3847e-03 -6.9712e-04
  1.8605e-02  1.9058e-02  1.3702e-02
  9.8490e-03  1.2497e-02  7.6907e-03
      ⋮  

( 2 , 0 ,.,.) = 
 -3.6737e-03 -1.1626e-02 -1.0860e-02
 -1.0151e-02 -1.2250e-02 -2.3486e-02
 -1.8777e-02 -1.7928e-02 -1.9038e-02

( 2 , 1 ,.,.) = 
 -1.3083e-04  9.5283e-03  6.8781e-03
 -5.8247e-04 -2.1171e-03  9.9219e-03
  7.0848e-03  7.1240e-03  1.2951e-02

( 2 , 2 ,.,.) = 
 -1.1444e-02 -9.8910e-03 -1.1348e-02
 -9.3831e-03 -2.6998e-02 -2.0478e-02
 -1.3492e-02 -2.8705e-02 -1.0201e-02
    ... 

( 2 ,509,.,.) = 
  5.4220e-03  4.9237e-03  1.0751e-03
  9.1078e-03  2.8999e-03 -3.9658e-03
  3.3565e-03  8.7315e-03 -7.1396e-03

( 2 ,510,.,.) = 
 -1.5265e-02 -1.5173e-02 -4.1634e-03
 -4.1711e-03 -5.7112e-03  1.0662e-02
 -5.4534e-03 -2.9966e-03 -1.0922e-04

( 2 ,511,.,.) = 
  6.7162e-03  6.4612e-03  3.6989e-03
  5.6891e-03  1.7215e-02  3.8836e-03
 -6.8317e-03 -4.3384e-03 -1.2245e-02
...     
      ⋮  

(509, 0 ,.,.) = 
  1.2545e-02  1.6637e-02  1.9601e-02
  1.3381e-03  1.4747e-02  9.9264e-03
 -1.7171e-03  8.2806e-04  3.3050e-03

(509, 1 ,.,.) = 
  6.7253e-04 -1.9530e-03  8.0310e-04
  3.2593e-03 -7.1111e-04  9.4879e-03
 -4.1057e-03 -2.3027e-03  2.3162e-03

(509, 2 ,.,.) = 
  3.1239e-03  5.2456e-03  5.8967e-03
 -4.6978e-03  9.1447e-03  2.7352e-03
 -5.9662e-03 -2.4252e-03 -8.8665e-04
    ... 

(509,509,.,.) = 
 -2.4380e-02 -2.6508e-02 -3.4107e-02
 -2.6101e-02 -2.7859e-02 -3.2786e-02
 -2.2585e-02 -1.6162e-02 -2.3904e-02

(509,510,.,.) = 
 -1.5282e-02 -4.9976e-03 -1.9550e-02
 -1.5422e-03  5.9163e-03 -1.3940e-02
 -1.3601e-02 -1.2448e-02 -2.3471e-02

(509,511,.,.) = 
 -3.9811e-03 -1.0139e-02 -1.7083e-03
 -9.4603e-04 -1.3200e-03  2.9700e-03
  2.6670e-03 -4.8483e-03 -7.3068e-03
      ⋮  

(510, 0 ,.,.) = 
  3.1321e-03  2.4799e-03  1.7518e-04
  4.1623e-03  5.6350e-03  2.2252e-03
 -2.6790e-03  1.6938e-03  1.9851e-03

(510, 1 ,.,.) = 
  3.8500e-03  8.1773e-03  7.1938e-03
 -7.3350e-03  1.0496e-03 -5.7411e-03
 -1.0332e-03 -1.8658e-03  9.6808e-04

(510, 2 ,.,.) = 
  6.7614e-03  2.4829e-04  8.0311e-03
  2.9092e-02  8.3311e-03  3.3510e-02
  3.1210e-02  1.5521e-02  2.3171e-02
    ... 

(510,509,.,.) = 
  4.3113e-04  9.5483e-03  2.5289e-03
  5.0199e-03 -1.0324e-03 -5.6947e-03
  7.1174e-04  5.0711e-03 -1.9954e-03

(510,510,.,.) = 
  5.5875e-03 -7.8272e-04  4.0052e-03
  3.7843e-03  9.2700e-03  8.4217e-04
  8.9503e-03  1.5059e-02  1.4497e-02

(510,511,.,.) = 
 -4.3989e-03  9.1815e-04 -4.7792e-04
  6.4700e-04 -3.3292e-03  2.1010e-03
 -8.5175e-03  1.9034e-03  2.6700e-03
      ⋮  

(511, 0 ,.,.) = 
 -6.6582e-03 -2.3293e-02 -1.8097e-02
 -2.1230e-02 -1.7678e-02 -2.3276e-02
 -2.3859e-02 -2.0605e-02 -2.6285e-02

(511, 1 ,.,.) = 
 -8.6777e-03 -1.2357e-02 -9.8458e-03
 -4.3524e-03 -6.2286e-03 -1.0477e-02
  6.1125e-03  6.3418e-03  5.5887e-03

(511, 2 ,.,.) = 
 -2.9206e-02 -9.0183e-03 -2.0365e-02
 -2.0446e-02  9.4982e-03 -1.5475e-02
 -2.5078e-02 -1.6393e-02 -2.5387e-02
    ... 

(511,509,.,.) = 
  1.3479e-02  1.4836e-02  1.6113e-02
  2.1448e-02  1.3383e-02  1.9334e-02
  8.6669e-03  1.3319e-02  1.3759e-02

(511,510,.,.) = 
 -4.5016e-03  1.1256e-03  2.0470e-03
 -5.3759e-04 -1.3488e-02 -4.0906e-03
 -2.3993e-03 -7.8897e-03  1.6079e-03

(511,511,.,.) = 
 -2.6149e-04 -5.2235e-03 -6.7558e-03
  2.7854e-03  4.7315e-03  2.6768e-03
  5.5235e-03  1.2337e-02 -3.7986e-03
[torch.FloatTensor of size 512x512x3x3]
), ('layer4.1.bn2.weight', 
 0.1877
 0.2265
 0.1861
 0.2233
 0.2291
 0.2273
 0.1859
 0.2056
 0.2021
 0.2021
 0.2152
 0.2187
 0.1999
 0.2125
 0.2047
 0.1989
 0.2277
 0.2308
 0.2018
 0.1943
 0.1823
 0.2081
 0.2040
 0.2022
 0.2179
 0.1940
 0.1963
 0.2029
 0.2220
 0.2233
 0.2097
 0.2119
 0.2199
 0.1800
 0.2060
 0.1578
 0.1966
 0.1982
 0.1960
 0.2210
 0.2283
 0.1959
 0.2060
 0.1961
 0.2044
 0.1890
 0.2079
 0.2204
 0.2132
 0.2264
 0.2237
 0.1906
 0.1848
 0.2075
 0.2095
 0.1922
 0.1978
 0.2317
 0.1926
 0.2640
 0.2141
 0.2147
 0.2118
 0.1930
 0.1537
 0.2294
 0.2204
 0.2024
 0.2028
 0.2018
 0.2172
 0.2046
 0.2046
 0.2256
 0.1912
 0.2290
 0.1933
 0.1819
 0.2002
 0.2029
 0.2438
 0.2132
 0.2061
 0.2058
 0.2035
 0.2016
 0.2155
 0.1986
 0.2122
 0.2041
 0.2133
 0.2339
 0.2279
 0.1976
 0.2176
 0.1965
 0.2141
 0.2003
 0.2176
 0.1982
 0.2055
 0.2103
 0.1931
 0.2011
 0.1790
 0.1800
 0.2139
 0.2068
 0.2326
 0.1899
 0.2351
 0.2020
 0.2564
 0.2137
 0.2417
 0.2046
 0.2323
 0.2105
 0.2181
 0.1952
 0.2214
 0.2131
 0.2130
 0.1926
 0.2285
 0.1837
 0.2096
 0.1849
 0.2112
 0.2219
 0.1998
 0.2238
 0.2311
 0.2045
 0.1833
 0.2924
 0.1982
 0.2045
 0.1830
 0.2415
 0.1952
 0.2507
 0.2090
 0.2311
 0.2314
 0.1972
 0.2200
 0.2188
 0.2099
 0.1983
 0.2275
 0.1658
 0.2343
 0.2041
 0.2053
 0.2379
 0.2303
 0.2050
 0.2032
 0.2138
 0.1761
 0.2714
 0.1988
 0.2457
 0.2317
 0.2133
 0.2136
 0.2453
 0.2037
 0.2050
 0.2086
 0.1953
 0.2204
 0.2013
 0.2175
 0.2406
 0.2185
 0.2083
 0.2319
 0.1947
 0.1970
 0.2133
 0.2094
 0.2242
 0.2087
 0.2097
 0.2044
 0.2132
 0.2252
 0.2217
 0.2362
 0.1767
 0.2105
 0.2087
 0.2073
 0.2015
 0.2189
 0.2170
 0.2251
 0.2110
 0.2145
 0.1949
 0.1881
 0.1873
 0.2152
 0.2225
 0.1949
 0.2034
 0.2244
 0.2199
 0.2169
 0.2114
 0.1691
 0.1991
 0.2159
 0.1981
 0.1966
 0.2232
 0.2095
 0.2254
 0.2108
 0.1817
 0.2104
 0.2018
 0.2236
 0.1937
 0.2043
 0.2115
 0.2210
 0.2155
 0.2425
 0.2192
 0.1879
 0.1946
 0.2290
 0.2032
 0.2012
 0.1901
 0.2085
 0.2404
 0.2097
 0.2078
 0.1861
 0.2423
 0.1966
 0.1975
 0.2133
 0.1936
 0.2260
 0.2206
 0.1988
 0.2139
 0.1954
 0.1956
 0.1757
 0.2406
 0.2006
 0.2110
 0.2102
 0.2285
 0.2070
 0.1968
 0.1841
 0.1980
 0.1966
 0.2008
 0.1783
 0.2123
 0.2165
 0.2112
 0.2551
 0.1839
 0.2105
 0.2223
 0.2288
 0.1913
 0.2208
 0.1697
 0.2048
 0.1726
 0.2305
 0.2307
 0.1899
 0.2057
 0.2161
 0.1995
 0.2171
 0.2288
 0.1967
 0.2225
 0.2231
 0.2639
 0.2163
 0.1864
 0.2170
 0.2700
 0.2134
 0.1905
 0.2038
 0.2115
 0.1840
 0.2064
 0.1746
 0.2146
 0.1887
 0.2207
 0.2075
 0.1798
 0.2195
 0.2042
 0.1969
 0.2171
 0.1983
 0.1999
 0.2003
 0.1473
 0.1988
 0.2178
 0.1939
 0.2081
 0.2223
 0.2403
 0.2221
 0.2086
 0.2187
 0.2057
 0.2408
 0.2352
 0.2019
 0.1693
 0.2287
 0.2191
 0.2201
 0.2056
 0.1920
 0.1953
 0.2094
 0.2042
 0.2109
 0.1902
 0.2243
 0.2321
 0.2152
 0.2048
 0.2029
 0.2001
 0.2239
 0.2011
 0.2210
 0.2276
 0.2059
 0.2088
 0.2494
 0.2329
 0.1847
 0.2044
 0.2087
 0.2492
 0.2251
 0.2173
 0.2512
 0.2315
 0.2024
 0.1926
 0.2307
 0.2000
 0.2005
 0.2406
 0.2365
 0.2186
 0.1998
 0.2376
 0.2230
 0.2269
 0.1841
 0.2122
 0.1993
 0.2483
 0.2204
 0.2124
 0.2193
 0.2157
 0.2092
 0.1869
 0.2108
 0.2349
 0.2069
 0.2046
 0.1955
 0.2372
 0.1720
 0.2225
 0.2287
 0.2192
 0.2005
 0.2221
 0.2168
 0.2167
 0.3054
 0.2000
 0.1885
 0.2377
 0.2321
 0.2356
 0.2150
 0.1996
 0.1824
 0.1944
 0.2107
 0.2193
 0.2187
 0.2016
 0.2213
 0.1964
 0.2004
 0.2147
 0.2039
 0.2380
 0.2457
 0.2006
 0.2178
 0.1935
 0.2269
 0.2085
 0.1940
 0.2258
 0.2191
 0.2327
 0.2127
 0.2118
 0.1946
 0.2229
 0.2086
 0.2141
 0.2183
 0.2032
 0.1734
 0.1726
 0.2115
 0.2024
 0.2318
 0.2245
 0.2173
 0.1902
 0.2442
 0.2308
 0.1763
 0.1980
 0.1776
 0.2106
 0.1994
 0.2048
 0.2079
 0.2175
 0.2068
 0.2084
 0.2442
 0.2017
 0.2196
 0.2138
 0.2349
 0.1634
 0.2257
 0.2372
 0.1918
 0.2324
 0.2118
 0.1857
 0.2018
 0.2149
 0.1939
 0.2113
 0.2151
 0.1848
 0.2081
 0.1990
 0.2267
 0.2270
 0.1818
 0.1993
 0.2078
 0.2088
 0.2124
 0.1959
 0.1846
 0.1978
 0.2004
 0.2162
 0.2220
 0.2108
 0.2061
 0.1713
 0.2395
 0.2221
 0.2119
 0.2170
 0.2326
 0.2162
 0.2233
 0.2225
 0.2172
 0.2106
 0.2066
 0.2153
 0.1806
 0.2300
 0.2600
 0.1723
 0.2103
 0.1667
 0.1893
 0.1956
[torch.FloatTensor of size 512]
), ('layer4.1.bn2.bias', 
-0.1375
-0.1440
-0.0849
-0.1497
-0.1884
-0.1662
-0.0487
-0.0901
-0.1060
-0.0974
-0.1388
-0.1742
-0.1475
-0.1270
-0.1201
-0.1246
-0.1503
-0.1361
-0.1302
-0.1069
-0.1102
-0.1296
-0.0833
-0.1228
-0.1583
-0.1224
-0.0547
-0.0932
-0.1440
-0.1305
-0.0976
-0.1109
-0.1726
-0.0436
-0.1675
-0.0833
-0.1241
-0.0978
-0.1016
-0.1611
-0.1565
-0.0706
-0.1009
-0.0857
-0.1154
-0.0706
-0.0963
-0.1305
-0.1015
-0.1535
-0.1213
-0.0639
-0.0291
-0.1423
-0.1390
-0.1226
-0.1175
-0.1745
-0.1194
-0.2714
-0.1202
-0.1482
-0.1610
-0.1080
 0.0002
-0.1466
-0.1377
-0.1673
-0.1120
-0.0645
-0.1309
-0.0929
-0.1375
-0.1423
-0.0541
-0.2089
-0.0938
-0.0441
-0.1218
-0.0988
-0.1894
-0.0997
-0.1529
-0.1072
-0.0400
-0.0965
-0.1344
-0.1116
-0.1237
-0.1345
-0.1210
-0.1783
-0.1459
-0.0913
-0.1518
-0.1056
-0.1303
-0.0896
-0.1806
-0.1640
-0.0813
-0.0725
-0.0823
-0.0947
-0.0748
-0.0047
-0.1147
-0.1460
-0.1888
-0.0859
-0.1236
-0.1096
-0.1124
-0.0818
-0.1573
-0.0559
-0.1575
-0.1071
-0.1139
-0.0897
-0.2126
-0.0975
-0.1273
-0.0559
-0.1494
-0.0658
-0.1501
-0.0470
-0.1355
-0.1625
-0.0854
-0.1406
-0.1726
-0.0622
-0.1087
-0.3225
-0.0961
-0.0957
-0.0692
-0.1492
-0.1124
-0.1854
-0.0973
-0.1903
-0.1862
-0.0850
-0.1134
-0.0830
-0.1418
-0.0580
-0.1837
-0.1116
-0.1539
-0.1144
-0.0937
-0.2002
-0.1815
-0.0851
-0.1045
-0.1602
-0.0706
-0.2672
-0.0719
-0.2147
-0.1778
-0.0770
-0.1026
-0.2081
-0.0847
-0.1425
-0.1422
-0.0619
-0.1457
-0.0750
-0.0766
-0.1444
-0.1440
-0.1088
-0.1738
-0.0641
-0.0767
-0.0697
-0.1330
-0.1547
-0.1099
-0.0797
-0.1006
-0.1112
-0.1301
-0.1106
-0.2129
-0.0649
-0.1087
-0.1196
-0.1456
-0.0904
-0.0945
-0.1591
-0.1544
-0.1664
-0.0997
-0.1184
-0.0831
-0.1142
-0.1909
-0.1641
-0.0730
-0.1236
-0.1110
-0.1919
-0.1766
-0.1358
-0.1087
-0.1148
-0.1313
-0.1066
-0.0924
-0.0879
-0.1067
-0.1439
-0.1004
-0.0615
-0.0948
-0.1667
-0.1598
-0.0906
-0.0534
-0.1449
-0.1687
-0.1580
-0.1438
-0.1569
-0.0816
-0.0429
-0.1339
-0.1313
-0.0813
-0.0885
-0.1161
-0.1870
-0.1184
-0.0751
-0.1257
-0.1896
-0.0761
-0.0941
-0.1415
-0.0819
-0.1481
-0.1503
-0.0694
-0.1454
-0.1116
-0.0891
-0.0061
-0.1622
-0.0938
-0.1469
-0.0934
-0.1176
-0.0946
-0.0967
-0.0812
-0.0975
-0.0726
-0.1074
-0.0317
-0.1193
-0.1544
-0.1263
-0.1667
 0.0080
-0.1150
-0.1404
-0.1904
-0.1680
-0.1531
-0.0591
-0.1107
-0.0298
-0.1729
-0.1606
-0.1098
-0.1355
-0.1554
-0.1474
-0.1652
-0.1196
-0.0585
-0.1256
-0.1592
-0.2696
-0.1741
-0.1161
-0.1399
-0.2254
-0.1322
-0.0752
-0.1150
-0.1480
-0.1354
-0.1353
-0.0392
-0.0780
-0.0837
-0.1176
-0.0743
-0.0552
-0.1329
-0.1482
-0.1078
-0.1458
-0.0924
-0.0990
-0.1020
 0.0807
-0.1646
-0.1456
-0.1693
-0.1252
-0.1117
-0.1982
-0.1451
-0.0798
-0.0975
-0.0930
-0.2075
-0.1645
-0.1473
-0.0531
-0.1155
-0.1508
-0.1208
-0.1485
-0.0839
-0.0677
-0.0918
-0.1209
-0.1428
-0.0355
-0.1249
-0.0635
-0.1064
-0.1231
-0.0547
-0.0926
-0.1567
-0.1017
-0.1232
-0.1386
-0.1391
-0.1288
-0.1586
-0.1284
-0.0765
-0.1188
-0.1491
-0.1389
-0.1172
-0.0947
-0.1941
-0.1456
-0.0967
-0.1445
-0.1160
-0.1699
-0.0634
-0.1621
-0.1897
-0.1239
-0.1098
-0.1222
-0.1986
-0.1558
-0.0750
-0.1420
-0.0554
-0.1526
-0.1659
-0.1083
-0.1715
-0.1808
-0.1181
-0.0838
-0.1133
-0.1369
-0.1422
-0.0910
-0.0813
-0.1627
-0.0413
-0.1575
-0.1533
-0.1391
-0.0873
-0.1657
-0.1413
-0.1320
-0.3477
-0.1198
-0.0668
-0.1554
-0.1077
-0.1717
-0.1298
-0.1272
-0.0599
-0.0310
-0.1321
-0.1725
-0.1228
-0.0960
-0.1308
-0.1469
-0.1074
-0.1116
-0.1215
-0.1342
-0.1298
-0.1156
-0.1677
-0.0561
-0.1430
-0.1124
-0.0643
-0.1392
-0.1334
-0.1571
-0.1127
-0.1081
-0.0584
-0.1123
-0.0989
-0.1259
-0.1287
-0.1409
-0.0392
-0.0489
-0.0597
-0.1206
-0.1956
-0.1464
-0.1096
-0.1223
-0.1745
-0.1811
-0.0405
-0.0361
-0.1041
-0.1058
-0.0809
-0.0886
-0.1182
-0.1501
-0.1165
-0.1307
-0.1575
-0.1887
-0.1348
-0.1486
-0.1793
-0.0445
-0.1137
-0.1604
-0.0660
-0.1258
-0.1110
-0.0692
-0.1123
-0.1785
-0.1240
-0.1168
-0.1250
-0.0764
-0.1016
-0.0954
-0.1234
-0.2029
-0.1185
-0.0874
-0.1230
-0.1530
-0.1109
-0.1434
-0.1366
-0.0815
-0.0802
-0.1590
-0.1229
-0.1386
-0.1078
-0.0135
-0.1715
-0.1167
-0.1101
-0.0946
-0.1359
-0.1840
-0.1368
-0.1301
-0.1466
-0.1013
-0.1257
-0.1364
-0.0840
-0.1586
-0.1680
-0.0110
-0.1026
-0.0153
-0.0750
-0.0634
[torch.FloatTensor of size 512]
), ('layer4.1.bn2.running_mean', 
-0.0998
-0.1049
-0.0920
-0.1030
-0.0679
-0.1003
-0.0974
-0.1096
-0.0928
-0.1285
-0.0764
-0.1052
-0.0919
-0.1013
-0.0522
-0.0961
-0.0934
-0.0833
-0.0378
-0.0783
-0.0857
-0.1152
-0.1171
-0.0862
-0.0752
-0.1050
-0.1134
-0.1004
-0.0846
-0.1073
-0.0919
-0.0756
-0.0646
-0.0856
-0.0452
-0.0627
-0.0910
-0.0860
-0.0824
-0.0639
-0.0989
-0.1325
-0.1014
-0.1155
-0.1013
-0.0603
-0.0756
-0.0926
-0.1496
-0.0811
-0.1128
-0.0917
 0.3198
-0.0275
-0.0484
-0.0891
 0.0068
-0.0719
-0.0688
-0.0948
-0.1229
-0.0820
-0.0502
 0.1257
-0.0777
-0.0983
-0.1176
-0.0498
-0.1057
-0.0848
-0.1013
-0.1597
-0.0489
-0.0624
-0.0748
-0.0705
-0.1020
-0.0774
-0.0821
-0.0715
-0.1211
-0.0833
-0.0904
-0.0574
-0.1146
-0.1100
-0.0555
-0.0291
-0.0854
-0.0863
-0.1275
-0.1266
-0.0817
-0.0590
-0.0966
-0.0816
-0.0900
-0.1030
-0.0668
-0.0981
-0.1135
-0.1809
-0.0825
-0.0967
-0.1008
-0.1231
-0.0633
-0.1122
-0.0271
-0.0332
-0.1558
-0.0718
-0.1624
-0.0971
-0.1426
-0.0752
-0.1361
-0.1398
-0.1347
-0.0597
-0.0578
-0.0964
-0.0809
-0.0980
-0.0787
-0.0866
-0.0512
-0.0753
-0.1012
-0.1200
-0.0982
-0.1010
-0.0659
-0.1070
-0.0208
-0.2654
-0.0595
-0.0722
-0.0320
-0.1048
-0.0645
-0.1332
-0.1101
-0.0815
-0.0755
-0.1183
-0.1259
-0.1692
-0.0791
-0.1208
-0.1246
-0.2385
-0.1394
-0.0739
-0.1049
-0.0980
-0.0995
-0.0772
-0.1174
-0.0971
-0.0429
-0.1489
-0.0511
-0.0817
-0.0945
-0.1574
-0.1127
-0.0972
-0.0850
-0.1074
-0.0827
-0.0710
-0.0741
-0.1164
-0.1568
-0.1126
-0.0553
-0.0969
-0.0676
-0.1167
-0.0514
-0.1441
-0.0618
-0.1075
-0.0779
-0.0725
-0.0778
-0.1025
-0.1238
-0.1352
-0.0871
-0.0757
-0.1165
-0.0868
 0.0485
-0.0739
-0.0255
-0.0903
-0.0760
-0.1172
-0.1208
-0.0061
-0.0476
-0.0771
-0.0978
-0.0728
-0.0321
-0.0390
-0.1068
-0.0972
-0.0919
-0.0885
-0.2003
-0.0988
-0.0819
-0.0883
-0.1044
-0.1371
-0.1252
-0.1031
-0.1258
-0.0451
-0.0752
 0.0533
-0.0590
-0.0591
-0.1052
-0.1037
-0.0451
-0.1061
-0.1759
-0.0598
-0.0932
-0.1379
-0.1220
-0.1003
-0.1637
-0.0785
-0.1024
-0.0969
-0.0762
-0.1181
-0.1093
-0.0772
-0.0829
-0.0598
-0.0938
-0.0984
-0.0834
-0.0620
-0.1194
-0.0463
-0.0899
-0.1487
-0.1046
-0.1030
-0.0905
-0.0488
-0.1411
-0.1832
-0.0152
-0.0469
-0.0581
-0.0570
-0.1130
-0.0949
-0.1252
-0.0107
-0.1065
-0.0857
-0.1351
-0.0572
-0.1261
-0.0942
-0.0951
-0.1147
-0.0673
-0.0996
-0.0680
-0.1329
-0.1464
-0.0587
-0.0641
-0.0477
-0.1173
-0.0936
-0.0984
-0.1173
-0.1097
-0.1427
-0.0740
-0.0472
-0.0970
-0.0136
-0.0329
-0.1437
-0.0621
-0.0201
-0.0718
 0.0095
-0.1051
-0.0848
-0.0718
-0.1300
-0.1062
-0.1190
-0.1653
-0.0543
-0.0727
-0.0692
-0.0594
-0.1309
-0.0650
-0.0713
-0.0592
-0.0062
-0.1064
-0.0686
-0.1128
-0.0994
-0.0945
-0.0581
-0.1074
-0.1106
-0.1708
-0.0814
-0.1046
-0.0814
-0.0891
-0.0711
-0.1420
-0.1068
-0.1230
-0.0924
-0.0903
-0.1099
-0.1137
-0.0480
-0.0917
-0.1004
-0.0793
-0.2084
-0.1454
-0.0653
-0.1446
-0.0786
-0.1035
-0.0827
-0.0434
-0.1107
-0.0977
-0.1115
-0.0849
-0.1437
-0.0695
-0.0841
-0.1179
-0.1221
-0.1189
-0.1356
-0.1045
-0.0940
-0.0845
-0.1135
-0.1425
-0.0787
-0.0862
-0.1360
-0.0582
-0.0148
-0.1197
-0.1607
-0.0790
-0.0981
-0.0492
-0.0475
-0.0962
-0.1602
-0.0637
-0.0805
-0.0523
-0.0653
-0.1141
-0.0157
-0.1156
-0.1464
-0.0788
-0.1248
-0.0911
-0.0703
-0.0893
-0.0664
-0.1395
-0.1016
-0.1145
-0.1313
-0.0821
-0.0740
-0.3793
-0.0903
-0.0601
-0.1433
-0.1673
-0.1116
-0.1318
-0.0694
-0.1003
-0.1807
-0.1036
-0.0949
-0.0935
-0.1401
-0.0779
-0.1054
-0.0528
-0.0824
-0.0640
-0.1238
-0.1470
-0.0812
-0.0238
-0.0526
-0.0560
-0.1033
-0.0828
-0.1408
-0.0969
-0.0921
-0.0807
-0.1064
-0.0393
-0.0681
-0.1001
-0.1073
-0.1063
-0.0380
-0.1039
-0.0190
-0.1141
-0.0322
-0.0442
-0.1605
-0.1353
-0.0353
-0.1171
-0.0847
-0.0566
-0.1633
-0.0632
-0.0976
-0.1255
-0.1017
-0.1051
-0.0973
-0.0626
-0.0664
-0.1349
-0.0768
-0.0694
-0.0452
-0.0932
-0.0577
-0.1047
-0.1423
-0.0881
-0.0998
-0.1234
-0.0359
-0.0901
 0.4489
-0.0697
-0.0949
-0.0658
-0.0577
-0.0570
-0.0612
-0.0972
-0.0708
-0.0523
-0.0330
-0.0806
-0.0611
-0.0728
-0.0878
-0.1096
-0.0993
-0.0493
-0.1016
-0.0962
-0.0803
-0.0918
-0.0526
-0.0824
-0.1075
-0.0845
-0.1079
-0.1398
-0.0633
-0.1075
-0.0621
-0.0009
-0.1791
-0.1019
-0.1005
-0.0699
-0.0965
-0.1558
-0.0726
-0.1185
-0.1248
-0.0784
-0.1231
[torch.FloatTensor of size 512]
), ('layer4.1.bn2.running_var', 
1.00000e-02 *
  3.4040
  1.0231
  1.1828
  1.1573
  0.9719
  1.0794
  1.3733
  1.1659
  1.1259
  1.1169
  1.0381
  1.1489
  1.0066
  1.2344
  1.1124
  1.4280
  1.1369
  1.2499
  0.9874
  1.2759
  1.0410
  1.3026
  1.2327
  1.2438
  1.1133
  1.1134
  1.4613
  1.8261
  1.0643
  1.1492
  1.3010
  1.0177
  1.1668
  1.0650
  1.0125
  1.2488
  1.0383
  1.1723
  1.2157
  1.1042
  1.2291
  1.4103
  1.4056
  1.4245
  1.5585
  1.2459
  1.4788
  1.3206
  1.3875
  1.1933
  1.1469
  1.1931
  3.7554
  1.0908
  1.1726
  1.0260
  1.1864
  1.2349
  1.0371
  0.8198
  1.2864
  1.4050
  1.0619
  1.6031
  1.1092
  1.1412
  1.4836
  0.8689
  1.1872
  1.2862
  1.2878
  1.1961
  1.0521
  1.0219
  1.1360
  0.9761
  1.0476
  1.2837
  1.0841
  1.3802
  1.2304
  1.4095
  1.0603
  1.0043
  2.0866
  1.4100
  0.9974
  1.3366
  1.0866
  1.1072
  1.1601
  1.4088
  1.0983
  1.3082
  1.0503
  1.0919
  1.0627
  1.3651
  0.9975
  1.1467
  1.1654
  1.7252
  1.0487
  1.3301
  1.0815
  1.2762
  1.0609
  1.1078
  1.0816
  1.2501
  1.4872
  1.1322
  1.9698
  1.2427
  1.2620
  1.4583
  1.1444
  1.6561
  1.2797
  1.1913
  1.1246
  1.1943
  1.1878
  1.3162
  1.2799
  1.0262
  0.9766
  1.2201
  1.3224
  1.0722
  1.0721
  1.0305
  1.3052
  1.2408
  1.1952
  4.4064
  1.1929
  1.3939
  1.5303
  1.3886
  1.0634
  1.0754
  1.2400
  1.2282
  1.1235
  1.5243
  1.1219
  1.8918
  1.2306
  1.5097
  1.0626
  2.7491
  0.9920
  1.1136
  1.3155
  1.1759
  1.1679
  1.3727
  1.1703
  0.9973
  1.0403
  1.0791
  1.3161
  1.1544
  1.1889
  1.2924
  1.2609
  1.0895
  1.6020
  1.2642
  1.0130
  1.4367
  1.1536
  1.3024
  1.9647
  1.3357
  1.1576
  1.0846
  1.0043
  1.3496
  1.2173
  1.4542
  1.0137
  1.3483
  1.1081
  1.1988
  1.0288
  1.1402
  1.2763
  1.3195
  1.1938
  1.3484
  1.1966
  1.2850
  1.4900
  1.2614
  1.4478
  1.1086
  1.0605
  1.2401
  1.3684
  1.0467
  1.1558
  1.0333
  1.0213
  1.1784
  1.2516
  0.9965
  1.4626
  0.9959
  0.9732
  1.0688
  1.8049
  0.9452
  1.4027
  1.3278
  1.4323
  1.7874
  1.2281
  1.0498
  1.2603
  1.0586
  1.1670
  1.2466
  0.9458
  1.0247
  1.3939
  0.9148
  1.2335
  1.0900
  1.6745
  1.1162
  1.4295
  1.5475
  1.4625
  1.0425
  1.8812
  0.9259
  1.1562
  1.1860
  1.1001
  1.3249
  0.9989
  1.1714
  1.0560
  1.1325
  1.0232
  1.4755
  1.2644
  1.0343
  1.8201
  1.6585
  1.3212
  1.2497
  1.2259
  1.1033
  1.1502
  1.2670
  1.4359
  1.4045
  1.2359
  0.9586
  0.9713
  1.0655
  1.3826
  1.0426
  1.6768
  1.4457
  1.1355
  1.2630
  1.6188
  2.0307
  1.2548
  1.3711
  1.2146
  1.2668
  1.0362
  1.0596
  1.1098
  1.5406
  1.2155
  1.0436
  0.9213
  1.0949
  1.2341
  1.0179
  1.1536
  1.3009
  1.5072
  1.1359
  0.9409
  2.3957
  1.0554
  0.9501
  0.9694
  1.5588
  1.1789
  1.1845
  1.1213
  1.1181
  1.3315
  1.0085
  1.1805
  1.5436
  1.2574
  1.2704
  1.6658
  1.3639
  1.2000
  1.0927
  1.2045
  1.2090
  1.3009
  1.1979
  1.1044
  1.8325
  1.2638
  1.0388
  1.1063
  1.1155
  1.6083
  1.0281
  1.0816
  1.3439
  1.4074
  1.1956
  1.0633
  1.1403
  1.0850
  0.9692
  1.2842
  1.1076
  1.5063
  1.1244
  1.1779
  1.4118
  1.2485
  1.1312
  1.5182
  1.4617
  1.3556
  2.3880
  1.4808
  1.2441
  2.0347
  1.3830
  1.1427
  1.2721
  1.3480
  1.0138
  1.0313
  1.1274
  1.1321
  1.4453
  1.1113
  1.2533
  1.1329
  1.3503
  1.5860
  1.6325
  0.9784
  1.2798
  1.1097
  1.6733
  1.7699
  1.2173
  1.2718
  1.1664
  1.2262
  1.5498
  1.1583
  1.7654
  1.1694
  1.1070
  1.0669
  1.0450
  1.1666
  1.6639
  1.1576
  1.2943
  0.9795
  0.9620
  1.4789
  1.1680
  1.1615
  1.5228
  0.9879
  1.2110
  1.2441
  1.2384
  1.1275
  1.1330
  1.5667
  1.3090
  1.1672
  1.0550
  1.1978
  1.1640
  4.8845
  1.0631
  1.2856
  1.0582
  1.4913
  1.3643
  1.3397
  1.0742
  1.2823
  2.0139
  1.0626
  1.0012
  1.5181
  1.3868
  1.1614
  1.2564
  1.3726
  1.2695
  1.1792
  1.4534
  1.3061
  1.0587
  0.9425
  1.3576
  1.2420
  1.3031
  1.2612
  1.1644
  1.2459
  1.5460
  1.3093
  1.2570
  1.4070
  1.4103
  1.5399
  1.3080
  1.0012
  0.9952
  1.3807
  0.9919
  1.4889
  0.9434
  1.0238
  1.2607
  1.1636
  1.0860
  1.3271
  1.1973
  1.3344
  1.7797
  0.8987
  1.1890
  1.3561
  1.2215
  1.1212
  1.0180
  1.0629
  1.0225
  1.4644
  1.0335
  1.0130
  1.1225
  1.3140
  0.9427
  1.3496
  1.3275
  1.2133
  1.2350
  1.1170
  1.2656
  1.1595
  2.9531
  0.9853
  1.3699
  1.1046
  1.0483
  1.6291
  1.2269
  1.4666
  1.0572
  1.0526
  1.2391
  1.1312
  1.0953
  1.1996
  1.2481
  0.9836
  1.2358
  1.2376
  1.0309
  1.4470
  1.0012
  1.3128
  1.4123
  1.1571
  1.3605
  1.3555
  1.2858
  1.6200
  0.9427
  1.2463
  1.0552
  1.2591
  1.5623
  1.1120
  1.0390
  0.9809
  1.1809
  1.5663
  1.1977
  1.3520
  1.7491
  1.0518
  1.4318
[torch.FloatTensor of size 512]
), ('layer4.1.conv3.weight', 
( 0  , 0  ,.,.) = 
  1.2493e-03

( 0  , 1  ,.,.) = 
 -3.2917e-03

( 0  , 2  ,.,.) = 
  5.4150e-03
      ... 

( 0  ,509 ,.,.) = 
 -1.0302e-02

( 0  ,510 ,.,.) = 
  2.4232e-03

( 0  ,511 ,.,.) = 
 -3.0752e-03
        ⋮  

( 1  , 0  ,.,.) = 
  1.0414e-02

( 1  , 1  ,.,.) = 
 -4.7258e-02

( 1  , 2  ,.,.) = 
  3.0868e-02
      ... 

( 1  ,509 ,.,.) = 
 -1.7297e-02

( 1  ,510 ,.,.) = 
 -6.6708e-03

( 1  ,511 ,.,.) = 
  2.1605e-02
        ⋮  

( 2  , 0  ,.,.) = 
  2.6783e-03

( 2  , 1  ,.,.) = 
 -5.1706e-03

( 2  , 2  ,.,.) = 
 -2.4595e-02
      ... 

( 2  ,509 ,.,.) = 
  4.2276e-03

( 2  ,510 ,.,.) = 
 -9.3935e-03

( 2  ,511 ,.,.) = 
 -1.6475e-03
 ...      
        ⋮  

(2045, 0  ,.,.) = 
  2.0176e-02

(2045, 1  ,.,.) = 
 -2.2239e-02

(2045, 2  ,.,.) = 
  4.1976e-03
      ... 

(2045,509 ,.,.) = 
 -6.8267e-03

(2045,510 ,.,.) = 
  3.3010e-02

(2045,511 ,.,.) = 
 -2.3050e-03
        ⋮  

(2046, 0  ,.,.) = 
 -5.8628e-03

(2046, 1  ,.,.) = 
  2.1406e-02

(2046, 2  ,.,.) = 
  5.1443e-03
      ... 

(2046,509 ,.,.) = 
  4.8729e-03

(2046,510 ,.,.) = 
  1.1051e-02

(2046,511 ,.,.) = 
  3.7923e-03
        ⋮  

(2047, 0  ,.,.) = 
  6.3248e-03

(2047, 1  ,.,.) = 
 -3.5184e-04

(2047, 2  ,.,.) = 
 -1.6119e-02
      ... 

(2047,509 ,.,.) = 
  1.6269e-03

(2047,510 ,.,.) = 
  4.5523e-03

(2047,511 ,.,.) = 
  5.5428e-03
[torch.FloatTensor of size 2048x512x1x1]
), ('layer4.1.bn3.weight', 
 0.2866
 0.5016
 0.3292
   ⋮   
 0.2542
 0.3169
 0.2519
[torch.FloatTensor of size 2048]
), ('layer4.1.bn3.bias', 
-0.0921
-0.0789
-0.0743
   ⋮   
-0.0763
-0.0897
-0.0772
[torch.FloatTensor of size 2048]
), ('layer4.1.bn3.running_mean', 
 4.6832e-03
 6.2964e-03
-5.6273e-03
     ⋮     
-4.0602e-04
-2.6624e-03
 2.8254e-03
[torch.FloatTensor of size 2048]
), ('layer4.1.bn3.running_var', 
1.00000e-03 *
  0.5995
  1.2044
  0.9223
    ⋮   
  0.4871
  0.7379
  0.4970
[torch.FloatTensor of size 2048]
), ('layer4.2.conv1.weight', 
( 0  , 0  ,.,.) = 
  1.3643e-02

( 0  , 1  ,.,.) = 
  1.3352e-02

( 0  , 2  ,.,.) = 
  2.5176e-03
      ... 

( 0  ,2045,.,.) = 
  1.4776e-02

( 0  ,2046,.,.) = 
  1.5398e-02

( 0  ,2047,.,.) = 
  4.3547e-03
        ⋮  

( 1  , 0  ,.,.) = 
 -2.6536e-02

( 1  , 1  ,.,.) = 
  7.8533e-03

( 1  , 2  ,.,.) = 
 -1.8003e-02
      ... 

( 1  ,2045,.,.) = 
 -2.9470e-02

( 1  ,2046,.,.) = 
 -6.6564e-03

( 1  ,2047,.,.) = 
  2.7845e-03
        ⋮  

( 2  , 0  ,.,.) = 
 -4.3014e-03

( 2  , 1  ,.,.) = 
 -1.0007e-02

( 2  , 2  ,.,.) = 
 -1.4744e-02
      ... 

( 2  ,2045,.,.) = 
  6.6933e-03

( 2  ,2046,.,.) = 
 -8.1382e-03

( 2  ,2047,.,.) = 
 -1.7922e-03
 ...      
        ⋮  

(509 , 0  ,.,.) = 
 -8.3784e-03

(509 , 1  ,.,.) = 
  1.9727e-02

(509 , 2  ,.,.) = 
 -1.7556e-02
      ... 

(509 ,2045,.,.) = 
  7.7989e-03

(509 ,2046,.,.) = 
  2.2152e-02

(509 ,2047,.,.) = 
  1.9289e-02
        ⋮  

(510 , 0  ,.,.) = 
 -1.8149e-02

(510 , 1  ,.,.) = 
 -8.3862e-03

(510 , 2  ,.,.) = 
 -2.7932e-03
      ... 

(510 ,2045,.,.) = 
  2.5652e-04

(510 ,2046,.,.) = 
  4.6211e-03

(510 ,2047,.,.) = 
  3.9791e-02
        ⋮  

(511 , 0  ,.,.) = 
  6.2797e-03

(511 , 1  ,.,.) = 
  3.4523e-03

(511 , 2  ,.,.) = 
 -3.2129e-02
      ... 

(511 ,2045,.,.) = 
 -3.0930e-02

(511 ,2046,.,.) = 
 -2.9227e-02

(511 ,2047,.,.) = 
  8.8959e-03
[torch.FloatTensor of size 512x2048x1x1]
), ('layer4.2.bn1.weight', 
 0.2137
 0.2040
 0.2130
 0.2214
 0.2106
 0.2261
 0.2279
 0.2252
 0.1967
 0.1902
 0.1897
 0.2007
 0.2447
 0.2326
 0.2226
 0.2356
 0.2152
 0.2092
 0.2083
 0.2287
 0.1644
 0.2195
 0.1718
 0.2129
 0.2169
 0.2081
 0.2058
 0.2223
 0.2165
 0.2074
 0.2150
 0.2532
 0.2369
 0.2307
 0.2287
 0.1755
 0.2400
 0.2255
 0.2051
 0.1561
 0.2189
 0.2200
 0.2297
 0.1885
 0.2016
 0.2164
 0.2401
 0.2131
 0.2233
 0.2616
 0.2167
 0.1854
 0.2002
 0.2443
 0.2369
 0.2076
 0.2544
 0.2443
 0.2235
 0.2024
 0.2121
 0.2360
 0.2232
 0.1463
 0.2330
 0.2411
 0.1834
 0.1762
 0.1860
 0.2314
 0.2309
 0.2423
 0.1554
 0.2323
 0.1429
 0.2170
 0.2041
 0.2116
 0.2431
 0.2230
 0.1852
 0.2168
 0.1836
 0.1447
 0.2104
 0.2331
 0.1827
 0.2368
 0.2288
 0.2432
 0.1969
 0.2609
 0.2449
 0.2226
 0.2366
 0.2304
 0.2242
 0.2512
 0.2490
 0.2278
 0.2165
 0.2144
 0.2329
 0.2406
 0.2013
 0.2148
 0.1990
 0.2419
 0.1802
 0.2435
 0.2388
 0.2152
 0.2243
 0.2168
 0.1931
 0.2159
 0.2123
 0.2239
 0.2421
 0.2290
 0.2111
 0.2262
 0.2219
 0.1632
 0.1774
 0.2063
 0.2599
 0.1609
 0.2303
 0.2371
 0.1980
 0.2297
 0.2478
 0.2144
 0.1964
 0.2534
 0.2318
 0.2212
 0.2613
 0.1586
 0.2281
 0.2022
 0.2231
 0.1972
 0.2087
 0.2202
 0.2280
 0.2175
 0.2297
 0.2520
 0.1938
 0.2463
 0.2189
 0.2088
 0.2213
 0.2236
 0.2112
 0.2439
 0.2057
 0.2476
 0.2063
 0.1128
 0.2078
 0.2238
 0.2378
 0.2303
 0.1874
 0.2236
 0.2233
 0.2239
 0.2412
 0.2388
 0.2025
 0.2101
 0.2465
 0.2167
 0.1822
 0.2116
 0.2248
 0.2228
 0.1737
 0.2223
 0.2407
 0.2335
 0.1918
 0.2032
 0.2224
 0.2210
 0.2423
 0.1901
 0.2077
 0.2532
 0.1258
 0.2483
 0.2204
 0.2377
 0.2124
 0.2310
 0.2324
 0.2311
 0.2320
 0.2006
 0.2060
 0.2350
 0.1802
 0.2484
 0.2028
 0.2184
 0.1746
 0.2498
 0.2296
 0.2026
 0.2287
 0.2172
 0.2115
 0.2339
 0.2056
 0.2174
 0.2476
 0.2119
 0.2230
 0.2384
 0.1510
 0.1458
 0.2131
 0.2323
 0.1555
 0.2142
 0.2096
 0.1983
 0.1955
 0.2301
 0.2196
 0.2306
 0.2008
 0.2526
 0.2100
 0.2083
 0.2167
 0.2428
 0.2398
 0.1984
 0.2194
 0.1965
 0.2071
 0.2126
 0.2208
 0.2232
 0.2086
 0.2524
 0.2071
 0.2726
 0.2097
 0.2287
 0.2143
 0.2379
 0.2134
 0.2374
 0.2481
 0.2239
 0.2357
 0.2089
 0.2734
 0.1905
 0.2156
 0.2438
 0.2293
 0.2482
 0.2102
 0.2379
 0.2085
 0.2106
 0.2050
 0.2235
 0.2440
 0.1873
 0.2639
 0.1576
 0.2230
 0.2170
 0.2377
 0.2051
 0.1779
 0.1384
 0.2074
 0.2397
 0.1960
 0.2373
 0.2044
 0.2067
 0.2471
 0.2146
 0.2290
 0.2510
 0.2338
 0.1728
 0.2325
 0.2295
 0.2145
 0.2023
 0.1951
 0.2475
 0.1248
 0.2080
 0.2398
 0.2447
 0.2215
 0.2399
 0.1459
 0.2425
 0.1251
 0.2402
 0.1249
 0.2252
 0.2166
 0.1973
 0.2343
 0.2408
 0.1961
 0.2130
 0.2359
 0.2202
 0.1998
 0.2124
 0.2355
 0.2474
 0.1984
 0.2187
 0.2351
 0.2273
 0.2457
 0.2275
 0.2248
 0.2086
 0.2266
 0.2544
 0.2107
 0.1974
 0.2409
 0.1414
 0.1913
 0.2203
 0.2027
 0.2258
 0.2454
 0.2222
 0.1660
 0.2488
 0.2268
 0.2109
 0.1763
 0.1776
 0.2203
 0.2317
 0.2276
 0.1667
 0.2197
 0.2182
 0.2107
 0.2269
 0.2462
 0.2055
 0.2231
 0.1980
 0.2490
 0.2242
 0.2222
 0.2126
 0.1974
 0.2339
 0.4865
 0.2146
 0.2279
 0.1927
 0.2287
 0.2060
 0.2341
 0.2113
 0.2140
 0.1797
 0.2188
 0.1787
 0.2380
 0.2352
 0.2126
 0.1832
 0.2201
 0.2206
 0.2157
 0.2208
 0.2341
 0.2330
 0.2137
 0.1503
 0.1908
 0.1825
 0.2240
 0.1760
 0.2409
 0.2406
 0.2167
 0.2361
 0.2262
 0.2551
 0.2627
 0.2180
 0.2278
 0.2240
 0.2233
 0.2085
 0.2085
 0.2015
 0.1781
 0.1753
 0.2188
 0.2329
 0.1251
 0.2324
 0.1952
 0.2141
 0.1709
 0.2130
 0.2244
 0.1991
 0.2222
 0.1716
 0.2229
 0.2225
 0.2417
 0.1735
 0.2221
 0.1961
 0.2164
 0.2141
 0.2270
 0.2608
 0.2341
 0.2243
 0.2285
 0.1894
 0.2260
 0.2166
 0.2361
 0.2414
 0.1910
 0.2153
 0.2266
 0.2654
 0.2050
 0.2103
 0.1905
 0.2331
 0.1690
 0.2316
 0.2160
 0.2390
 0.2177
 0.2194
 0.1938
 0.2274
 0.2470
 0.2270
 0.2209
 0.2220
 0.2583
 0.1836
 0.2611
 0.2018
 0.2165
 0.1717
 0.2015
 0.2055
 0.2124
 0.2271
 0.2188
 0.2188
 0.2074
 0.2040
 0.2330
 0.2581
 0.2228
 0.2231
 0.2298
 0.2422
 0.2507
 0.2276
 0.1678
 0.3001
 0.1829
 0.1975
 0.1935
 0.2241
 0.1993
 0.2353
 0.2538
 0.1172
 0.2374
 0.2285
 0.2320
 0.1895
 0.2017
 0.2319
 0.1781
 0.2241
 0.2789
 0.2471
 0.2000
 0.2124
 0.2254
 0.2087
 0.2252
 0.2318
[torch.FloatTensor of size 512]
), ('layer4.2.bn1.bias', 
-0.1556
-0.1563
-0.1872
-0.2099
-0.1670
-0.1957
-0.2123
-0.2006
-0.1196
-0.1273
-0.1174
-0.1501
-0.2137
-0.2261
-0.1976
-0.1978
-0.0450
-0.1765
-0.1590
-0.2102
-0.0738
-0.1973
-0.1026
-0.1764
-0.1863
-0.1545
-0.1914
-0.1765
-0.1642
-0.1557
-0.1453
-0.2747
-0.1896
-0.2021
-0.1797
-0.0736
-0.1957
-0.1821
-0.1435
-0.0305
-0.1680
-0.1743
-0.1843
-0.1268
-0.1497
-0.1741
-0.2016
-0.1601
-0.1969
-0.2855
-0.1622
-0.1189
-0.1357
-0.2401
-0.2096
-0.1487
-0.2270
-0.2302
-0.1880
-0.1394
-0.1454
-0.2186
-0.1764
-0.0144
-0.2049
-0.1993
-0.0677
-0.0908
-0.1063
-0.2137
-0.2243
-0.2320
-0.0448
-0.2091
-0.0047
-0.1591
-0.1667
-0.1892
-0.2412
-0.1843
-0.0912
-0.1781
-0.0919
-0.0472
-0.1586
-0.2129
-0.1058
-0.2240
-0.1887
-0.2116
-0.1377
-0.2800
-0.2572
-0.1953
-0.2334
-0.2214
-0.1913
-0.2564
-0.2492
-0.2038
-0.1990
-0.1442
-0.1948
-0.2404
-0.1384
-0.1773
-0.1683
-0.2344
-0.0757
-0.2658
-0.2241
-0.1786
-0.1804
-0.1641
-0.1201
-0.1664
-0.1764
-0.1926
-0.2430
-0.1925
-0.1697
-0.1811
-0.1796
-0.0547
-0.0901
-0.1486
-0.2577
-0.0622
-0.2031
-0.2077
-0.1463
-0.2122
-0.2715
-0.1592
-0.1345
-0.2431
-0.2050
-0.1773
-0.2569
-0.0627
-0.1993
-0.1424
-0.1975
-0.1378
-0.1862
-0.1959
-0.1989
-0.1641
-0.2028
-0.2688
-0.1123
-0.2369
-0.1992
-0.1645
-0.1872
-0.1810
-0.1759
-0.2363
-0.1402
-0.2302
-0.1696
 0.0619
-0.1606
-0.1912
-0.2267
-0.1761
-0.1011
-0.2050
-0.1862
-0.1868
-0.2193
-0.2363
-0.1394
-0.1673
-0.2303
-0.1745
-0.0515
-0.1695
-0.1667
-0.1876
-0.1042
-0.1820
-0.2105
-0.2196
-0.1143
-0.1294
-0.2042
-0.1650
-0.2227
-0.1175
-0.1697
-0.2818
 0.0566
-0.2376
-0.2212
-0.2032
-0.1573
-0.1821
-0.1915
-0.1888
-0.2231
-0.1454
-0.1520
-0.2389
-0.0801
-0.2559
-0.1428
-0.1717
-0.0797
-0.2664
-0.2097
-0.1525
-0.1899
-0.1752
-0.1635
-0.2333
-0.1571
-0.1880
-0.2416
-0.1457
-0.1842
-0.2325
-0.0299
-0.0210
-0.1633
-0.2051
-0.0554
-0.1654
-0.1426
-0.1518
-0.1109
-0.2070
-0.1876
-0.1706
-0.1538
-0.2534
-0.1746
-0.1705
-0.1646
-0.2128
-0.2240
-0.1411
-0.1699
-0.1424
-0.1578
-0.1609
-0.1927
-0.2198
-0.1428
-0.2520
-0.1255
-0.3133
-0.1463
-0.2294
-0.1804
-0.2154
-0.1645
-0.2292
-0.2220
-0.1929
-0.2182
-0.1414
-0.2807
-0.0872
-0.1587
-0.2334
-0.1928
-0.2548
-0.1284
-0.2228
-0.1710
-0.1632
-0.1425
-0.1791
-0.2446
-0.1127
-0.1979
-0.0389
-0.1928
-0.1708
-0.2000
-0.1407
-0.0783
-0.0097
-0.1207
-0.2162
-0.1367
-0.2190
-0.1761
-0.1442
-0.2626
-0.1703
-0.1758
-0.2606
-0.2430
-0.0659
-0.2141
-0.1888
-0.1692
-0.1480
-0.1140
-0.2093
 0.0616
-0.1473
-0.2207
-0.2146
-0.1735
-0.2264
-0.0112
-0.2395
 0.0523
-0.2161
 0.0635
-0.1775
-0.1730
-0.1342
-0.1964
-0.2164
-0.1062
-0.1401
-0.2513
-0.1750
-0.1475
-0.1535
-0.2049
-0.2186
-0.1333
-0.1994
-0.2295
-0.2116
-0.2473
-0.1953
-0.2031
-0.1720
-0.1933
-0.2416
-0.1511
-0.1351
-0.2554
 0.0785
-0.1196
-0.1831
-0.1289
-0.1774
-0.2408
-0.1777
-0.0376
-0.2609
-0.2069
-0.1423
-0.0846
-0.0867
-0.1596
-0.1913
-0.2025
-0.0936
-0.1905
-0.1752
-0.1970
-0.1778
-0.2656
-0.1741
-0.1796
-0.1071
-0.2513
-0.1919
-0.1904
-0.1841
-0.1407
-0.2235
-0.3347
-0.1682
-0.1734
-0.1268
-0.1942
-0.1666
-0.2194
-0.1450
-0.1479
-0.0852
-0.2006
-0.1029
-0.2197
-0.2144
-0.1703
 0.0454
-0.1796
-0.1980
-0.1769
-0.1632
-0.2167
-0.2317
-0.1639
-0.0352
-0.1490
-0.0918
-0.1983
-0.1199
-0.2107
-0.2528
-0.1682
-0.2438
-0.2076
-0.2585
-0.2664
-0.1922
-0.1960
-0.1994
-0.1818
-0.1843
-0.1426
-0.1707
-0.1122
-0.0938
-0.1724
-0.2033
 0.0811
-0.1943
-0.1326
-0.1924
-0.0743
-0.1851
-0.2087
-0.1570
-0.1808
-0.1016
-0.1977
-0.1764
-0.2468
-0.0771
-0.1823
-0.1295
-0.1669
-0.1828
-0.2115
-0.2660
-0.2096
-0.1656
-0.2044
-0.1068
-0.2301
-0.1662
-0.2261
-0.2455
-0.1257
-0.1797
-0.1862
-0.2740
-0.1459
-0.1560
-0.1219
-0.2025
 0.0716
-0.2226
-0.1737
-0.1684
-0.1860
-0.1658
-0.1381
-0.2117
-0.2476
-0.1843
-0.1512
-0.1839
-0.2647
-0.1146
-0.2773
-0.0833
-0.1997
-0.1027
-0.1202
-0.1392
-0.1433
-0.2185
-0.1657
-0.1625
-0.1478
-0.1418
-0.2365
-0.2930
-0.1897
-0.1926
-0.1880
-0.2259
-0.2378
-0.2199
-0.0692
-0.0794
-0.1015
-0.1274
-0.1301
-0.1931
-0.1450
-0.2111
-0.2627
 0.0520
-0.2241
-0.2148
-0.1888
-0.1013
-0.1391
-0.2104
-0.1049
-0.1919
-0.3346
-0.2700
-0.1254
-0.1498
-0.1665
-0.1445
-0.1841
-0.2120
[torch.FloatTensor of size 512]
), ('layer4.2.bn1.running_mean', 
-0.2626
-0.5603
-0.3218
-0.3026
-0.3882
-0.3460
-0.2490
-0.3176
-0.1872
-0.3276
-0.5745
-0.4072
-0.4119
-0.2654
-0.4331
-0.4738
 0.6992
-0.2551
-0.4248
-0.3124
-0.2453
-0.1248
-0.1702
-0.2058
-0.5727
-0.1815
-0.3869
-0.4980
-0.4941
-0.4376
-0.4637
-0.4668
-0.5546
-0.3624
-0.6562
-0.1853
-0.2636
-0.3020
-0.3448
-0.1832
-0.3150
-0.2526
-0.2487
-0.3653
-0.2388
-0.3602
-0.3010
-0.3598
-0.5975
-0.5111
-0.3354
-0.1312
-0.3208
-0.6275
-0.5224
-0.3760
-0.6632
-0.4121
-0.4938
-0.2794
-0.6104
-0.4372
-0.2818
-0.3208
-0.1961
-0.5246
-0.0471
-0.3280
-0.0564
-0.2588
-0.4986
-0.4464
 0.0571
-0.3652
 0.1279
-0.2844
-0.3162
-0.6053
-0.2662
-0.3758
-0.3633
-0.2908
-0.3413
 0.0736
-0.1069
-0.2573
-0.2510
-0.4679
 0.0301
-0.3414
-0.2671
-0.6389
-0.3351
-0.2362
-0.4770
-0.5192
-0.4226
-0.4181
-0.4680
-0.4060
-0.5017
-0.4201
-0.7437
-0.5236
-0.4498
-0.2936
-0.4261
-0.2603
-0.3010
-0.4279
-0.5260
-0.5895
-0.2359
-0.3987
-0.2533
-0.4317
-0.2313
-0.1964
-0.5829
-0.3285
-0.2869
-0.4254
-0.3951
-0.3332
-0.2275
-0.2995
-0.3587
-0.0067
-0.3125
-0.3505
-0.4625
-0.3215
-0.4186
-0.3902
-0.3696
-0.3636
-0.4727
-0.3271
-0.3072
-0.1086
-0.1057
-0.3453
-0.3433
-0.4330
-0.5765
-0.3889
-0.4758
-0.2295
-0.4689
-0.5516
-0.3910
-0.5385
-0.5414
-0.2803
-0.4891
-0.3643
-0.2566
-0.2181
-0.3898
-0.3635
-0.3600
-0.0905
-0.1855
-0.4156
-0.4898
-0.2324
-0.2719
-0.4100
-0.4233
-0.3300
-0.3459
-0.2799
-0.4289
-0.3900
-0.3903
-0.4044
-0.7143
-0.3203
-0.6184
-0.2456
 0.0037
-0.3670
-0.4941
-0.1995
-0.2840
-0.4368
-0.5597
-0.2941
-0.2228
-0.1143
-0.2332
-0.6375
-0.0196
-0.2803
-0.1003
-0.3071
-0.5130
-0.4361
-0.3544
-0.3357
-0.3802
-0.4787
-0.2833
-0.4871
-0.1793
-0.4844
-0.5029
-0.2571
-0.2562
-0.5134
-0.4115
-0.1701
-0.3699
-0.3609
-0.3259
-0.5685
-0.1643
-0.2973
-0.3248
-0.4490
-0.3260
-0.4661
-0.2283
-0.0034
-0.2615
-0.5619
-0.2959
-0.0931
-0.3002
-0.5209
-0.4617
-0.4811
-0.1732
-0.3999
-0.3744
-0.5251
-0.1720
-0.3383
-0.1979
-0.4211
-0.5537
-0.3192
-0.1568
-0.2705
-0.2539
-0.3762
-0.6078
-0.3154
-0.4958
-0.2711
-0.3193
-0.3666
-0.1885
-0.3331
-0.3666
-0.3802
-0.3635
-0.4391
-0.3221
-0.4120
-0.3666
-0.3346
-0.4466
-0.2352
-0.4104
-0.5311
-0.2942
-0.4936
-0.1812
-0.2294
-0.4051
-0.1460
-0.3588
-0.4241
-0.4480
-0.4235
 0.7152
-0.3351
-0.3736
-0.3666
-0.4096
-0.2864
-0.4245
-0.0913
-0.5429
-0.5886
-0.0483
-0.3643
-0.4309
-0.5026
-0.4856
-0.4762
-0.3384
-0.5513
-0.6750
-0.4400
-0.2395
-0.3700
-0.3126
-0.2272
-0.2246
-0.3913
 0.2828
-0.3416
-0.6286
-0.5936
-0.2959
-0.4543
-0.1445
-0.5037
 0.0496
-0.3536
-0.3509
-0.4357
-0.2669
-0.3446
-0.3023
-0.4708
-0.6876
-0.4062
-0.4402
-0.3741
-0.1505
-0.4239
-0.2551
-0.5563
-0.2709
-0.3090
-0.4174
-0.3613
-0.4889
-0.4109
-0.3778
-0.1184
-0.5201
-0.2069
-0.2282
-0.2275
-0.4648
 0.4279
-0.1444
-0.3240
-0.4818
-0.3520
-0.4249
-0.3413
-0.3862
-0.4410
-0.6291
-0.3062
-0.3734
-0.1222
-0.2109
-0.3268
-0.5323
-0.2342
-0.4227
-0.2665
-0.6941
-0.3915
-0.4594
-0.1717
-0.4317
-0.2920
-0.4689
-0.4349
-0.5981
-0.3201
-0.1543
-0.5046
 2.2972
-0.3812
-0.3601
-0.5034
-0.2931
-0.2089
-0.4901
-0.2319
-0.2387
 0.0567
-0.2852
-0.4491
-0.3919
-0.5759
-0.4117
-0.4028
-0.2569
-0.4495
-0.3810
-0.3982
-0.3952
-0.1494
-0.3373
 0.0069
-0.1708
-0.1656
-0.4305
-0.2563
-0.4697
-0.4215
-0.3797
-0.5992
-0.3869
-0.4300
-0.5352
-0.4093
-0.3382
-0.3739
-0.5372
-0.3239
-0.3479
-0.3935
-0.2926
-0.4007
-0.2427
-0.5758
-0.1724
-0.4071
-0.4728
-0.1733
-0.2230
-0.3475
-0.3224
-0.2905
-0.4865
-0.0988
-0.2763
-0.2635
-0.6430
-0.3446
-0.2510
-0.4395
-0.2975
-0.4110
-0.3162
-0.4113
-0.4019
 0.0828
-0.5060
-0.3740
-0.3712
-0.1463
-0.3657
-0.5425
-0.2179
-0.2648
-0.4054
-0.4462
-0.5600
-0.1504
-0.3738
-0.3313
-0.2073
-0.6197
-0.2962
-0.1220
-0.2652
-0.5129
-0.1835
-0.3741
-0.3887
-0.1614
-0.3872
-0.1927
-0.4682
 0.0290
-0.5943
-0.5992
-0.4537
 0.0103
-0.1447
-0.2962
-0.4778
-0.4211
-0.3957
-0.4048
-0.3724
-0.4985
-0.1879
-0.4250
-0.3182
-0.2570
-0.4537
-0.4125
-0.5024
-0.4488
-0.2441
-0.9873
-0.3296
-0.2764
-0.3559
-0.3133
-0.2586
-0.3456
-0.4226
 0.1065
-0.4285
-0.4964
-0.5584
-0.0059
-0.2451
-0.4385
-0.0933
-0.1106
-0.6889
-0.3673
-0.3323
-0.4028
-0.2629
-0.3249
-0.2231
-0.5770
[torch.FloatTensor of size 512]
), ('layer4.2.bn1.running_var', 
 0.2621
 0.3216
 0.2449
 0.2115
 0.3268
 0.2435
 0.2156
 0.2307
 0.2695
 0.2751
 0.2825
 0.2512
 0.2563
 0.2037
 0.2354
 0.2638
 0.4754
 0.2649
 0.2370
 0.2415
 0.2728
 0.2451
 0.2361
 0.2171
 0.3015
 0.2697
 0.2468
 0.2789
 0.3304
 0.2917
 0.3452
 0.2318
 0.3255
 0.2574
 0.3032
 0.2646
 0.2618
 0.2798
 0.2798
 0.3068
 0.2609
 0.2642
 0.2329
 0.2804
 0.3044
 0.2758
 0.2431
 0.2710
 0.2863
 0.2182
 0.3449
 0.2841
 0.2519
 0.2694
 0.2632
 0.3126
 0.3638
 0.2616
 0.2657
 0.2560
 0.2917
 0.2495
 0.2844
 0.4718
 0.2657
 0.3068
 0.3445
 0.2991
 0.2642
 0.2470
 0.2525
 0.2515
 0.3081
 0.2446
 0.3432
 0.2712
 0.2238
 0.3013
 0.2443
 0.2402
 0.3475
 0.2576
 0.3270
 0.3539
 0.2733
 0.2255
 0.2899
 0.2278
 0.2351
 0.2396
 0.3011
 0.2404
 0.2224
 0.2479
 0.2246
 0.2240
 0.2469
 0.2309
 0.2633
 0.2634
 0.2791
 0.2518
 0.2800
 0.2446
 0.2977
 0.2752
 0.2420
 0.2187
 0.2845
 0.2397
 0.2798
 0.3142
 0.2763
 0.2981
 0.2911
 0.2840
 0.2811
 0.2724
 0.2351
 0.2382
 0.2590
 0.2495
 0.2484
 0.3141
 0.3129
 0.2751
 0.2403
 0.3563
 0.2469
 0.2236
 0.2915
 0.3248
 0.2187
 0.2662
 0.3039
 0.2682
 0.2716
 0.2511
 0.2502
 0.3170
 0.2251
 0.2531
 0.2507
 0.3078
 0.3041
 0.2269
 0.2614
 0.2576
 0.2605
 0.2548
 0.2604
 0.2639
 0.2277
 0.2928
 0.2573
 0.3470
 0.2346
 0.2562
 0.2768
 0.2591
 0.2468
 0.3630
 0.2377
 0.2364
 0.2634
 0.2696
 0.3163
 0.2683
 0.2288
 0.2532
 0.2583
 0.2396
 0.2456
 0.2459
 0.2765
 0.2787
 0.3769
 0.2458
 0.3137
 0.2286
 0.3002
 0.2732
 0.2225
 0.2333
 0.2699
 0.2767
 0.3008
 0.2911
 0.2491
 0.2733
 0.2699
 0.2331
 0.3383
 0.2443
 0.2187
 0.2160
 0.2684
 0.2911
 0.2737
 0.2731
 0.2388
 0.2742
 0.2566
 0.2151
 0.2666
 0.2221
 0.3124
 0.2516
 0.3028
 0.2210
 0.2550
 0.2486
 0.2588
 0.2554
 0.2719
 0.2526
 0.2366
 0.2273
 0.2148
 0.4157
 0.2862
 0.2299
 0.2674
 0.2972
 0.2810
 0.2581
 0.3225
 0.2483
 0.2455
 0.2363
 0.3372
 0.2682
 0.2831
 0.2891
 0.2696
 0.2112
 0.2317
 0.2806
 0.2584
 0.2218
 0.2283
 0.2977
 0.2597
 0.2588
 0.2350
 0.2814
 0.2353
 0.2185
 0.2884
 0.2114
 0.3295
 0.2334
 0.2370
 0.2295
 0.2726
 0.2383
 0.2576
 0.2500
 0.2626
 0.2841
 0.2556
 0.2931
 0.2637
 0.3387
 0.2636
 0.2403
 0.2602
 0.2746
 0.2925
 0.2565
 0.2692
 0.2738
 0.2711
 0.2962
 0.2272
 0.3080
 0.3414
 0.3478
 0.2546
 0.2585
 0.2292
 0.2588
 0.3912
 0.3005
 0.3348
 0.2614
 0.2407
 0.2506
 0.2759
 0.2892
 0.2189
 0.2635
 0.2661
 0.2208
 0.2231
 0.3373
 0.2450
 0.2829
 0.2600
 0.2835
 0.2619
 0.2330
 0.4313
 0.2553
 0.2945
 0.2591
 0.2616
 0.2385
 0.2954
 0.2719
 0.3859
 0.2689
 0.3475
 0.2407
 0.2934
 0.2799
 0.2849
 0.2963
 0.3132
 0.2532
 0.2291
 0.2250
 0.2885
 0.2962
 0.2658
 0.2861
 0.2414
 0.2619
 0.2094
 0.2571
 0.2542
 0.2646
 0.2616
 0.2483
 0.2497
 0.2166
 0.2678
 0.2486
 0.2767
 0.6513
 0.3164
 0.2430
 0.2631
 0.3476
 0.2502
 0.2500
 0.2929
 0.2127
 0.2541
 0.2715
 0.2728
 0.2793
 0.2598
 0.2706
 0.3016
 0.3030
 0.2334
 0.2553
 0.3031
 0.2689
 0.2281
 0.2191
 0.3079
 0.2798
 0.2787
 0.2834
 0.2556
 0.2538
 0.2560
 0.2791
 1.1206
 0.2174
 0.2662
 0.3038
 0.2415
 0.2518
 0.2055
 0.2739
 0.3175
 0.2857
 0.2426
 0.3349
 0.2652
 0.2995
 0.2906
 0.4672
 0.2541
 0.2569
 0.2342
 0.2695
 0.2242
 0.2061
 0.2754
 0.2974
 0.2351
 0.3158
 0.2182
 0.2401
 0.2757
 0.2193
 0.2332
 0.2422
 0.2356
 0.2282
 0.2908
 0.2720
 0.2450
 0.2382
 0.2733
 0.2740
 0.2910
 0.2249
 0.2779
 0.2586
 0.2297
 0.3190
 0.3093
 0.2724
 0.3375
 0.1893
 0.2817
 0.2382
 0.2737
 0.2084
 0.2653
 0.2809
 0.2366
 0.2550
 0.2485
 0.3287
 0.2296
 0.2809
 0.2599
 0.2409
 0.2590
 0.2638
 0.2453
 0.3010
 0.2139
 0.3168
 0.2148
 0.2226
 0.2734
 0.2440
 0.2436
 0.2736
 0.2583
 0.2373
 0.2968
 0.2626
 0.2675
 0.2658
 0.5061
 0.3259
 0.2765
 0.2989
 0.2276
 0.2645
 0.2431
 0.2333
 0.2308
 0.2826
 0.3089
 0.2737
 0.2277
 0.2492
 0.2703
 0.5312
 0.2247
 0.3026
 0.2297
 0.3076
 0.2984
 0.2398
 0.2680
 0.3097
 0.2821
 0.2705
 0.2148
 0.2240
 0.2385
 0.2548
 0.2858
 0.2283
 0.2508
 0.2341
 0.3604
 1.2090
 0.3078
 0.2214
 0.3070
 0.2045
 0.2542
 0.2394
 0.2725
 0.9146
 0.2464
 0.2471
 0.2915
 0.3195
 0.2404
 0.2455
 0.2645
 0.2559
 0.2457
 0.2264
 0.2538
 0.2843
 0.2743
 0.2981
 0.2957
 0.2691
[torch.FloatTensor of size 512]
), ('layer4.2.conv2.weight', 
( 0 , 0 ,.,.) = 
  4.0117e-03  6.4560e-03  7.5797e-03
  6.3191e-03  8.6818e-03  1.0155e-02
  1.0467e-02  7.4264e-03  1.0289e-02

( 0 , 1 ,.,.) = 
  8.9806e-03  8.8232e-03  1.2227e-02
  7.9335e-03  1.2900e-02  5.7527e-03
  7.0791e-03  1.0065e-02  4.1151e-03

( 0 , 2 ,.,.) = 
  6.3044e-03  5.0667e-03  5.7373e-03
 -2.9706e-04 -2.2638e-03  7.9506e-05
  6.0903e-04  3.4252e-04  2.0867e-03
    ... 

( 0 ,509,.,.) = 
 -4.3498e-03  6.7560e-03  3.5414e-03
  4.0447e-03  1.1341e-02  1.0289e-02
  6.2327e-03  1.8316e-02  1.1889e-02

( 0 ,510,.,.) = 
 -4.3072e-04 -5.9304e-04 -2.7477e-03
  1.7383e-03  1.1411e-03 -2.5095e-03
 -7.0297e-03 -8.4976e-03 -1.1197e-02

( 0 ,511,.,.) = 
 -6.5198e-03 -9.3173e-03 -6.2551e-03
 -3.1477e-04  1.2694e-03  5.0760e-03
 -1.5420e-03 -1.1563e-03  1.1397e-03
      ⋮  

( 1 , 0 ,.,.) = 
 -8.9216e-03 -5.1781e-03 -8.9250e-03
 -7.3784e-03 -2.7041e-03 -6.4671e-03
 -6.6261e-03 -2.8952e-04 -6.2928e-03

( 1 , 1 ,.,.) = 
  9.6507e-03  1.1344e-02  1.4753e-02
  3.7595e-03  7.9248e-03  1.0181e-02
  2.9932e-03  7.7303e-03  7.2364e-03

( 1 , 2 ,.,.) = 
  1.3329e-02  2.0545e-02  1.9574e-02
  1.4139e-02  1.7057e-02  1.8544e-02
  1.4318e-02  1.7824e-02  2.1029e-02
    ... 

( 1 ,509,.,.) = 
 -7.2171e-03 -1.0446e-02 -1.1656e-02
 -6.9924e-03 -7.2246e-03 -6.1957e-03
  4.4194e-03  2.9223e-03  4.9812e-03

( 1 ,510,.,.) = 
 -5.0227e-03 -9.9073e-03 -5.1346e-03
 -6.9575e-03 -6.8294e-03 -4.9583e-03
 -4.0500e-03 -4.9242e-03 -4.1461e-03

( 1 ,511,.,.) = 
  6.4591e-04  7.8191e-04 -2.3758e-03
  2.8326e-03  7.6060e-03  2.7473e-03
 -2.8813e-03  1.0263e-03 -7.1370e-03
      ⋮  

( 2 , 0 ,.,.) = 
 -2.0092e-03 -3.8951e-03 -7.9365e-03
 -8.7227e-03 -7.1758e-03 -1.0349e-02
 -6.6341e-03 -6.6840e-03 -7.7295e-03

( 2 , 1 ,.,.) = 
  2.7262e-02  2.2136e-02  3.1470e-02
  2.0322e-02  1.5134e-02  2.4656e-02
  1.6629e-02  1.5191e-02  2.1166e-02

( 2 , 2 ,.,.) = 
  1.4919e-02  1.1747e-02  1.6182e-02
  1.5794e-02  4.7102e-03  1.4320e-02
  1.6125e-02  9.6735e-03  1.3646e-02
    ... 

( 2 ,509,.,.) = 
 -1.0187e-02 -1.2948e-02 -1.2582e-02
 -5.8166e-03 -9.2604e-03 -1.0283e-02
 -9.7321e-03 -1.4145e-02 -1.2022e-02

( 2 ,510,.,.) = 
 -1.0733e-02 -1.1489e-02 -8.0587e-03
 -8.3126e-03 -9.2558e-03 -4.4427e-03
 -5.1682e-03 -1.2494e-02 -8.8080e-03

( 2 ,511,.,.) = 
 -1.8538e-02 -2.2482e-02 -1.8929e-02
 -1.2926e-02 -1.1466e-02 -1.2845e-02
 -1.1635e-02 -1.3297e-02 -1.0621e-02
...     
      ⋮  

(509, 0 ,.,.) = 
  3.6848e-03  3.9875e-03  9.5603e-04
  1.0862e-03  7.5079e-04  2.0638e-03
 -3.1387e-03 -3.3409e-03 -7.9984e-04

(509, 1 ,.,.) = 
 -1.0203e-02 -1.1155e-02 -1.2017e-02
 -6.8558e-03 -3.6928e-03 -7.0323e-03
  3.7379e-04  4.4168e-03 -3.2229e-03

(509, 2 ,.,.) = 
 -1.9583e-03 -1.4304e-03 -1.2296e-03
  1.5170e-03 -7.2486e-04 -3.8884e-04
  3.4177e-03 -1.4350e-03 -2.0299e-04
    ... 

(509,509,.,.) = 
  1.4414e-03  4.6242e-03  5.3990e-03
  1.0178e-02  1.6874e-02  1.9453e-02
  4.8118e-03  1.1081e-02  1.2913e-02

(509,510,.,.) = 
  4.3356e-03  2.5304e-03  6.3943e-03
 -1.5454e-03 -2.6431e-03  1.1696e-03
 -6.9420e-03 -4.1809e-03 -4.5079e-03

(509,511,.,.) = 
  2.8397e-02  2.3649e-02  3.4782e-02
  2.5609e-02  1.8501e-02  3.1097e-02
  2.8974e-02  2.0715e-02  3.1061e-02
      ⋮  

(510, 0 ,.,.) = 
 -4.6084e-03 -1.5983e-02 -1.5005e-02
 -8.9812e-03 -1.3049e-02 -1.6584e-02
 -1.1798e-02 -1.2441e-02 -1.3363e-02

(510, 1 ,.,.) = 
  2.0662e-02  1.8107e-02  1.5833e-02
  2.1960e-02  1.4245e-02  2.0089e-02
  2.0738e-02  1.8482e-02  2.0067e-02

(510, 2 ,.,.) = 
 -6.4609e-03 -3.7295e-03 -5.2296e-03
 -2.6789e-03 -2.6843e-03 -5.5198e-03
 -6.6036e-03 -5.2171e-03 -6.2095e-03
    ... 

(510,509,.,.) = 
  1.0919e-02  9.0998e-03  6.3412e-03
  1.1221e-02  9.0333e-03  8.0636e-03
  3.2362e-03  4.1336e-03 -2.1570e-03

(510,510,.,.) = 
 -1.0596e-02 -1.2175e-02 -9.6459e-03
 -1.7528e-03 -4.9599e-03 -7.4674e-04
 -1.1956e-03 -3.2552e-03 -4.2784e-04

(510,511,.,.) = 
 -5.8218e-03 -4.8902e-03 -7.8034e-03
 -5.0635e-03 -5.6798e-03 -3.5475e-03
 -3.1549e-03 -3.7742e-03 -1.5114e-03
      ⋮  

(511, 0 ,.,.) = 
  2.9647e-03  2.2503e-03 -2.1969e-04
 -3.0770e-03 -2.6898e-03 -8.3853e-04
 -2.9215e-03 -2.8657e-03 -2.7692e-03

(511, 1 ,.,.) = 
  1.2049e-02  1.2095e-02  1.1018e-02
  1.3211e-02  8.8383e-03  1.4752e-02
  1.7452e-02  1.7951e-02  1.8580e-02

(511, 2 ,.,.) = 
 -2.3812e-03 -1.2207e-02 -1.1278e-02
 -3.3263e-03 -3.7232e-03 -1.0250e-02
 -5.1692e-03 -7.3431e-03 -1.3124e-02
    ... 

(511,509,.,.) = 
  2.8325e-04  1.8360e-03 -4.6064e-03
 -1.4945e-04 -2.3635e-03 -7.8466e-03
 -5.4216e-03 -6.0023e-03 -5.1872e-03

(511,510,.,.) = 
 -3.5320e-04 -1.8844e-03 -5.7870e-04
 -5.6706e-03 -2.6146e-03  1.0965e-03
 -6.2577e-04  3.7468e-03 -1.5856e-04

(511,511,.,.) = 
 -1.9726e-03 -5.1465e-03 -6.4104e-03
 -6.4423e-03 -1.1479e-02 -1.2132e-02
 -1.3768e-02 -1.4513e-02 -1.8401e-02
[torch.FloatTensor of size 512x512x3x3]
), ('layer4.2.bn2.weight', 
 0.2260
 0.1807
 0.1841
 0.2345
 0.2439
 0.1779
 0.2456
 0.2356
 0.2234
 0.2278
 0.2040
 0.2543
 0.2216
 0.2049
 0.2018
 0.2867
 0.2227
 0.1987
 0.2191
 0.1870
 0.1829
 0.1995
 0.1810
 0.2308
 0.2484
 0.2377
 0.1960
 0.1859
 0.2132
 0.2252
 0.2012
 0.1483
 0.2979
 0.2266
 0.2426
 0.2005
 0.1925
 0.1972
 0.2443
 0.1987
 0.1844
 0.2068
 0.2257
 0.2203
 0.2203
 0.2308
 0.1911
 0.2165
 0.2275
 0.2027
 0.1966
 0.1910
 0.2303
 0.2196
 0.2043
 0.2051
 0.2242
 0.2122
 0.1982
 0.2262
 0.2216
 0.2145
 0.1894
 0.2069
 0.2078
 0.1779
 0.1906
 0.1344
 0.2233
 0.2471
 0.2210
 0.1751
 0.2248
 0.2056
 0.2112
 0.2252
 0.2548
 0.2046
 0.2169
 0.2254
 0.2072
 0.1979
 0.1969
 0.2173
 0.2423
 0.1994
 0.1645
 0.2098
 0.1969
 0.2374
 0.2047
 0.2413
 0.2246
 0.2031
 0.2330
 0.2173
 0.2198
 0.2254
 0.2537
 0.2513
 0.1749
 0.2341
 0.2185
 0.1938
 0.2178
 0.2238
 0.1996
 0.2104
 0.2161
 0.2078
 0.2319
 0.1875
 0.2319
 0.1956
 0.2068
 0.2110
 0.1882
 0.2517
 0.1834
 0.2247
 0.1754
 0.2183
 0.2284
 0.2004
 0.2222
 0.2430
 0.2453
 0.2217
 0.2083
 0.1983
 0.1748
 0.1980
 0.2227
 0.1982
 0.1872
 0.2004
 0.2243
 0.2000
 0.1869
 0.2303
 0.1721
 0.2558
 0.1800
 0.1763
 0.2148
 0.2216
 0.2426
 0.2002
 0.2212
 0.1965
 0.2189
 0.2310
 0.2153
 0.2361
 0.1721
 0.1846
 0.1926
 0.2165
 0.2150
 0.2853
 0.1851
 0.2329
 0.2228
 0.2037
 0.2379
 0.1941
 0.2224
 0.1898
 0.1967
 0.2040
 0.2264
 0.2253
 0.2300
 0.2866
 0.2814
 0.2037
 0.2277
 0.2479
 0.2290
 0.2232
 0.1869
 0.2117
 0.2359
 0.2254
 0.2141
 0.2470
 0.2101
 0.2399
 0.2420
 0.2411
 0.1968
 0.2039
 0.2234
 0.2398
 0.1909
 0.1938
 0.2121
 0.2114
 0.2014
 0.2023
 0.1854
 0.2008
 0.1917
 0.1904
 0.1933
 0.1810
 0.1998
 0.1751
 0.1697
 0.1902
 0.2151
 0.2029
 0.2018
 0.2316
 0.2177
 0.2625
 0.2750
 0.1903
 0.2448
 0.2286
 0.1968
 0.1789
 0.1863
 0.2176
 0.1854
 0.2115
 0.2008
 0.2620
 0.2323
 0.1785
 0.2029
 0.1886
 0.1771
 0.1704
 0.2042
 0.2175
 0.2190
 0.1728
 0.1781
 0.1984
 0.1760
 0.2028
 0.2210
 0.2098
 0.1604
 0.2524
 0.1990
 0.2395
 0.2130
 0.1992
 0.2376
 0.2212
 0.2229
 0.1970
 0.2190
 0.1922
 0.2726
 0.2488
 0.2102
 0.2135
 0.1978
 0.1750
 0.2057
 0.2373
 0.1771
 0.1987
 0.2012
 0.2152
 0.2240
 0.2321
 0.2671
 0.1822
 0.1881
 0.1984
 0.2053
 0.2215
 0.1620
 0.2056
 0.2067
 0.2472
 0.2254
 0.2224
 0.1738
 0.1835
 0.2070
 0.1998
 0.2083
 0.2022
 0.2385
 0.2105
 0.2140
 0.1598
 0.1792
 0.2460
 0.2092
 0.2030
 0.2145
 0.2099
 0.1807
 0.1895
 0.2026
 0.1947
 0.1970
 0.2123
 0.2078
 0.1992
 0.2009
 0.2248
 0.1913
 0.2327
 0.2060
 0.1828
 0.2211
 0.2013
 0.2053
 0.2127
 0.2225
 0.2141
 0.1946
 0.2061
 0.2197
 0.2151
 0.1670
 0.2045
 0.2157
 0.2118
 0.2171
 0.2005
 0.2520
 0.2530
 0.2228
 0.2020
 0.2464
 0.2515
 0.1760
 0.1996
 0.2071
 0.2143
 0.2148
 0.2328
 0.2001
 0.2097
 0.1920
 0.2368
 0.2172
 0.1864
 0.2163
 0.2024
 0.2177
 0.1788
 0.2351
 0.2312
 0.2060
 0.2259
 0.1932
 0.2236
 0.1876
 0.2167
 0.1766
 0.2775
 0.2028
 0.2111
 0.1855
 0.2200
 0.2272
 0.1749
 0.2214
 0.2166
 0.2530
 0.1953
 0.2028
 0.2247
 0.1922
 0.2219
 0.1850
 0.2043
 0.2166
 0.2163
 0.1951
 0.2201
 0.2618
 0.2291
 0.2533
 0.1997
 0.2491
 0.2439
 0.2143
 0.1866
 0.2176
 0.1693
 0.1611
 0.2155
 0.2256
 0.2587
 0.2461
 0.1938
 0.2017
 0.1787
 0.2486
 0.1991
 0.2391
 0.2064
 0.2166
 0.2349
 0.2051
 0.1981
 0.2142
 0.2314
 0.1854
 0.1947
 0.2153
 0.2271
 0.1937
 0.2024
 0.2223
 0.1892
 0.2219
 0.2249
 0.2071
 0.2350
 0.2109
 0.1946
 0.2004
 0.2214
 0.2135
 0.2048
 0.2501
 0.2216
 0.1864
 0.2303
 0.1981
 0.2717
 0.2243
 0.2129
 0.2416
 0.2498
 0.1985
 0.1933
 0.1886
 0.1971
 0.1979
 0.2177
 0.2004
 0.2077
 0.1989
 0.2008
 0.2175
 0.2056
 0.2410
 0.2232
 0.2205
 0.1810
 0.2313
 0.2451
 0.1835
 0.2372
 0.1907
 0.2206
 0.1862
 0.2177
 0.1903
 0.1860
 0.1838
 0.2027
 0.2431
 0.1943
 0.2269
 0.2288
 0.1937
 0.2091
 0.2050
 0.2028
 0.2096
 0.2308
 0.2839
 0.2203
 0.1669
 0.1748
 0.2042
 0.2320
 0.1934
 0.1954
 0.1889
 0.1843
 0.2199
 0.1845
 0.2103
 0.2322
 0.2126
 0.2254
 0.2299
 0.2154
 0.1733
 0.1817
 0.2290
 0.2111
 0.3290
 0.2199
 0.2123
 0.2205
 0.2340
 0.2248
 0.1878
 0.2234
 0.2227
 0.2509
 0.2065
 0.1985
 0.1975
 0.2067
 0.1925
 0.2127
[torch.FloatTensor of size 512]
), ('layer4.2.bn2.bias', 
-0.0998
-0.0088
-0.0588
-0.1434
-0.1690
-0.0280
-0.1573
-0.1458
-0.1280
-0.1191
-0.0186
-0.1582
-0.1301
-0.0648
-0.0699
-0.2021
-0.1048
-0.0709
-0.0927
-0.0397
-0.0354
-0.0483
-0.0352
-0.1410
-0.1370
-0.1513
-0.0826
-0.0532
-0.0754
-0.1187
-0.0662
 0.0215
-0.2279
-0.1512
-0.1770
-0.0544
-0.0676
-0.0820
-0.1557
-0.0519
-0.0535
-0.0767
-0.0951
-0.1177
-0.1211
-0.0916
-0.0646
-0.1053
-0.1050
-0.0737
-0.0623
-0.0402
-0.1109
-0.1002
-0.0654
-0.0680
-0.1204
-0.0911
-0.0620
-0.1142
-0.0985
-0.0970
-0.0693
-0.0783
-0.0859
-0.0066
-0.0385
 0.2013
-0.0928
-0.1404
-0.1103
-0.0441
-0.1294
-0.0817
-0.0657
-0.1051
-0.1563
-0.0701
-0.0899
-0.0735
-0.0867
-0.0592
-0.0406
-0.1170
-0.1424
-0.0821
 0.0241
-0.0837
-0.0597
-0.1295
-0.0728
-0.1332
-0.1105
-0.0808
-0.1123
-0.1019
-0.0975
-0.1335
-0.1550
-0.1464
-0.0365
-0.1335
-0.1080
-0.0553
-0.0964
-0.1106
-0.0482
-0.0667
-0.1304
-0.0806
-0.1240
-0.0143
-0.1219
-0.0463
-0.0989
-0.0896
-0.0208
-0.1634
-0.0257
-0.1212
-0.0296
-0.0794
-0.1002
-0.0604
-0.1029
-0.1579
-0.1288
-0.1047
-0.0760
-0.0829
-0.0159
-0.0747
-0.0974
-0.0503
-0.0542
-0.0819
-0.0942
-0.0558
-0.0643
-0.1054
-0.0136
-0.1777
-0.0418
-0.0354
-0.0988
-0.1049
-0.1467
-0.0553
-0.1006
-0.0767
-0.1002
-0.1035
-0.0836
-0.1244
-0.0200
-0.0657
-0.0601
-0.0989
-0.1023
-0.2241
-0.0541
-0.1328
-0.1026
-0.0799
-0.1342
-0.0669
-0.1246
-0.0509
-0.0727
-0.0735
-0.1136
-0.1300
-0.1253
-0.2721
-0.2021
-0.0858
-0.1243
-0.1487
-0.1287
-0.0921
-0.0410
-0.1020
-0.1540
-0.1215
-0.0998
-0.1586
-0.0840
-0.1616
-0.1492
-0.1420
-0.0550
-0.0579
-0.0879
-0.1546
-0.0427
-0.0609
-0.0997
-0.0763
-0.0764
-0.0724
-0.0085
-0.0983
-0.0562
-0.0341
-0.0586
-0.0549
-0.0548
-0.0203
 0.0103
-0.0410
-0.0873
 0.0380
-0.0917
-0.1302
-0.1086
-0.1884
-0.1897
-0.0433
-0.1495
-0.1272
-0.0478
-0.0194
-0.0253
-0.1104
-0.0577
-0.0684
-0.0398
-0.1630
-0.1375
-0.0119
-0.0782
-0.0509
-0.0245
-0.0287
-0.0854
-0.1037
-0.1003
-0.0379
-0.0179
-0.0573
-0.0078
-0.0463
-0.1099
-0.0795
 0.0836
-0.1549
-0.0829
-0.1448
-0.1071
-0.0664
-0.1245
-0.1029
-0.0962
-0.0631
-0.1058
-0.0805
-0.2027
-0.1589
-0.1059
-0.0902
-0.0626
-0.0151
-0.0753
-0.1321
-0.0282
-0.0451
-0.0682
-0.0940
-0.1047
-0.1347
-0.1870
-0.0084
-0.0368
-0.0632
-0.0782
-0.1088
 0.0117
-0.0751
-0.0921
-0.1793
-0.1111
-0.1199
-0.0469
-0.0715
-0.0902
-0.0732
-0.0751
-0.0937
-0.1238
-0.0748
-0.0633
 0.0073
-0.0208
-0.1269
-0.0951
-0.0816
-0.0817
-0.0874
-0.0270
-0.0506
-0.0397
-0.0568
-0.0524
-0.1116
-0.0693
-0.0775
-0.0886
-0.1013
-0.0533
-0.1373
-0.0689
-0.0312
-0.1149
-0.0686
-0.0736
-0.0764
-0.1041
-0.1202
-0.0572
-0.0910
-0.0903
-0.0685
-0.0134
-0.0685
-0.1033
-0.0949
-0.0986
-0.0759
-0.1696
-0.1627
-0.0939
-0.0749
-0.1218
-0.1558
-0.0251
-0.0727
-0.0744
-0.1126
-0.1010
-0.1218
-0.0671
-0.1002
-0.0606
-0.0981
-0.1014
-0.0573
-0.0883
-0.0640
-0.1191
-0.0105
-0.1107
-0.1162
-0.0668
-0.1015
-0.0597
-0.1427
-0.0294
-0.0942
-0.0184
-0.1984
-0.0698
-0.0723
-0.0324
-0.0965
-0.1633
-0.0068
-0.1026
-0.1037
-0.1527
-0.0435
-0.0647
-0.1073
-0.0526
-0.1353
-0.0369
-0.0871
-0.1052
-0.0961
-0.0830
-0.0958
-0.1381
-0.1304
-0.1635
-0.0761
-0.1604
-0.1647
-0.0862
-0.0475
-0.0939
-0.0177
 0.0052
-0.0782
-0.0854
-0.1527
-0.1343
-0.0596
-0.0928
-0.0100
-0.1562
-0.0552
-0.1349
-0.0759
-0.1072
-0.1393
-0.0420
-0.0753
-0.1155
-0.1234
-0.0285
-0.0537
-0.0836
-0.1072
-0.0658
-0.0532
-0.0873
-0.0196
-0.1089
-0.1421
-0.0507
-0.1392
-0.0786
-0.0574
-0.0655
-0.0983
-0.0884
-0.0519
-0.1583
-0.0888
-0.0605
-0.1102
-0.0423
-0.2398
-0.0946
-0.1050
-0.1497
-0.1554
-0.0866
-0.0523
-0.0366
-0.0472
-0.0806
-0.1023
-0.0720
-0.0919
-0.0608
-0.0726
-0.0944
-0.0922
-0.1203
-0.1080
-0.0754
-0.0328
-0.1287
-0.1507
-0.0551
-0.1440
-0.0325
-0.1261
-0.0513
-0.0846
-0.0517
-0.0326
-0.0307
-0.0716
-0.1573
-0.0660
-0.1197
-0.1163
-0.0377
-0.1092
-0.0857
-0.0732
-0.0665
-0.1104
-0.1927
-0.1020
-0.0255
-0.0175
-0.0949
-0.1281
-0.0340
-0.0502
-0.0412
-0.0557
-0.1011
-0.0473
-0.1062
-0.1063
-0.0897
-0.1047
-0.1253
-0.1090
 0.0092
-0.0317
-0.1205
-0.0663
-0.2939
-0.1074
-0.0772
-0.0850
-0.1148
-0.1265
-0.0564
-0.1030
-0.1038
-0.1395
-0.0805
-0.0830
-0.0490
-0.0906
-0.0434
-0.1156
[torch.FloatTensor of size 512]
), ('layer4.2.bn2.running_mean', 
-0.0854
-0.0834
-0.0828
-0.0834
-0.1129
-0.0712
-0.0907
-0.0873
-0.0817
-0.0732
-0.1204
-0.1103
-0.0825
-0.0779
-0.0901
-0.0995
-0.1090
-0.1033
-0.0948
-0.0751
-0.0748
-0.0780
-0.0732
-0.1069
-0.0826
-0.0924
-0.0616
-0.0846
-0.0822
-0.0941
-0.0960
-0.0679
-0.1040
-0.0398
-0.0901
-0.1000
-0.0817
-0.0804
-0.1192
-0.0875
-0.0825
-0.1045
-0.0806
-0.0935
-0.0883
-0.1231
-0.0681
-0.0912
-0.0869
-0.0922
-0.0659
-0.0940
-0.1115
-0.0925
-0.0867
-0.0802
-0.0879
-0.0920
-0.0841
-0.0948
-0.0718
-0.0981
-0.0642
-0.0739
-0.0719
-0.0854
-0.0786
-0.0189
-0.1030
-0.0945
-0.0695
-0.0736
-0.0816
-0.0711
-0.0911
-0.1078
-0.0884
-0.0941
-0.0897
-0.1101
-0.0723
-0.0809
-0.1043
-0.0894
-0.0999
-0.0785
-0.0714
-0.0791
-0.0908
-0.0834
-0.0826
-0.0963
-0.0841
-0.0639
-0.1258
-0.0878
-0.0719
-0.0810
-0.1002
-0.1068
-0.0507
-0.0956
-0.1085
-0.0730
-0.0842
-0.0841
-0.1026
-0.1007
-0.0792
-0.0887
-0.0767
-0.1138
-0.0767
-0.0994
-0.0998
-0.0938
-0.0922
-0.1115
-0.0820
-0.1064
-0.0661
-0.1142
-0.0959
-0.0779
-0.0902
-0.0808
-0.0979
-0.0953
-0.1105
-0.0559
-0.0769
-0.0923
-0.1079
-0.0762
-0.0829
-0.0869
-0.0699
-0.1012
-0.0760
-0.1070
-0.0948
-0.0708
-0.0282
-0.0665
-0.0641
-0.1014
-0.0936
-0.0829
-0.1132
-0.0846
-0.0929
-0.0919
-0.0707
-0.1001
-0.0711
-0.0555
-0.0949
-0.0983
-0.0899
-0.1157
-0.0747
-0.1085
-0.0661
-0.0929
-0.0941
-0.0678
-0.0943
-0.0712
-0.1087
-0.0518
-0.1082
-0.0917
-0.0931
-0.1290
-0.1224
-0.0897
-0.0728
-0.0900
-0.1049
-0.0972
-0.0541
-0.1060
-0.0802
-0.1096
-0.0905
-0.0998
-0.0798
-0.0540
-0.0921
-0.1247
-0.1015
-0.1113
-0.0909
-0.0933
-0.0612
-0.1071
-0.0735
-0.1048
-0.0751
-0.0830
-0.0959
-0.0726
-0.0499
-0.0574
-0.1016
-0.0765
-0.0773
-0.0865
-0.0866
-0.0622
-0.0723
-0.0789
-0.0682
-0.0919
-0.0870
-0.0998
-0.1013
-0.0826
-0.0911
-0.0793
-0.0836
-0.0926
-0.0770
-0.0952
-0.0781
-0.0963
-0.0982
-0.1252
-0.0852
-0.0796
-0.0729
-0.0758
-0.0985
-0.0869
-0.0817
-0.0713
-0.0759
-0.0856
-0.0746
-0.0836
-0.0592
-0.0931
-0.1108
-0.0952
-0.0635
-0.1048
-0.0727
-0.0701
-0.0877
-0.0738
-0.0792
-0.1153
-0.0846
-0.0705
-0.0673
-0.0712
-0.1285
-0.0883
-0.0862
-0.0627
-0.0804
-0.0625
-0.0622
-0.0948
-0.0660
-0.0951
-0.0899
-0.0832
-0.1099
-0.0949
-0.0896
-0.0906
-0.0735
-0.0760
-0.0629
-0.1009
-0.0688
-0.0789
-0.0690
-0.0792
-0.0944
-0.0926
-0.0916
-0.0575
-0.0733
-0.0604
-0.0863
-0.0839
-0.0781
-0.1016
-0.0920
-0.0569
-0.0896
-0.1009
-0.0985
-0.0923
-0.0699
-0.0946
-0.0707
-0.0523
-0.0874
-0.1024
-0.0748
-0.0774
-0.0609
-0.0772
-0.0858
-0.0997
-0.0746
-0.0823
-0.1057
-0.0643
-0.0761
-0.0680
-0.0704
-0.0746
-0.1116
-0.1059
-0.1025
-0.0976
-0.1119
-0.1003
-0.1045
-0.0843
-0.0826
-0.0784
-0.1057
-0.0877
-0.0890
-0.0990
-0.1185
-0.0839
-0.0948
-0.1084
-0.0827
-0.0764
-0.0742
-0.0776
-0.0810
-0.0782
-0.0669
-0.0824
-0.0636
-0.1167
-0.0785
-0.0661
-0.0994
-0.0899
-0.0918
-0.0874
-0.1119
-0.0650
-0.0863
-0.0655
-0.0681
-0.1210
-0.0755
-0.0838
-0.0887
-0.1084
-0.0745
-0.0879
-0.0767
-0.1085
-0.0708
-0.0919
-0.0835
-0.0873
-0.0893
-0.0920
-0.1033
-0.1061
-0.0834
-0.0629
-0.1017
-0.0984
-0.0714
-0.0961
-0.0751
-0.1021
-0.1336
-0.0950
-0.1150
-0.0602
-0.1012
-0.0889
-0.1081
-0.0940
-0.0765
-0.0896
-0.1009
-0.0909
-0.0847
-0.1040
-0.0853
-0.0939
-0.0851
-0.1082
-0.1038
-0.0852
-0.0829
-0.0848
-0.1007
-0.0915
-0.1042
-0.0795
-0.0951
-0.0836
-0.0708
-0.1052
-0.0909
-0.0862
-0.0837
-0.0921
-0.0790
-0.0826
-0.0959
-0.1024
-0.1481
-0.1009
-0.0947
-0.0732
-0.0715
-0.0886
-0.0833
-0.0798
-0.0819
-0.1027
-0.0769
-0.1148
-0.0884
-0.1248
-0.0960
-0.0880
-0.0824
-0.0929
-0.0755
-0.0841
-0.0700
-0.1203
-0.0796
-0.1072
-0.0867
-0.0802
-0.0759
-0.0831
-0.0962
-0.0991
-0.1133
-0.0767
-0.0996
-0.0653
-0.0784
-0.0815
-0.0688
-0.0898
-0.0865
-0.0702
-0.0800
-0.0745
-0.0696
-0.0781
-0.0794
-0.0670
-0.0737
-0.0696
-0.0656
-0.0877
-0.0948
-0.0525
-0.0832
-0.0596
-0.0836
-0.0956
-0.1012
-0.0949
-0.0877
-0.0913
-0.0749
-0.0895
-0.1035
-0.0825
-0.0740
-0.0848
-0.1120
-0.0645
-0.0913
-0.1116
-0.0887
-0.0956
-0.0964
-0.0728
-0.0939
-0.0737
-0.0718
-0.0957
-0.1146
-0.0996
-0.0805
-0.0855
-0.0752
-0.0680
-0.0571
-0.0953
-0.0797
-0.0826
-0.0933
-0.0876
-0.0910
-0.0982
-0.0807
-0.0686
[torch.FloatTensor of size 512]
), ('layer4.2.bn2.running_var', 
1.00000e-02 *
  1.0961
  1.2417
  1.2641
  1.0483
  1.0326
  1.0879
  1.0528
  0.9180
  0.9669
  1.0871
  1.3802
  1.1701
  1.0262
  1.0587
  0.9646
  1.1599
  1.1610
  1.0458
  1.0670
  1.0309
  1.0868
  1.3219
  0.9375
  1.0764
  1.1977
  0.9227
  0.8010
  1.0586
  0.9785
  1.0142
  1.1581
  1.0418
  1.1479
  0.8152
  0.8607
  1.1217
  0.9872
  1.0741
  1.2988
  1.0384
  1.1463
  1.2154
  1.1659
  0.9377
  0.9457
  1.4372
  0.9719
  1.0481
  1.1728
  1.0358
  1.0079
  1.1229
  1.2152
  1.2789
  1.0514
  1.0748
  0.9190
  1.0664
  1.0364
  1.0321
  0.8695
  1.1898
  0.9402
  0.9321
  0.9291
  1.1939
  1.3237
  2.3669
  1.2213
  1.1055
  0.8802
  0.8962
  0.9929
  0.9694
  1.1854
  1.1183
  0.9133
  1.1374
  1.0352
  1.2370
  0.9574
  1.0889
  1.4560
  0.9917
  1.1315
  0.9148
  1.2376
  1.0916
  1.2222
  0.9087
  1.0502
  0.9644
  0.9914
  0.8805
  1.1762
  1.1916
  0.9196
  0.9408
  1.2860
  1.0518
  0.9301
  1.1327
  1.2032
  0.9632
  1.0531
  1.0146
  1.1574
  1.0651
  1.0091
  1.1373
  0.9437
  1.3668
  0.8910
  1.2294
  0.9759
  1.0252
  1.2254
  1.1548
  1.2433
  1.1095
  1.1008
  1.3807
  1.2275
  1.0474
  1.0884
  1.0368
  1.3687
  1.1642
  1.1502
  0.8419
  1.0608
  0.9562
  1.1485
  1.0270
  1.1450
  0.9698
  1.0774
  1.0998
  1.1135
  1.2529
  1.1901
  0.8797
  0.9109
  0.9886
  0.8906
  1.1502
  0.9303
  1.1344
  1.2160
  0.9200
  1.2096
  1.2694
  1.3039
  1.1596
  1.0970
  0.8778
  1.0808
  1.0208
  0.9839
  1.1245
  0.9915
  1.2036
  1.1206
  1.1043
  1.0321
  1.0152
  0.8938
  1.0550
  1.0606
  0.8450
  1.1811
  0.8397
  1.0693
  1.1154
  1.2336
  1.1054
  0.8514
  1.1224
  1.0492
  1.1925
  0.9554
  1.0042
  0.8965
  1.0776
  0.9686
  1.0148
  1.1089
  0.9677
  0.9885
  1.1841
  1.2027
  1.3050
  1.3088
  0.9304
  1.1139
  1.0967
  0.9204
  1.1235
  1.2011
  1.2389
  1.4827
  0.9397
  1.0289
  1.1095
  0.9238
  1.1036
  1.0885
  1.1026
  1.2044
  1.1024
  0.9577
  1.8363
  0.8207
  1.0870
  0.9321
  1.0786
  1.4279
  1.0256
  1.0525
  0.8692
  1.2797
  1.0799
  1.0304
  0.9784
  0.9971
  1.1737
  1.2852
  1.4152
  0.9431
  1.2468
  0.9091
  1.1190
  1.1675
  1.0257
  1.0007
  1.0810
  0.9685
  0.9775
  1.3237
  1.1213
  1.1510
  1.2409
  1.1363
  1.0501
  1.7053
  1.1360
  0.9431
  0.9305
  0.9902
  0.9562
  1.0369
  1.3102
  1.1237
  1.0654
  0.9150
  1.0254
  1.2197
  1.0056
  0.9587
  1.1049
  1.1016
  1.1966
  0.9571
  1.2657
  1.0758
  1.2197
  1.0273
  1.0542
  1.0777
  1.0022
  1.2016
  1.3771
  1.1019
  1.0173
  0.8645
  1.2272
  1.1433
  0.9676
  0.9991
  0.8090
  1.1019
  1.0360
  0.9200
  0.9436
  0.9823
  0.9428
  1.0808
  1.0333
  1.0419
  1.3187
  1.1943
  1.0354
  1.1045
  1.2274
  1.0164
  0.9840
  1.1914
  1.2783
  1.1532
  0.9939
  1.0310
  1.0275
  1.0231
  0.7736
  1.0404
  0.9957
  1.0621
  1.1157
  1.0133
  1.1286
  1.0381
  1.1115
  1.0430
  0.9410
  1.0364
  1.1340
  1.2625
  0.9909
  1.3001
  0.9621
  1.2486
  1.2474
  1.2222
  0.8812
  0.9082
  1.0453
  1.1715
  1.1042
  0.9224
  1.0528
  1.2593
  1.0932
  1.2417
  1.3342
  1.0530
  0.9383
  1.0931
  0.9919
  1.0465
  1.1457
  0.9736
  0.9916
  0.9837
  1.2719
  0.8803
  0.9273
  1.2449
  1.1703
  0.9789
  1.3596
  1.2436
  0.9563
  1.0069
  1.0193
  0.9067
  0.9691
  1.0761
  1.0325
  1.2665
  1.1957
  1.0837
  1.0494
  1.0842
  1.0746
  0.8534
  1.0091
  1.2025
  1.0987
  1.1972
  1.2404
  1.1681
  1.1875
  1.1088
  0.8421
  1.2013
  0.9727
  0.9939
  1.1097
  0.9117
  1.1268
  1.6413
  1.0345
  1.1208
  0.8680
  0.9680
  1.0526
  1.3911
  1.1524
  1.0583
  1.0775
  1.5498
  1.2365
  1.2884
  1.1445
  0.9229
  1.0514
  0.8950
  1.7591
  1.0145
  1.0608
  0.9312
  1.0006
  1.0250
  1.0482
  1.6271
  1.0055
  1.0117
  1.0408
  1.1566
  1.0456
  1.2145
  1.0543
  0.9031
  1.1615
  1.2230
  1.3490
  0.8977
  0.9717
  1.4797
  0.9664
  1.0366
  0.9950
  1.0512
  0.9600
  1.0331
  1.2383
  1.2090
  1.3579
  0.9602
  1.3252
  1.1721
  1.1093
  1.3537
  0.9713
  0.9475
  1.0086
  0.9740
  1.0198
  1.1013
  1.3184
  0.9389
  1.1939
  1.0733
  1.0617
  1.1793
  1.1269
  1.1429
  0.8578
  1.1406
  1.2019
  1.2233
  0.9882
  1.0208
  1.1037
  1.0425
  0.9020
  1.0474
  0.9658
  0.9997
  1.0374
  0.9758
  1.2961
  1.4090
  0.9610
  1.0449
  1.0132
  1.0169
  1.0121
  1.2108
  0.8972
  0.9473
  1.0183
  1.2093
  0.9714
  1.2020
  1.1685
  1.1362
  1.0852
  1.0393
  1.1738
  1.2492
  1.0988
  0.8887
  1.0235
  1.0731
  1.1348
  1.0751
  1.2306
  1.1760
  1.2484
  1.0452
  0.8210
  1.4388
  0.9851
  0.9382
  1.1786
  1.2558
  1.0266
  1.1311
  1.1168
  1.0910
  0.8363
  0.9718
  1.1507
  1.0352
  1.1362
  0.9449
  0.9511
  1.1678
  0.9916
  1.1138
  0.8978
[torch.FloatTensor of size 512]
), ('layer4.2.conv3.weight', 
( 0  , 0  ,.,.) = 
  4.9945e-03

( 0  , 1  ,.,.) = 
  8.3249e-03

( 0  , 2  ,.,.) = 
 -2.4038e-02
      ... 

( 0  ,509 ,.,.) = 
  5.9724e-03

( 0  ,510 ,.,.) = 
 -1.7599e-02

( 0  ,511 ,.,.) = 
  7.9416e-03
        ⋮  

( 1  , 0  ,.,.) = 
  2.6285e-03

( 1  , 1  ,.,.) = 
 -5.4645e-03

( 1  , 2  ,.,.) = 
  6.0172e-03
      ... 

( 1  ,509 ,.,.) = 
 -7.6950e-03

( 1  ,510 ,.,.) = 
  2.2687e-02

( 1  ,511 ,.,.) = 
  3.2305e-03
        ⋮  

( 2  , 0  ,.,.) = 
  8.0322e-03

( 2  , 1  ,.,.) = 
 -7.8471e-03

( 2  , 2  ,.,.) = 
  1.8648e-02
      ... 

( 2  ,509 ,.,.) = 
 -4.1171e-03

( 2  ,510 ,.,.) = 
 -1.9112e-02

( 2  ,511 ,.,.) = 
 -8.0315e-03
 ...      
        ⋮  

(2045, 0  ,.,.) = 
  3.0916e-03

(2045, 1  ,.,.) = 
 -6.7272e-03

(2045, 2  ,.,.) = 
  1.1115e-02
      ... 

(2045,509 ,.,.) = 
 -6.1974e-03

(2045,510 ,.,.) = 
 -8.7446e-03

(2045,511 ,.,.) = 
 -9.6589e-03
        ⋮  

(2046, 0  ,.,.) = 
  4.8518e-02

(2046, 1  ,.,.) = 
 -2.2156e-02

(2046, 2  ,.,.) = 
  3.2804e-04
      ... 

(2046,509 ,.,.) = 
  7.7394e-03

(2046,510 ,.,.) = 
  6.5305e-03

(2046,511 ,.,.) = 
 -8.6066e-03
        ⋮  

(2047, 0  ,.,.) = 
 -2.6602e-04

(2047, 1  ,.,.) = 
 -2.6791e-03

(2047, 2  ,.,.) = 
 -5.9936e-03
      ... 

(2047,509 ,.,.) = 
 -8.7326e-03

(2047,510 ,.,.) = 
  1.6933e-02

(2047,511 ,.,.) = 
 -1.2037e-02
[torch.FloatTensor of size 2048x512x1x1]
), ('layer4.2.bn3.weight', 
 0.6431
 0.8315
 0.7858
   ⋮   
 0.5616
 0.7822
 0.5634
[torch.FloatTensor of size 2048]
), ('layer4.2.bn3.bias', 
 0.0084
 0.0549
 0.0348
   ⋮   
 0.0219
 0.0207
 0.0076
[torch.FloatTensor of size 2048]
), ('layer4.2.bn3.running_mean', 
-6.2177e-03
-2.4681e-02
-3.0545e-03
     ⋮     
 1.5639e-05
 4.7883e-03
-4.7526e-03
[torch.FloatTensor of size 2048]
), ('layer4.2.bn3.running_var', 
1.00000e-03 *
  0.7171
  1.3794
  1.0266
    ⋮   
  0.5486
  1.0902
  0.5968
[torch.FloatTensor of size 2048]
), ('fconv.weight', 
( 0  , 0  ,.,.) = 
  2.1824e-01

( 0  , 1  ,.,.) = 
 -4.6338e-01

( 0  , 2  ,.,.) = 
 -3.0439e-01
      ... 

( 0  ,2045,.,.) = 
  2.1742e-01

( 0  ,2046,.,.) = 
 -6.3444e-02

( 0  ,2047,.,.) = 
 -1.6364e-01
        ⋮  

( 1  , 0  ,.,.) = 
  1.9439e-01

( 1  , 1  ,.,.) = 
 -1.2673e-01

( 1  , 2  ,.,.) = 
 -2.6543e-01
      ... 

( 1  ,2045,.,.) = 
  1.0648e-01

( 1  ,2046,.,.) = 
 -2.5667e-01

( 1  ,2047,.,.) = 
  6.7307e-01
        ⋮  

( 2  , 0  ,.,.) = 
 -6.1715e-01

( 2  , 1  ,.,.) = 
  9.4073e-02

( 2  , 2  ,.,.) = 
 -5.1460e-01
      ... 

( 2  ,2045,.,.) = 
 -4.2418e-02

( 2  ,2046,.,.) = 
  2.0075e-02

( 2  ,2047,.,.) = 
  1.6199e-01
 ...      
        ⋮  

( 18 , 0  ,.,.) = 
  3.1543e-02

( 18 , 1  ,.,.) = 
  1.6004e-01

( 18 , 2  ,.,.) = 
  2.5030e-01
      ... 

( 18 ,2045,.,.) = 
 -3.9406e-01

( 18 ,2046,.,.) = 
 -1.2512e-01

( 18 ,2047,.,.) = 
 -4.5061e-02
        ⋮  

( 19 , 0  ,.,.) = 
  4.3904e-01

( 19 , 1  ,.,.) = 
 -3.0772e-02

( 19 , 2  ,.,.) = 
 -4.6831e-01
      ... 

( 19 ,2045,.,.) = 
 -2.6033e-02

( 19 ,2046,.,.) = 
 -5.9088e-02

( 19 ,2047,.,.) = 
  1.5265e-01
        ⋮  

( 20 , 0  ,.,.) = 
 -7.3541e-02

( 20 , 1  ,.,.) = 
 -3.0770e-01

( 20 , 2  ,.,.) = 
  3.3599e-02
      ... 

( 20 ,2045,.,.) = 
 -4.2856e-02

( 20 ,2046,.,.) = 
  4.1040e-01

( 20 ,2047,.,.) = 
  2.1054e-01
[torch.FloatTensor of size 21x2048x1x1]
)])
----Begin Training----
---Total: 72 Epochs---
---Base lr:0.001000---Momentum:0.900000---Weight Decay:0.000100---
-----------Epoch:0-----Iter:0-----------
Time:1.942511s(0.514798 iter/s)----lr:0.001000-----Loss:26.297464
Accuracy: 0.978549668229 %
-----------Epoch:0-----Iter:1-----------
Time:0.550947s(1.815057 iter/s)----lr:0.001000-----Loss:17.642708
Accuracy: 4.85896743658 %
-----------Epoch:0-----Iter:2-----------
Time:0.527857s(1.894452 iter/s)----lr:0.001000-----Loss:18.351246
Accuracy: 6.2157837416 %
-----------Epoch:0-----Iter:3-----------
Time:0.542359s(1.843797 iter/s)----lr:0.001000-----Loss:8.929102
Accuracy: 24.5118374741 %
-----------Epoch:0-----Iter:4-----------
Time:0.556348s(1.797436 iter/s)----lr:0.001000-----Loss:7.263081
Accuracy: 42.387127926 %
-----------Epoch:0-----Iter:5-----------
Time:0.584064s(1.712141 iter/s)----lr:0.001000-----Loss:5.885635
Accuracy: 57.0091602282 %
-----------Epoch:0-----Iter:6-----------
Time:0.586313s(1.705574 iter/s)----lr:0.001000-----Loss:7.722076
Accuracy: 38.962952301 %
-----------Epoch:0-----Iter:7-----------
Time:0.562850s(1.776672 iter/s)----lr:0.001000-----Loss:4.644492
Accuracy: 52.4804335198 %
-----------Epoch:0-----Iter:8-----------
Time:0.578068s(1.729900 iter/s)----lr:0.001000-----Loss:6.354990
Accuracy: 62.4334555107 %
-----------Epoch:0-----Iter:9-----------
Time:0.575035s(1.739025 iter/s)----lr:0.001000-----Loss:5.114651
Accuracy: 58.5924157234 %
-----------Epoch:0-----Iter:10-----------
Time:0.556861s(1.795780 iter/s)----lr:0.001000-----Loss:3.759667
Accuracy: 62.6854834112 %
-----------Epoch:0-----Iter:11-----------
Time:0.548401s(1.823483 iter/s)----lr:0.001000-----Loss:4.909984
Accuracy: 60.1183950418 %
-----------Epoch:0-----Iter:12-----------
Time:0.530024s(1.886707 iter/s)----lr:0.001000-----Loss:5.752640
Accuracy: 47.6959216387 %
-----------Epoch:0-----Iter:13-----------
Time:0.524504s(1.906563 iter/s)----lr:0.001000-----Loss:3.782099
Accuracy: 64.4130657007 %
-----------Epoch:0-----Iter:14-----------
Time:0.521537s(1.917410 iter/s)----lr:0.001000-----Loss:4.194042
Accuracy: 57.6148354264 %
-----------Epoch:0-----Iter:15-----------
Time:0.527621s(1.895300 iter/s)----lr:0.001000-----Loss:5.126745
Accuracy: 53.7538260505 %
-----------Epoch:0-----Iter:16-----------
Time:0.538896s(1.855646 iter/s)----lr:0.001000-----Loss:5.342061
Accuracy: 44.3081076984 %
-----------Epoch:0-----Iter:17-----------
Time:0.564337s(1.771991 iter/s)----lr:0.001000-----Loss:4.706436
Accuracy: 52.8027885712 %
-----------Epoch:0-----Iter:18-----------
Time:0.504157s(1.983509 iter/s)----lr:0.001000-----Loss:5.554061
Accuracy: 40.2558404759 %
-----------Epoch:0-----Iter:19-----------
Time:0.546839s(1.828692 iter/s)----lr:0.001000-----Loss:3.611918
Accuracy: 52.8065883232 %
-----------Epoch:0-----Iter:20-----------
Time:0.534641s(1.870414 iter/s)----lr:0.001000-----Loss:4.409239
Accuracy: 45.0304316619 %
-----------Epoch:0-----Iter:21-----------
Time:0.548269s(1.823922 iter/s)----lr:0.001000-----Loss:2.869031
Accuracy: 60.6017194013 %
-----------Epoch:0-----Iter:22-----------
Time:0.522453s(1.914048 iter/s)----lr:0.001000-----Loss:2.859672
Accuracy: 60.4404810338 %
-----------Epoch:0-----Iter:23-----------
Time:0.525093s(1.904425 iter/s)----lr:0.001000-----Loss:3.395411
Accuracy: 58.7113785166 %
-----------Epoch:0-----Iter:24-----------
Time:0.511343s(1.955634 iter/s)----lr:0.001000-----Loss:2.787985
Accuracy: 54.3963345638 %
-----------Epoch:0-----Iter:25-----------
Time:0.542044s(1.844869 iter/s)----lr:0.001000-----Loss:2.859025
Accuracy: 56.9758478843 %
-----------Epoch:0-----Iter:26-----------
Time:0.489791s(2.041687 iter/s)----lr:0.000999-----Loss:2.207221
Accuracy: 61.8884016672 %
-----------Epoch:0-----Iter:27-----------
Time:0.516138s(1.937466 iter/s)----lr:0.000999-----Loss:1.570601
Accuracy: 74.4531512294 %
-----------Epoch:0-----Iter:28-----------
Time:0.540578s(1.849872 iter/s)----lr:0.000999-----Loss:2.073529
Accuracy: 64.6493082575 %
-----------Epoch:0-----Iter:29-----------
Time:0.515645s(1.939319 iter/s)----lr:0.000999-----Loss:2.118134
Accuracy: 65.7839748067 %
-----------Epoch:0-----Iter:30-----------
Time:0.542206s(1.844317 iter/s)----lr:0.000999-----Loss:3.036278
Accuracy: 59.5562243672 %
-----------Epoch:0-----Iter:31-----------
Time:0.545900s(1.831837 iter/s)----lr:0.000999-----Loss:2.474221
Accuracy: 58.0567324498 %
-----------Epoch:0-----Iter:32-----------
Time:0.533797s(1.873371 iter/s)----lr:0.000999-----Loss:2.144058
Accuracy: 61.0988355126 %
-----------Epoch:0-----Iter:33-----------
Time:0.535115s(1.868757 iter/s)----lr:0.000999-----Loss:2.126413
Accuracy: 62.2439049644 %
-----------Epoch:0-----Iter:34-----------
Time:0.527766s(1.894779 iter/s)----lr:0.000999-----Loss:2.336119
Accuracy: 60.62841086 %
-----------Epoch:0-----Iter:35-----------
Time:0.546257s(1.830640 iter/s)----lr:0.000999-----Loss:1.669447
Accuracy: 66.0642337873 %
-----------Epoch:0-----Iter:36-----------
Time:0.523572s(1.909957 iter/s)----lr:0.000999-----Loss:1.991775
Accuracy: 67.2166028706 %
-----------Epoch:0-----Iter:37-----------
Time:0.532549s(1.877761 iter/s)----lr:0.000999-----Loss:1.982390
Accuracy: 61.4501977484 %
-----------Epoch:0-----Iter:38-----------
Time:0.528749s(1.891257 iter/s)----lr:0.000999-----Loss:1.662315
Accuracy: 62.3601375799 %
-----------Epoch:0-----Iter:39-----------
Time:0.542366s(1.843773 iter/s)----lr:0.000999-----Loss:1.495204
Accuracy: 66.1710380108 %
-----------Epoch:0-----Iter:40-----------
Time:0.524767s(1.905608 iter/s)----lr:0.000999-----Loss:1.684750
Accuracy: 70.1891226911 %
-----------Epoch:0-----Iter:41-----------
Time:0.572848s(1.745664 iter/s)----lr:0.000999-----Loss:1.398739
Accuracy: 67.4603679105 %
-----------Epoch:0-----Iter:42-----------
Time:0.544079s(1.837968 iter/s)----lr:0.000999-----Loss:1.877088
Accuracy: 67.0752704629 %
-----------Epoch:0-----Iter:43-----------
Time:0.541515s(1.846671 iter/s)----lr:0.000999-----Loss:1.230182
Accuracy: 70.0311495141 %
-----------Epoch:0-----Iter:44-----------
Time:0.539951s(1.852020 iter/s)----lr:0.000999-----Loss:1.947789
Accuracy: 63.1874184644 %
-----------Epoch:0-----Iter:45-----------
Time:0.568622s(1.758638 iter/s)----lr:0.000999-----Loss:2.406707
Accuracy: 52.8779061202 %
-----------Epoch:0-----Iter:46-----------
Time:0.513317s(1.948114 iter/s)----lr:0.000999-----Loss:1.497783
Accuracy: 68.9274705232 %
-----------Epoch:0-----Iter:47-----------
Time:0.542958s(1.841763 iter/s)----lr:0.000999-----Loss:1.592978
Accuracy: 67.3855277987 %
-----------Epoch:0-----Iter:48-----------
Time:0.516024s(1.937894 iter/s)----lr:0.000999-----Loss:1.586459
Accuracy: 62.4491531387 %
-----------Epoch:0-----Iter:49-----------
Time:0.536009s(1.865640 iter/s)----lr:0.000999-----Loss:2.549758
Accuracy: 54.8380146611 %
-----------Epoch:0-----Iter:50-----------
Time:0.554433s(1.803644 iter/s)----lr:0.000999-----Loss:2.312182
Accuracy: 47.199240472 %
-----------Epoch:0-----Iter:51-----------
Time:0.523479s(1.910296 iter/s)----lr:0.000999-----Loss:1.519835
Accuracy: 63.6628240197 %
-----------Epoch:0-----Iter:52-----------
Time:0.523606s(1.909833 iter/s)----lr:0.000999-----Loss:1.767378
Accuracy: 59.8274194269 %
-----------Epoch:0-----Iter:53-----------
Time:0.532582s(1.877645 iter/s)----lr:0.000999-----Loss:1.713591
Accuracy: 59.7051566866 %
-----------Epoch:0-----Iter:54-----------
Time:0.521879s(1.916153 iter/s)----lr:0.000999-----Loss:1.502808
Accuracy: 61.3662053342 %
-----------Epoch:0-----Iter:55-----------
Time:0.584518s(1.710811 iter/s)----lr:0.000999-----Loss:1.259499
Accuracy: 72.1435893808 %
-----------Epoch:0-----Iter:56-----------
Time:0.549507s(1.819813 iter/s)----lr:0.000999-----Loss:1.505108
Accuracy: 68.5171006095 %
-----------Epoch:0-----Iter:57-----------
Time:0.569628s(1.755532 iter/s)----lr:0.000999-----Loss:1.559773
Accuracy: 62.7817910333 %
-----------Epoch:0-----Iter:58-----------
Time:0.570413s(1.753116 iter/s)----lr:0.000999-----Loss:2.050067
Accuracy: 52.7120549224 %
-----------Epoch:0-----Iter:59-----------
Time:0.560722s(1.783415 iter/s)----lr:0.000999-----Loss:1.552941
Accuracy: 62.2796708069 %
-----------Epoch:0-----Iter:60-----------
Time:0.567413s(1.762385 iter/s)----lr:0.000999-----Loss:2.569505
Accuracy: 55.849396756 %
-----------Epoch:0-----Iter:61-----------
Time:0.562770s(1.776925 iter/s)----lr:0.000999-----Loss:1.706084
Accuracy: 58.7159938513 %
-----------Epoch:0-----Iter:62-----------
Time:0.543861s(1.838705 iter/s)----lr:0.000999-----Loss:1.047332
Accuracy: 76.1618829693 %
-----------Epoch:0-----Iter:63-----------
Time:0.568714s(1.758353 iter/s)----lr:0.000999-----Loss:1.564675
Accuracy: 63.2673918459 %
-----------Epoch:0-----Iter:64-----------
Time:0.553118s(1.807932 iter/s)----lr:0.000999-----Loss:1.394712
Accuracy: 65.2530652249 %
-----------Epoch:0-----Iter:65-----------
Time:0.557767s(1.792863 iter/s)----lr:0.000999-----Loss:1.340850
Accuracy: 66.2487146036 %
-----------Epoch:0-----Iter:66-----------
Time:0.551411s(1.813529 iter/s)----lr:0.000999-----Loss:1.395325
Accuracy: 68.8985476139 %
-----------Epoch:0-----Iter:67-----------
Time:0.571722s(1.749102 iter/s)----lr:0.000999-----Loss:1.233567
Accuracy: 70.4965533309 %
-----------Epoch:0-----Iter:68-----------
Time:0.555855s(1.799030 iter/s)----lr:0.000999-----Loss:1.614434
Accuracy: 63.7961649104 %
-----------Epoch:0-----Iter:69-----------
Time:0.554269s(1.804178 iter/s)----lr:0.000999-----Loss:1.887716
Accuracy: 57.5032766302 %
-----------Epoch:0-----Iter:70-----------
Time:0.521086s(1.919069 iter/s)----lr:0.000999-----Loss:1.290960
Accuracy: 69.2715172942 %
-----------Epoch:0-----Iter:71-----------
Time:0.530012s(1.886750 iter/s)----lr:0.000999-----Loss:1.446844
Accuracy: 68.6884260553 %
-----------Epoch:0-----Iter:72-----------
Time:0.561823s(1.779920 iter/s)----lr:0.000999-----Loss:1.380176
Accuracy: 66.9708086555 %
-----------Epoch:0-----Iter:73-----------
Time:0.525038s(1.904624 iter/s)----lr:0.000999-----Loss:2.533755
Accuracy: 48.3678483424 %
-----------Epoch:0-----Iter:74-----------
Time:0.527422s(1.896015 iter/s)----lr:0.000999-----Loss:2.039892
Accuracy: 50.1109940566 %
-----------Epoch:0-----Iter:75-----------
Time:0.549089s(1.821198 iter/s)----lr:0.000999-----Loss:1.295915
Accuracy: 68.7023424279 %
-----------Epoch:0-----Iter:76-----------
Time:0.517635s(1.931863 iter/s)----lr:0.000999-----Loss:1.417278
Accuracy: 68.0585593698 %
-----------Epoch:0-----Iter:77-----------
Time:0.531217s(1.882470 iter/s)----lr:0.000999-----Loss:1.958761
Accuracy: 61.4693295715 %
-----------Epoch:0-----Iter:78-----------
Time:0.532621s(1.877508 iter/s)----lr:0.000999-----Loss:1.346823
Accuracy: 63.6671453791 %
-----------Epoch:0-----Iter:79-----------
Time:0.536830s(1.862787 iter/s)----lr:0.000998-----Loss:1.532237
Accuracy: 66.9225204432 %
-----------Epoch:0-----Iter:80-----------
Time:0.554787s(1.802494 iter/s)----lr:0.000998-----Loss:1.261493
Accuracy: 72.3895753748 %
-----------Epoch:0-----Iter:81-----------
Time:0.521472s(1.917649 iter/s)----lr:0.000998-----Loss:1.364355
Accuracy: 67.3343327068 %
-----------Epoch:0-----Iter:82-----------
Time:0.546880s(1.828555 iter/s)----lr:0.000998-----Loss:1.694671
Accuracy: 62.0488656777 %
-----------Epoch:0-----Iter:83-----------
Time:0.535015s(1.869106 iter/s)----lr:0.000998-----Loss:1.145107
Accuracy: 71.7778401801 %
-----------Epoch:0-----Iter:84-----------
Time:0.534458s(1.871054 iter/s)----lr:0.000998-----Loss:1.402844
Accuracy: 68.0179203442 %
-----------Epoch:0-----Iter:85-----------
Time:0.542561s(1.843111 iter/s)----lr:0.000998-----Loss:1.087787
Accuracy: 72.5765649648 %
-----------Epoch:0-----Iter:86-----------
Time:0.530921s(1.883519 iter/s)----lr:0.000998-----Loss:1.484629
Accuracy: 68.9609584087 %
-----------Epoch:0-----Iter:87-----------
Time:0.529277s(1.889370 iter/s)----lr:0.000998-----Loss:1.031294
Accuracy: 73.2280892602 %
-----------Epoch:0-----Iter:88-----------
Time:0.522419s(1.914172 iter/s)----lr:0.000998-----Loss:1.347612
Accuracy: 66.1884807006 %
-----------Epoch:0-----Iter:89-----------
Time:0.557234s(1.794578 iter/s)----lr:0.000998-----Loss:1.412347
Accuracy: 65.0513725312 %
-----------Epoch:0-----Iter:90-----------
Time:0.575186s(1.738568 iter/s)----lr:0.000998-----Loss:1.502182
Accuracy: 63.0551507331 %
-----------Epoch:0-----Iter:91-----------
Time:0.577946s(1.730265 iter/s)----lr:0.000998-----Loss:1.065950
Accuracy: 74.9584044347 %
-----------Epoch:0-----Iter:92-----------
Time:0.583319s(1.714328 iter/s)----lr:0.000998-----Loss:1.019064
Accuracy: 69.6928318683 %
-----------Epoch:0-----Iter:93-----------
Time:0.597664s(1.673181 iter/s)----lr:0.000998-----Loss:1.257335
Accuracy: 70.1466766494 %
-----------Epoch:0-----Iter:94-----------
Time:0.548218s(1.824092 iter/s)----lr:0.000998-----Loss:1.127558
Accuracy: 74.6647927645 %
-----------Epoch:0-----Iter:95-----------
Time:0.547321s(1.827081 iter/s)----lr:0.000998-----Loss:1.444346
Accuracy: 67.3716596683 %
-----------Epoch:0-----Iter:96-----------
Time:0.558991s(1.788938 iter/s)----lr:0.000998-----Loss:1.930248
Accuracy: 59.6499513497 %
-----------Epoch:0-----Iter:97-----------
Time:0.554366s(1.803862 iter/s)----lr:0.000998-----Loss:1.236526
Accuracy: 71.2132641482 %
-----------Epoch:0-----Iter:98-----------
Time:0.536875s(1.862631 iter/s)----lr:0.000998-----Loss:1.573979
Accuracy: 62.6733499075 %
-----------Epoch:0-----Iter:99-----------
Time:0.552216s(1.810886 iter/s)----lr:0.000998-----Loss:1.934590
Accuracy: 54.3297981416 %
-----------Epoch:0-----Iter:100-----------
Time:0.552083s(1.811322 iter/s)----lr:0.000998-----Loss:1.165107
Accuracy: 66.3762841961 %
-----------Epoch:0-----Iter:101-----------
Time:0.496416s(2.014440 iter/s)----lr:0.000998-----Loss:1.352361
Accuracy: 69.4172586367 %
-----------Epoch:0-----Iter:102-----------
Time:0.527844s(1.894499 iter/s)----lr:0.000998-----Loss:1.090652
Accuracy: 73.0773614423 %
-----------Epoch:0-----Iter:103-----------
Time:0.549711s(1.819138 iter/s)----lr:0.000998-----Loss:1.319732
Accuracy: 64.8866314903 %
-----------Epoch:0-----Iter:104-----------
Time:0.550759s(1.815676 iter/s)----lr:0.000998-----Loss:1.248319
Accuracy: 68.2113753117 %
-----------Epoch:0-----Iter:105-----------
Time:0.501305s(1.994794 iter/s)----lr:0.000998-----Loss:1.408980
Accuracy: 60.0700057879 %
-----------Epoch:0-----Iter:106-----------
Time:0.522140s(1.915195 iter/s)----lr:0.000998-----Loss:0.970376
Accuracy: 73.5230732491 %
-----------Epoch:0-----Iter:107-----------
Time:0.544184s(1.837614 iter/s)----lr:0.000998-----Loss:1.063516
Accuracy: 73.9470606734 %
-----------Epoch:0-----Iter:108-----------
Time:0.542877s(1.842038 iter/s)----lr:0.000998-----Loss:1.100476
Accuracy: 70.0385582984 %
-----------Epoch:0-----Iter:109-----------
Time:0.507449s(1.970641 iter/s)----lr:0.000998-----Loss:1.125486
Accuracy: 69.7465481078 %
-----------Epoch:0-----Iter:110-----------
Time:0.534357s(1.871408 iter/s)----lr:0.000998-----Loss:1.451643
Accuracy: 67.1388597903 %
-----------Epoch:0-----Iter:111-----------
Time:0.535119s(1.868743 iter/s)----lr:0.000998-----Loss:1.082701
Accuracy: 73.7439354208 %
-----------Epoch:0-----Iter:112-----------
Time:0.507815s(1.969221 iter/s)----lr:0.000998-----Loss:1.668658
Accuracy: 64.4409718584 %
-----------Epoch:0-----Iter:113-----------
Time:0.545871s(1.831935 iter/s)----lr:0.000998-----Loss:1.282118
Accuracy: 66.3441445364 %
-----------Epoch:0-----Iter:114-----------
Time:0.515523s(1.939778 iter/s)----lr:0.000998-----Loss:1.339038
Accuracy: 63.9562651241 %
-----------Epoch:0-----Iter:115-----------
Time:0.509001s(1.964633 iter/s)----lr:0.000998-----Loss:1.429125
Accuracy: 69.6752869304 %
-----------Epoch:0-----Iter:116-----------
Time:0.522236s(1.914843 iter/s)----lr:0.000998-----Loss:1.677350
Accuracy: 59.955100292 %
-----------Epoch:0-----Iter:117-----------
Time:0.516654s(1.935531 iter/s)----lr:0.000998-----Loss:1.460169
Accuracy: 63.1078856689 %
-----------Epoch:0-----Iter:118-----------
Time:0.532692s(1.877257 iter/s)----lr:0.000998-----Loss:1.068324
Accuracy: 73.6088812083 %
-----------Epoch:0-----Iter:119-----------
Time:0.543686s(1.839297 iter/s)----lr:0.000998-----Loss:1.260334
Accuracy: 68.6561901122 %
-----------Epoch:0-----Iter:120-----------
Time:0.564145s(1.772594 iter/s)----lr:0.000998-----Loss:0.995663
Accuracy: 74.6917051293 %
-----------Epoch:0-----Iter:121-----------
Time:0.533770s(1.873466 iter/s)----lr:0.000998-----Loss:1.153851
Accuracy: 69.7475411179 %
-----------Epoch:0-----Iter:122-----------
Time:0.555145s(1.801331 iter/s)----lr:0.000998-----Loss:1.384341
Accuracy: 67.0912614793 %
-----------Epoch:0-----Iter:123-----------
Time:0.581551s(1.719540 iter/s)----lr:0.000998-----Loss:1.657394
Accuracy: 62.834638713 %
-----------Epoch:0-----Iter:124-----------
Time:0.552153s(1.811092 iter/s)----lr:0.000998-----Loss:0.985223
Accuracy: 72.6467025281 %
-----------Epoch:0-----Iter:125-----------
Time:0.555390s(1.800537 iter/s)----lr:0.000998-----Loss:1.169999
Accuracy: 70.1639875871 %
-----------Epoch:0-----Iter:126-----------
Time:0.568468s(1.759114 iter/s)----lr:0.000998-----Loss:1.105099
Accuracy: 72.4326867941 %
-----------Epoch:0-----Iter:127-----------
Time:0.572127s(1.747864 iter/s)----lr:0.000998-----Loss:1.389645
Accuracy: 66.3614328776 %
-----------Epoch:0-----Iter:128-----------
Time:0.568340s(1.759510 iter/s)----lr:0.000998-----Loss:1.293881
Accuracy: 67.6761864106 %
-----------Epoch:0-----Iter:129-----------
Time:0.523184s(1.911373 iter/s)----lr:0.000998-----Loss:1.156367
Accuracy: 70.0440165928 %
-----------Epoch:0-----Iter:130-----------
Time:0.544380s(1.836952 iter/s)----lr:0.000998-----Loss:1.002272
Accuracy: 75.7405972724 %
-----------Epoch:0-----Iter:131-----------
Time:0.556134s(1.798128 iter/s)----lr:0.000998-----Loss:1.121838
Accuracy: 70.0588420721 %
-----------Epoch:0-----Iter:132-----------
Time:0.524019s(1.908328 iter/s)----lr:0.000997-----Loss:1.246760
Accuracy: 65.1316417792 %
-----------Epoch:0-----Iter:133-----------
Time:0.517565s(1.932124 iter/s)----lr:0.000997-----Loss:1.237760
Accuracy: 68.7709439915 %
-----------Epoch:0-----Iter:134-----------
Time:0.536447s(1.864117 iter/s)----lr:0.000997-----Loss:1.604267
Accuracy: 62.4594524208 %
-----------Epoch:0-----Iter:135-----------
Time:0.545469s(1.833285 iter/s)----lr:0.000997-----Loss:1.105836
Accuracy: 70.4659711778 %
-----------Epoch:0-----Iter:136-----------
Time:0.580513s(1.722614 iter/s)----lr:0.000997-----Loss:1.281569
Accuracy: 68.0999878688 %
-----------Epoch:0-----Iter:137-----------
Time:0.510672s(1.958204 iter/s)----lr:0.000997-----Loss:1.000664
Accuracy: 71.6632448398 %
-----------Epoch:0-----Iter:138-----------
Time:0.554730s(1.802679 iter/s)----lr:0.000997-----Loss:0.891106
Accuracy: 75.423110769 %
-----------Epoch:0-----Iter:139-----------
Time:0.542486s(1.843366 iter/s)----lr:0.000997-----Loss:1.436474
Accuracy: 65.7831493489 %
-----------Epoch:0-----Iter:140-----------
Time:0.521099s(1.919021 iter/s)----lr:0.000997-----Loss:1.601766
Accuracy: 62.5729451499 %
-----------Epoch:0-----Iter:141-----------
Time:0.514484s(1.943695 iter/s)----lr:0.000997-----Loss:0.972100
Accuracy: 73.229112585 %
-----------Epoch:0-----Iter:142-----------
Time:0.541906s(1.845338 iter/s)----lr:0.000997-----Loss:1.115665
Accuracy: 70.0732601751 %
-----------Epoch:0-----Iter:143-----------
Time:0.517527s(1.932266 iter/s)----lr:0.000997-----Loss:1.106862
Accuracy: 73.1613889423 %
-----------Epoch:0-----Iter:144-----------
Time:0.552703s(1.809290 iter/s)----lr:0.000997-----Loss:1.529604
Accuracy: 62.7608073282 %
-----------Epoch:0-----Iter:145-----------
Time:0.527321s(1.896378 iter/s)----lr:0.000997-----Loss:1.121176
Accuracy: 73.4585175194 %
-----------Epoch:0-----Iter:146-----------
Time:0.519418s(1.925232 iter/s)----lr:0.000997-----Loss:1.529102
Accuracy: 63.5124423654 %
-----------Epoch:0-----Iter:147-----------
Time:0.559182s(1.788327 iter/s)----lr:0.000997-----Loss:1.910041
Accuracy: 58.4083381973 %
-----------Epoch:0-----Iter:148-----------
Time:0.590500s(1.693480 iter/s)----lr:0.000997-----Loss:1.082125
Accuracy: 71.8305176584 %
-----------Epoch:0-----Iter:149-----------
Time:0.552439s(1.810155 iter/s)----lr:0.000997-----Loss:0.818279
Accuracy: 78.1162501141 %
-----------Epoch:0-----Iter:150-----------
Time:0.518476s(1.928730 iter/s)----lr:0.000997-----Loss:1.401687
Accuracy: 65.7468418694 %
-----------Epoch:0-----Iter:151-----------
Time:0.505872s(1.976785 iter/s)----lr:0.000997-----Loss:1.523964
Accuracy: 62.7752797075 %
-----------Epoch:0-----Iter:152-----------
Time:0.554718s(1.802718 iter/s)----lr:0.000997-----Loss:0.750725
Accuracy: 79.7295584723 %
-----------Epoch:0-----Iter:153-----------
Time:0.526987s(1.897580 iter/s)----lr:0.000997-----Loss:0.913268
Accuracy: 74.7890345096 %
-----------Epoch:0-----Iter:154-----------
Time:0.532271s(1.878742 iter/s)----lr:0.000997-----Loss:1.344162
Accuracy: 67.1319726055 %
-----------Epoch:0-----Iter:155-----------
Time:0.545457s(1.833325 iter/s)----lr:0.000997-----Loss:1.225116
Accuracy: 69.7394838239 %
-----------Epoch:0-----Iter:156-----------
Time:0.534173s(1.872053 iter/s)----lr:0.000997-----Loss:1.285281
Accuracy: 66.0063147627 %
-----------Epoch:0-----Iter:157-----------
Time:0.544141s(1.837759 iter/s)----lr:0.000997-----Loss:0.736462
Accuracy: 81.1630297251 %
-----------Epoch:0-----Iter:158-----------
Time:0.527123s(1.897090 iter/s)----lr:0.000997-----Loss:1.091013
Accuracy: 71.6382623581 %
-----------Epoch:0-----Iter:159-----------
Time:0.522491s(1.913909 iter/s)----lr:0.000997-----Loss:1.815149
Accuracy: 61.9750896912 %
-----------Epoch:0-----Iter:160-----------
Time:0.533111s(1.875782 iter/s)----lr:0.000997-----Loss:1.116812
Accuracy: 68.332261888 %
-----------Epoch:0-----Iter:161-----------
Time:0.512441s(1.951444 iter/s)----lr:0.000997-----Loss:0.866011
Accuracy: 75.5872612711 %
-----------Epoch:0-----Iter:162-----------
Time:0.526779s(1.898329 iter/s)----lr:0.000997-----Loss:1.255179
Accuracy: 72.8109194178 %
-----------Epoch:0-----Iter:163-----------
Time:0.527289s(1.896493 iter/s)----lr:0.000997-----Loss:0.709818
Accuracy: 81.0237629635 %
-----------Epoch:0-----Iter:164-----------
Time:0.550686s(1.815917 iter/s)----lr:0.000997-----Loss:1.344297
Accuracy: 66.7798028953 %
-----------Epoch:0-----Iter:165-----------
Time:0.543982s(1.838296 iter/s)----lr:0.000997-----Loss:1.094883
Accuracy: 67.6367709615 %
-----------Epoch:0-----Iter:166-----------
Time:0.511365s(1.955550 iter/s)----lr:0.000997-----Loss:1.181250
Accuracy: 69.4350414648 %
-----------Epoch:0-----Iter:167-----------
Time:0.500817s(1.996737 iter/s)----lr:0.000997-----Loss:0.809677
Accuracy: 76.9132219666 %
-----------Epoch:0-----Iter:168-----------
Time:0.518857s(1.927313 iter/s)----lr:0.000997-----Loss:1.376577
Accuracy: 68.3311114961 %
-----------Epoch:0-----Iter:169-----------
Time:0.525544s(1.902790 iter/s)----lr:0.000997-----Loss:1.805724
Accuracy: 59.4785079836 %
-----------Epoch:0-----Iter:170-----------
Time:0.560326s(1.784675 iter/s)----lr:0.000997-----Loss:1.206544
Accuracy: 66.7742934888 %
-----------Epoch:0-----Iter:171-----------
Time:0.526548s(1.899162 iter/s)----lr:0.000997-----Loss:1.281125
Accuracy: 69.7911876372 %
-----------Epoch:0-----Iter:172-----------
Time:0.520568s(1.920979 iter/s)----lr:0.000997-----Loss:1.138252
Accuracy: 70.9744669556 %
-----------Epoch:0-----Iter:173-----------
Time:0.502028s(1.991921 iter/s)----lr:0.000997-----Loss:0.829825
Accuracy: 78.6990883555 %
-----------Epoch:0-----Iter:174-----------
Time:0.535881s(1.866086 iter/s)----lr:0.000997-----Loss:1.332320
Accuracy: 66.2436174773 %
-----------Epoch:0-----Iter:175-----------
Time:0.527382s(1.896159 iter/s)----lr:0.000997-----Loss:1.413205
Accuracy: 63.8666839089 %
-----------Epoch:0-----Iter:176-----------
Time:0.526438s(1.899559 iter/s)----lr:0.000997-----Loss:1.058731
Accuracy: 69.5598294584 %
-----------Epoch:0-----Iter:177-----------
Time:0.522022s(1.915628 iter/s)----lr:0.000997-----Loss:1.170091
Accuracy: 68.8594606722 %
-----------Epoch:0-----Iter:178-----------
Time:0.539770s(1.852641 iter/s)----lr:0.000997-----Loss:1.276047
Accuracy: 65.1641639902 %
-----------Epoch:0-----Iter:179-----------
Time:0.521089s(1.919058 iter/s)----lr:0.000997-----Loss:1.746701
Accuracy: 59.5996508021 %
-----------Epoch:0-----Iter:180-----------
Time:0.518140s(1.929980 iter/s)----lr:0.000997-----Loss:1.301523
Accuracy: 67.4042461627 %
-----------Epoch:0-----Iter:181-----------
Time:0.520170s(1.922448 iter/s)----lr:0.000997-----Loss:1.134948
Accuracy: 68.3337784917 %
-----------Epoch:0-----Iter:182-----------
Time:0.533286s(1.875166 iter/s)----lr:0.000997-----Loss:1.026914
Accuracy: 72.6095073292 %
-----------Epoch:0-----Iter:183-----------
Time:0.528711s(1.891392 iter/s)----lr:0.000997-----Loss:0.881495
Accuracy: 76.3991046647 %
-----------Epoch:0-----Iter:184-----------
Time:0.542047s(1.844858 iter/s)----lr:0.000997-----Loss:1.173049
Accuracy: 72.310274579 %
-----------Epoch:0-----Iter:185-----------
Time:0.560032s(1.785612 iter/s)----lr:0.000996-----Loss:1.267464
Accuracy: 69.4785136852 %
-----------Epoch:0-----Iter:186-----------
Time:0.583089s(1.715004 iter/s)----lr:0.000996-----Loss:1.218135
Accuracy: 66.3828139459 %
-----------Epoch:0-----Iter:187-----------
Time:0.533390s(1.874801 iter/s)----lr:0.000996-----Loss:0.729010
Accuracy: 81.8917770726 %
-----------Epoch:0-----Iter:188-----------
Time:0.561433s(1.781156 iter/s)----lr:0.000996-----Loss:0.714267
Accuracy: 80.7820155933 %
-----------Epoch:0-----Iter:189-----------
Time:0.526580s(1.899047 iter/s)----lr:0.000996-----Loss:2.013314
Accuracy: 54.2660272804 %
-----------Epoch:0-----Iter:190-----------
Time:0.536563s(1.863714 iter/s)----lr:0.000996-----Loss:1.291740
Accuracy: 67.0180408731 %
-----------Epoch:0-----Iter:191-----------
Time:0.543951s(1.838401 iter/s)----lr:0.000996-----Loss:1.020776
Accuracy: 71.4504990059 %
-----------Epoch:0-----Iter:192-----------
Time:0.520586s(1.920912 iter/s)----lr:0.000996-----Loss:0.899735
Accuracy: 76.0162601626 %
-----------Epoch:0-----Iter:193-----------
Time:0.531207s(1.882505 iter/s)----lr:0.000996-----Loss:0.901679
Accuracy: 74.6345271173 %
-----------Epoch:0-----Iter:194-----------
Time:0.545659s(1.832646 iter/s)----lr:0.000996-----Loss:1.114173
Accuracy: 70.364457188 %
-----------Epoch:0-----Iter:195-----------
Time:0.534267s(1.871723 iter/s)----lr:0.000996-----Loss:1.367962
Accuracy: 63.7880076057 %
-----------Epoch:0-----Iter:196-----------
Time:0.516846s(1.934812 iter/s)----lr:0.000996-----Loss:1.683831
Accuracy: 61.7549237259 %
-----------Epoch:0-----Iter:197-----------
Time:0.538752s(1.856142 iter/s)----lr:0.000996-----Loss:1.108706
Accuracy: 69.3801675823 %
-----------Epoch:0-----Iter:198-----------
Time:0.527301s(1.896450 iter/s)----lr:0.000996-----Loss:0.934526
Accuracy: 74.9507438537 %
-----------Epoch:0-----Iter:199-----------
Time:0.538328s(1.857604 iter/s)----lr:0.000996-----Loss:0.841396
Accuracy: 74.604125214 %
-----------Epoch:0-----Iter:200-----------
Time:0.511420s(1.955340 iter/s)----lr:0.000996-----Loss:1.286084
Accuracy: 65.1435656278 %
-----------Epoch:0-----Iter:201-----------
Time:0.527428s(1.895993 iter/s)----lr:0.000996-----Loss:1.143589
Accuracy: 69.2832550635 %
-----------Epoch:0-----Iter:202-----------
Time:0.520700s(1.920492 iter/s)----lr:0.000996-----Loss:0.685892
Accuracy: 81.1394161485 %
-----------Epoch:0-----Iter:203-----------
Time:0.557805s(1.792741 iter/s)----lr:0.000996-----Loss:1.026565
Accuracy: 71.3446695422 %
-----------Epoch:0-----Iter:204-----------
Time:0.534760s(1.869998 iter/s)----lr:0.000996-----Loss:1.719595
Accuracy: 61.1562075829 %
-----------Epoch:0-----Iter:205-----------
Time:0.507954s(1.968682 iter/s)----lr:0.000996-----Loss:1.374079
Accuracy: 63.5217049496 %
-----------Epoch:0-----Iter:206-----------
Time:0.513580s(1.947116 iter/s)----lr:0.000996-----Loss:1.289737
Accuracy: 68.6194250971 %
-----------Epoch:0-----Iter:207-----------
Time:0.548384s(1.823540 iter/s)----lr:0.000996-----Loss:1.015609
Accuracy: 71.2993024581 %
-----------Epoch:0-----Iter:208-----------
Time:0.556120s(1.798173 iter/s)----lr:0.000996-----Loss:1.242429
Accuracy: 68.097001659 %
-----------Epoch:0-----Iter:209-----------
Time:0.570957s(1.751445 iter/s)----lr:0.000996-----Loss:1.446708
Accuracy: 64.4358377119 %
-----------Epoch:0-----Iter:210-----------
Time:0.519265s(1.925799 iter/s)----lr:0.000996-----Loss:0.778779
Accuracy: 75.9127650553 %
-----------Epoch:0-----Iter:211-----------
Time:0.545502s(1.833174 iter/s)----lr:0.000996-----Loss:0.973769
Accuracy: 73.063533061 %
-----------Epoch:0-----Iter:212-----------
Time:0.520663s(1.920628 iter/s)----lr:0.000996-----Loss:1.211800
Accuracy: 67.3325215893 %
-----------Epoch:0-----Iter:213-----------
Time:0.512939s(1.949550 iter/s)----lr:0.000996-----Loss:1.203066
Accuracy: 68.4756622828 %
-----------Epoch:0-----Iter:214-----------
Time:0.519045s(1.926615 iter/s)----lr:0.000996-----Loss:0.999770
Accuracy: 72.6805150427 %
-----------Epoch:0-----Iter:215-----------
Time:0.524633s(1.906094 iter/s)----lr:0.000996-----Loss:0.933809
Accuracy: 74.7872143062 %
-----------Epoch:0-----Iter:216-----------
Time:0.561883s(1.779730 iter/s)----lr:0.000996-----Loss:1.523522
Accuracy: 63.4992989067 %
-----------Epoch:0-----Iter:217-----------
Time:0.523498s(1.910227 iter/s)----lr:0.000996-----Loss:1.340665
Accuracy: 60.3710883813 %
-----------Epoch:0-----Iter:218-----------
Time:0.517711s(1.931580 iter/s)----lr:0.000996-----Loss:0.810137
Accuracy: 75.4564215486 %
-----------Epoch:0-----Iter:219-----------
Time:0.520403s(1.921588 iter/s)----lr:0.000996-----Loss:1.119903
Accuracy: 73.7324962361 %
-----------Epoch:0-----Iter:220-----------
Time:0.541505s(1.846705 iter/s)----lr:0.000996-----Loss:1.647820
Accuracy: 63.3132800408 %
-----------Epoch:0-----Iter:221-----------
Time:0.500383s(1.998469 iter/s)----lr:0.000996-----Loss:0.825291
Accuracy: 77.3818314935 %
-----------Epoch:0-----Iter:222-----------
Time:0.542614s(1.842931 iter/s)----lr:0.000996-----Loss:1.207255
Accuracy: 69.6501083945 %
-----------Epoch:0-----Iter:223-----------
Time:0.551375s(1.813648 iter/s)----lr:0.000996-----Loss:1.000334
Accuracy: 71.3461173316 %
-----------Epoch:0-----Iter:224-----------
Time:0.531017s(1.883179 iter/s)----lr:0.000996-----Loss:1.470809
Accuracy: 64.0123754644 %
-----------Epoch:0-----Iter:225-----------
Time:0.523334s(1.910826 iter/s)----lr:0.000996-----Loss:0.732262
Accuracy: 78.1736208435 %
-----------Epoch:0-----Iter:226-----------
Time:0.532000s(1.879699 iter/s)----lr:0.000996-----Loss:1.165921
Accuracy: 69.3785176314 %
-----------Epoch:0-----Iter:227-----------
Time:0.534960s(1.869299 iter/s)----lr:0.000996-----Loss:1.247137
Accuracy: 66.1150703385 %
-----------Epoch:0-----Iter:228-----------
Time:0.525614s(1.902537 iter/s)----lr:0.000996-----Loss:0.884748
Accuracy: 74.3257556614 %
-----------Epoch:0-----Iter:229-----------
Time:0.558315s(1.791104 iter/s)----lr:0.000996-----Loss:1.241465
Accuracy: 69.047382392 %
-----------Epoch:0-----Iter:230-----------
Time:0.550309s(1.817161 iter/s)----lr:0.000996-----Loss:0.753613
Accuracy: 79.6782278729 %
-----------Epoch:0-----Iter:231-----------
Time:0.523410s(1.910548 iter/s)----lr:0.000996-----Loss:0.979395
Accuracy: 73.4827211873 %
-----------Epoch:0-----Iter:232-----------
Time:0.559111s(1.788554 iter/s)----lr:0.000996-----Loss:0.988351
Accuracy: 71.6651061446 %
-----------Epoch:0-----Iter:233-----------
Time:0.535500s(1.867414 iter/s)----lr:0.000996-----Loss:0.802077
Accuracy: 76.6507479626 %
-----------Epoch:0-----Iter:234-----------
Time:0.524310s(1.907269 iter/s)----lr:0.000996-----Loss:0.642526
Accuracy: 81.7103333216 %
-----------Epoch:0-----Iter:235-----------
Time:0.516269s(1.936975 iter/s)----lr:0.000996-----Loss:0.971455
Accuracy: 75.0002601716 %
-----------Epoch:0-----Iter:236-----------
Time:0.534796s(1.869872 iter/s)----lr:0.000996-----Loss:1.343604
Accuracy: 64.3871529735 %
-----------Epoch:0-----Iter:237-----------
Time:0.532960s(1.876313 iter/s)----lr:0.000995-----Loss:1.049429
Accuracy: 72.0976716663 %
-----------Epoch:0-----Iter:238-----------
Time:0.518378s(1.929094 iter/s)----lr:0.000995-----Loss:0.792112
Accuracy: 79.0629866843 %
-----------Epoch:0-----Iter:239-----------
Time:0.526836s(1.898124 iter/s)----lr:0.000995-----Loss:1.200040
Accuracy: 74.7324060729 %
-----------Epoch:0-----Iter:240-----------
Time:0.499368s(2.002531 iter/s)----lr:0.000995-----Loss:1.007638
Accuracy: 75.2409135686 %
-----------Epoch:0-----Iter:241-----------
Time:0.527790s(1.894693 iter/s)----lr:0.000995-----Loss:0.879379
Accuracy: 75.1245704498 %
-----------Epoch:0-----Iter:242-----------
Time:0.503396s(1.986508 iter/s)----lr:0.000995-----Loss:1.295967
Accuracy: 64.3857531197 %
-----------Epoch:0-----Iter:243-----------
Time:0.519493s(1.924954 iter/s)----lr:0.000995-----Loss:1.047704
Accuracy: 71.7134308929 %
-----------Epoch:0-----Iter:244-----------
Time:0.528793s(1.891099 iter/s)----lr:0.000995-----Loss:0.982802
Accuracy: 72.5898097133 %
-----------Epoch:0-----Iter:245-----------
Time:0.516039s(1.937838 iter/s)----lr:0.000995-----Loss:1.277884
Accuracy: 68.1464318464 %
-----------Epoch:0-----Iter:246-----------
Time:0.515461s(1.940011 iter/s)----lr:0.000995-----Loss:1.157021
Accuracy: 69.2277732509 %
-----------Epoch:0-----Iter:247-----------
Time:0.522399s(1.914246 iter/s)----lr:0.000995-----Loss:1.504745
Accuracy: 61.5661867675 %
-----------Epoch:0-----Iter:248-----------
Time:0.493038s(2.028241 iter/s)----lr:0.000995-----Loss:1.141892
Accuracy: 68.5236428746 %
-----------Epoch:0-----Iter:249-----------
Time:0.548939s(1.821696 iter/s)----lr:0.000995-----Loss:1.657893
Accuracy: 64.5333089683 %
-----------Epoch:0-----Iter:250-----------
Time:0.527498s(1.895742 iter/s)----lr:0.000995-----Loss:1.420448
Accuracy: 60.8680340176 %
-----------Epoch:0-----Iter:251-----------
Time:0.537727s(1.859680 iter/s)----lr:0.000995-----Loss:0.936279
Accuracy: 73.8465517705 %
-----------Epoch:0-----Iter:252-----------
Time:0.558719s(1.789808 iter/s)----lr:0.000995-----Loss:1.087496
Accuracy: 70.6704364972 %
-----------Epoch:0-----Iter:253-----------
Time:0.543140s(1.841146 iter/s)----lr:0.000995-----Loss:1.309321
Accuracy: 65.1844401894 %
-----------Epoch:0-----Iter:254-----------
Time:0.538144s(1.858239 iter/s)----lr:0.000995-----Loss:1.006003
Accuracy: 70.7191975962 %
-----------Epoch:0-----Iter:255-----------
Time:0.530812s(1.883906 iter/s)----lr:0.000995-----Loss:0.970077
Accuracy: 72.7536712377 %
-----------Epoch:0-----Iter:256-----------
Time:0.562262s(1.778530 iter/s)----lr:0.000995-----Loss:1.157610
Accuracy: 70.425134847 %
-----------Epoch:0-----Iter:257-----------
Time:0.534200s(1.871958 iter/s)----lr:0.000995-----Loss:0.901373
Accuracy: 75.0302101468 %
-----------Epoch:0-----Iter:258-----------
Time:0.502090s(1.991675 iter/s)----lr:0.000995-----Loss:0.736082
Accuracy: 81.0957480588 %
-----------Epoch:0-----Iter:259-----------
Time:0.546606s(1.829471 iter/s)----lr:0.000995-----Loss:0.847429
Accuracy: 76.6194943901 %
-----------Epoch:0-----Iter:260-----------
Time:0.537233s(1.861390 iter/s)----lr:0.000995-----Loss:0.703149
Accuracy: 79.2377461869 %
-----------Epoch:0-----Iter:261-----------
Time:0.529298s(1.889295 iter/s)----lr:0.000995-----Loss:0.814526
Accuracy: 79.9280935311 %
-----------Epoch:0-----Iter:262-----------
Time:0.535664s(1.866842 iter/s)----lr:0.000995-----Loss:1.385174
Accuracy: 61.7974673056 %
-----------Epoch:0-----Iter:263-----------
Time:0.547902s(1.825144 iter/s)----lr:0.000995-----Loss:1.137272
Accuracy: 66.5128680765 %
-----------Epoch:0-----Iter:264-----------
Time:0.522388s(1.914286 iter/s)----lr:0.000995-----Loss:0.695094
Accuracy: 82.2432626554 %
-----------Epoch:0-----Iter:265-----------
Time:0.527031s(1.897422 iter/s)----lr:0.000995-----Loss:1.118273
Accuracy: 70.95082483 %
-----------Epoch:0-----Iter:266-----------
Time:0.545102s(1.834519 iter/s)----lr:0.000995-----Loss:1.315419
Accuracy: 62.8289518983 %
-----------Epoch:0-----Iter:267-----------
Time:0.487833s(2.049882 iter/s)----lr:0.000995-----Loss:1.065896
Accuracy: 71.7091147354 %
-----------Epoch:0-----Iter:268-----------
Time:0.536218s(1.864913 iter/s)----lr:0.000995-----Loss:1.370289
Accuracy: 63.767279831 %
-----------Epoch:0-----Iter:269-----------
Time:0.544488s(1.836588 iter/s)----lr:0.000995-----Loss:1.027444
Accuracy: 71.4233675904 %
-----------Epoch:0-----Iter:270-----------
Time:0.523374s(1.910680 iter/s)----lr:0.000995-----Loss:1.152024
Accuracy: 70.5457579289 %
-----------Epoch:0-----Iter:271-----------
Time:0.554990s(1.801834 iter/s)----lr:0.000995-----Loss:0.753785
Accuracy: 81.2149876703 %
-----------Epoch:0-----Iter:272-----------
Time:0.544952s(1.835024 iter/s)----lr:0.000995-----Loss:0.996958
Accuracy: 71.6700947701 %
-----------Epoch:0-----Iter:273-----------
Time:0.518837s(1.927388 iter/s)----lr:0.000995-----Loss:1.050046
Accuracy: 72.2022266918 %
-----------Epoch:0-----Iter:274-----------
Time:0.549860s(1.818645 iter/s)----lr:0.000995-----Loss:1.423714
Accuracy: 62.4063516659 %
-----------Epoch:0-----Iter:275-----------
Time:0.535034s(1.869040 iter/s)----lr:0.000995-----Loss:1.208448
Accuracy: 66.9658528829 %
-----------Epoch:0-----Iter:276-----------
Time:0.517106s(1.933839 iter/s)----lr:0.000995-----Loss:1.161607
Accuracy: 68.9241451558 %
-----------Epoch:0-----Iter:277-----------
Time:0.538174s(1.858135 iter/s)----lr:0.000995-----Loss:1.017231
Accuracy: 71.5995176945 %
-----------Epoch:0-----Iter:278-----------
Time:0.512568s(1.950961 iter/s)----lr:0.000995-----Loss:1.101736
Accuracy: 74.6484506584 %
-----------Epoch:0-----Iter:279-----------
Time:0.531543s(1.881315 iter/s)----lr:0.000995-----Loss:1.306772
Accuracy: 63.7753024777 %
-----------Epoch:0-----Iter:280-----------
Time:0.548776s(1.822237 iter/s)----lr:0.000995-----Loss:0.817083
Accuracy: 76.7451616324 %
-----------Epoch:0-----Iter:281-----------
Time:0.524600s(1.906214 iter/s)----lr:0.000995-----Loss:0.648532
Accuracy: 80.3349829422 %
-----------Epoch:0-----Iter:282-----------
Time:0.562239s(1.778603 iter/s)----lr:0.000995-----Loss:1.224602
Accuracy: 68.3226651902 %
-----------Epoch:0-----Iter:283-----------
Time:0.536458s(1.864079 iter/s)----lr:0.000995-----Loss:1.081336
Accuracy: 69.2763890961 %
-----------Epoch:0-----Iter:284-----------
Time:0.543670s(1.839351 iter/s)----lr:0.000995-----Loss:0.705140
Accuracy: 78.1944301907 %
-----------Epoch:0-----Iter:285-----------
Time:0.516007s(1.937958 iter/s)----lr:0.000995-----Loss:0.772792
Accuracy: 80.4707593535 %
-----------Epoch:0-----Iter:286-----------
Time:0.530096s(1.886451 iter/s)----lr:0.000995-----Loss:1.097541
Accuracy: 71.155234657 %
-----------Epoch:0-----Iter:287-----------
Time:0.525993s(1.901166 iter/s)----lr:0.000995-----Loss:0.965780
Accuracy: 74.4878340807 %
-----------Epoch:0-----Iter:288-----------
Time:0.535551s(1.867236 iter/s)----lr:0.000995-----Loss:1.047367
Accuracy: 69.5298841221 %
-----------Epoch:0-----Iter:289-----------
Time:0.516098s(1.937616 iter/s)----lr:0.000995-----Loss:1.274097
Accuracy: 71.141275493 %
-----------Epoch:0-----Iter:290-----------
Time:0.536105s(1.865306 iter/s)----lr:0.000994-----Loss:0.622867
Accuracy: 81.5134588222 %
-----------Epoch:0-----Iter:291-----------
Time:0.513908s(1.945874 iter/s)----lr:0.000994-----Loss:0.729947
Accuracy: 78.2899492352 %
-----------Epoch:0-----Iter:292-----------
Time:0.542382s(1.843719 iter/s)----lr:0.000994-----Loss:1.121337
Accuracy: 73.3402036526 %
-----------Epoch:0-----Iter:293-----------
Time:0.522141s(1.915191 iter/s)----lr:0.000994-----Loss:1.129131
Accuracy: 71.882945286 %
-----------Epoch:0-----Iter:294-----------
Time:0.532137s(1.879215 iter/s)----lr:0.000994-----Loss:1.344497
Accuracy: 66.6476996363 %
-----------Epoch:0-----Iter:295-----------
Time:0.531052s(1.883055 iter/s)----lr:0.000994-----Loss:1.364496
Accuracy: 61.6753544606 %
-----------Epoch:0-----Iter:296-----------
Time:0.518496s(1.928655 iter/s)----lr:0.000994-----Loss:0.883648
Accuracy: 75.802563292 %
-----------Epoch:0-----Iter:297-----------
Time:0.537561s(1.860254 iter/s)----lr:0.000994-----Loss:1.257553
Accuracy: 66.3657080709 %
-----------Epoch:0-----Iter:298-----------
Time:0.572085s(1.747992 iter/s)----lr:0.000994-----Loss:1.154804
Accuracy: 67.9354817242 %
-----------Epoch:0-----Iter:299-----------
Time:0.586152s(1.706042 iter/s)----lr:0.000994-----Loss:0.817018
Accuracy: 78.405481802 %
-----------Epoch:0-----Iter:300-----------
Time:0.570580s(1.752603 iter/s)----lr:0.000994-----Loss:0.692600
Accuracy: 79.4040134652 %
-----------Epoch:0-----Iter:301-----------
Time:0.515320s(1.940542 iter/s)----lr:0.000994-----Loss:1.002313
Accuracy: 72.0099227376 %
-----------Epoch:0-----Iter:302-----------
Time:0.524207s(1.907643 iter/s)----lr:0.000994-----Loss:0.852356
Accuracy: 75.9204540195 %
-----------Epoch:0-----Iter:303-----------
Time:0.583984s(1.712376 iter/s)----lr:0.000994-----Loss:0.990992
Accuracy: 73.442633516 %
-----------Epoch:0-----Iter:304-----------
Time:0.541479s(1.846794 iter/s)----lr:0.000994-----Loss:0.950892
Accuracy: 74.1357867377 %
-----------Epoch:0-----Iter:305-----------
Time:0.526007s(1.901115 iter/s)----lr:0.000994-----Loss:0.903806
Accuracy: 76.7277272563 %
-----------Epoch:0-----Iter:306-----------
Time:0.538085s(1.858442 iter/s)----lr:0.000994-----Loss:1.124512
Accuracy: 69.0035203429 %
-----------Epoch:0-----Iter:307-----------
Time:0.536033s(1.865557 iter/s)----lr:0.000994-----Loss:1.013299
Accuracy: 70.1881442858 %
-----------Epoch:0-----Iter:308-----------
Time:0.537805s(1.859410 iter/s)----lr:0.000994-----Loss:1.020419
Accuracy: 72.2111076966 %
-----------Epoch:0-----Iter:309-----------
Time:0.535609s(1.867034 iter/s)----lr:0.000994-----Loss:1.239726
Accuracy: 66.6583341666 %
-----------Epoch:0-----Iter:310-----------
Time:0.503931s(1.984399 iter/s)----lr:0.000994-----Loss:1.017551
Accuracy: 71.0645300027 %
-----------Epoch:0-----Iter:311-----------
Time:0.509504s(1.962693 iter/s)----lr:0.000994-----Loss:0.579099
Accuracy: 83.4320384233 %
-----------Epoch:0-----Iter:312-----------
Time:0.534243s(1.871807 iter/s)----lr:0.000994-----Loss:1.599667
Accuracy: 61.5087807234 %
-----------Epoch:0-----Iter:313-----------
Time:0.518187s(1.929805 iter/s)----lr:0.000994-----Loss:1.027391
Accuracy: 70.6457947462 %
-----------Epoch:0-----Iter:314-----------
Time:0.516714s(1.935307 iter/s)----lr:0.000994-----Loss:0.921873
Accuracy: 72.2744927379 %
-----------Epoch:0-----Iter:315-----------
Time:0.516173s(1.937335 iter/s)----lr:0.000994-----Loss:1.112852
Accuracy: 72.5367795438 %
-----------Epoch:0-----Iter:316-----------
Time:0.519755s(1.923983 iter/s)----lr:0.000994-----Loss:1.609665
Accuracy: 59.664206108 %
-----------Epoch:0-----Iter:317-----------
Time:0.534123s(1.872228 iter/s)----lr:0.000994-----Loss:1.049190
Accuracy: 70.3492264537 %
-----------Epoch:0-----Iter:318-----------
Time:0.509157s(1.964031 iter/s)----lr:0.000994-----Loss:0.742416
Accuracy: 79.3471813327 %
-----------Epoch:0-----Iter:319-----------
Time:0.512376s(1.951692 iter/s)----lr:0.000994-----Loss:0.652302
Accuracy: 82.6221688143 %
-----------Epoch:0-----Iter:320-----------
Time:0.546398s(1.830168 iter/s)----lr:0.000994-----Loss:1.223899
Accuracy: 70.3285659847 %
-----------Epoch:0-----Iter:321-----------
Time:0.545798s(1.832180 iter/s)----lr:0.000994-----Loss:1.074802
Accuracy: 68.1615259184 %
-----------Epoch:0-----Iter:322-----------
Time:0.517298s(1.933122 iter/s)----lr:0.000994-----Loss:1.175409
Accuracy: 66.1523358707 %
-----------Epoch:0-----Iter:323-----------
Time:0.557738s(1.792957 iter/s)----lr:0.000994-----Loss:1.058408
Accuracy: 70.1146036849 %
-----------Epoch:0-----Iter:324-----------
Time:0.541055s(1.848241 iter/s)----lr:0.000994-----Loss:0.923397
Accuracy: 74.2818162722 %
-----------Epoch:0-----Iter:325-----------
Time:0.514223s(1.944682 iter/s)----lr:0.000994-----Loss:0.983486
Accuracy: 73.0304797962 %
-----------Epoch:0-----Iter:326-----------
Time:0.520623s(1.920776 iter/s)----lr:0.000994-----Loss:1.139921
Accuracy: 70.2965650705 %
-----------Epoch:0-----Iter:327-----------
Time:0.543273s(1.840695 iter/s)----lr:0.000994-----Loss:0.930555
Accuracy: 72.3895937223 %
-----------Epoch:0-----Iter:328-----------
Time:0.507573s(1.970160 iter/s)----lr:0.000994-----Loss:1.079548
Accuracy: 69.0942087887 %
-----------Epoch:0-----Iter:329-----------
Time:0.539138s(1.854813 iter/s)----lr:0.000994-----Loss:0.637725
Accuracy: 81.6553228809 %
-----------Epoch:0-----Iter:330-----------
Time:0.531447s(1.881655 iter/s)----lr:0.000994-----Loss:0.887718
Accuracy: 74.148184505 %
-----------Epoch:0-----Iter:331-----------
Time:0.519672s(1.924291 iter/s)----lr:0.000994-----Loss:0.823434
Accuracy: 79.7484966302 %
-----------Epoch:0-----Iter:332-----------
Time:0.523497s(1.910231 iter/s)----lr:0.000994-----Loss:1.089324
Accuracy: 68.8837877288 %
-----------Epoch:0-----Iter:333-----------
Time:0.551046s(1.814731 iter/s)----lr:0.000994-----Loss:1.168248
Accuracy: 69.1225072283 %
-----------Epoch:0-----Iter:334-----------
Time:0.546168s(1.830938 iter/s)----lr:0.000994-----Loss:1.302797
Accuracy: 65.9737353036 %
-----------Epoch:0-----Iter:335-----------
Time:0.527361s(1.896234 iter/s)----lr:0.000994-----Loss:0.880292
Accuracy: 74.6764126318 %
-----------Epoch:0-----Iter:336-----------
Time:0.536622s(1.863509 iter/s)----lr:0.000994-----Loss:0.586880
Accuracy: 85.0745019604 %
-----------Epoch:0-----Iter:337-----------
Time:0.551495s(1.813253 iter/s)----lr:0.000994-----Loss:1.076100
Accuracy: 70.3205884004 %
-----------Epoch:0-----Iter:338-----------
Time:0.533485s(1.874467 iter/s)----lr:0.000994-----Loss:1.054334
Accuracy: 75.1132665801 %
-----------Epoch:0-----Iter:339-----------
Time:0.530210s(1.886045 iter/s)----lr:0.000994-----Loss:0.875300
Accuracy: 72.2219960398 %
-----------Epoch:0-----Iter:340-----------
Time:0.528747s(1.891264 iter/s)----lr:0.000994-----Loss:0.729245
Accuracy: 77.6571056313 %
-----------Epoch:0-----Iter:341-----------
Time:0.547630s(1.826050 iter/s)----lr:0.000994-----Loss:1.005041
Accuracy: 69.4071712237 %
-----------Epoch:0-----Iter:342-----------
Time:0.547619s(1.826087 iter/s)----lr:0.000994-----Loss:0.962227
Accuracy: 73.1854039687 %
-----------Epoch:0-----Iter:343-----------
Time:0.528801s(1.891071 iter/s)----lr:0.000993-----Loss:0.877867
Accuracy: 74.9878208059 %
-----------Epoch:0-----Iter:344-----------
Time:0.532690s(1.877264 iter/s)----lr:0.000993-----Loss:0.727501
Accuracy: 79.813214963 %
-----------Epoch:0-----Iter:345-----------
Time:0.563288s(1.775291 iter/s)----lr:0.000993-----Loss:1.205337
Accuracy: 67.9990474546 %
-----------Epoch:0-----Iter:346-----------
Time:0.542916s(1.841906 iter/s)----lr:0.000993-----Loss:1.053552
Accuracy: 73.3216961072 %
-----------Epoch:0-----Iter:347-----------
Time:0.550568s(1.816306 iter/s)----lr:0.000993-----Loss:0.959662
Accuracy: 72.3321032482 %
-----------Epoch:0-----Iter:348-----------
Time:0.527750s(1.894837 iter/s)----lr:0.000993-----Loss:1.038410
Accuracy: 70.9920780003 %
-----------Epoch:0-----Iter:349-----------
Time:0.518296s(1.929399 iter/s)----lr:0.000993-----Loss:0.825845
Accuracy: 72.8311346012 %
-----------Epoch:0-----Iter:350-----------
Time:0.523762s(1.909264 iter/s)----lr:0.000993-----Loss:0.929857
Accuracy: 73.0607383306 %
-----------Epoch:0-----Iter:351-----------
Time:0.550433s(1.816752 iter/s)----lr:0.000993-----Loss:1.166726
Accuracy: 70.2222538673 %
-----------Epoch:0-----Iter:352-----------
Time:0.549676s(1.819254 iter/s)----lr:0.000993-----Loss:0.715567
Accuracy: 79.92633038 %
-----------Epoch:0-----Iter:353-----------
Time:0.542851s(1.842126 iter/s)----lr:0.000993-----Loss:0.938429
Accuracy: 73.9008384625 %
-----------Epoch:0-----Iter:354-----------
Time:0.514612s(1.943212 iter/s)----lr:0.000993-----Loss:0.820226
Accuracy: 77.7475873232 %
-----------Epoch:0-----Iter:355-----------
Time:0.547874s(1.825237 iter/s)----lr:0.000993-----Loss:0.865986
Accuracy: 74.538030806 %
-----------Epoch:0-----Iter:356-----------
Time:0.534385s(1.871310 iter/s)----lr:0.000993-----Loss:0.668079
Accuracy: 78.8304528069 %
-----------Epoch:0-----Iter:357-----------
Time:0.535944s(1.865867 iter/s)----lr:0.000993-----Loss:0.841504
Accuracy: 75.4005744639 %
-----------Epoch:0-----Iter:358-----------
Time:0.534889s(1.869547 iter/s)----lr:0.000993-----Loss:1.141901
Accuracy: 69.7022251728 %
-----------Epoch:0-----Iter:359-----------
Time:0.568315s(1.759588 iter/s)----lr:0.000993-----Loss:0.975848
Accuracy: 73.1380685333 %
-----------Epoch:0-----Iter:360-----------
Time:0.533631s(1.873954 iter/s)----lr:0.000993-----Loss:1.061235
Accuracy: 69.8491345109 %
-----------Epoch:0-----Iter:361-----------
Time:0.541605s(1.846364 iter/s)----lr:0.000993-----Loss:0.957964
Accuracy: 70.1330762427 %
-----------Epoch:0-----Iter:362-----------
Time:0.573083s(1.744948 iter/s)----lr:0.000993-----Loss:0.910192
Accuracy: 75.434752659 %
-----------Epoch:0-----Iter:363-----------
Time:0.551354s(1.813717 iter/s)----lr:0.000993-----Loss:0.828835
Accuracy: 75.931208825 %
-----------Epoch:0-----Iter:364-----------
Time:0.549343s(1.820356 iter/s)----lr:0.000993-----Loss:1.479680
Accuracy: 61.6213331416 %
-----------Epoch:0-----Iter:365-----------
Time:0.522273s(1.914707 iter/s)----lr:0.000993-----Loss:0.734056
Accuracy: 77.9267077923 %
-----------Epoch:0-----Iter:366-----------
Time:0.555894s(1.798904 iter/s)----lr:0.000993-----Loss:1.113311
Accuracy: 72.6515987304 %
-----------Epoch:0-----Iter:367-----------
Time:0.546974s(1.828240 iter/s)----lr:0.000993-----Loss:0.882431
Accuracy: 75.3843527041 %
-----------Epoch:0-----Iter:368-----------
Time:0.548281s(1.823882 iter/s)----lr:0.000993-----Loss:1.176793
Accuracy: 70.5074565833 %
-----------Epoch:0-----Iter:369-----------
Time:0.519181s(1.926111 iter/s)----lr:0.000993-----Loss:0.612104
Accuracy: 80.8002216102 %
-----------Epoch:0-----Iter:370-----------
Time:0.541708s(1.846013 iter/s)----lr:0.000993-----Loss:1.052133
Accuracy: 70.2240384649 %
-----------Epoch:0-----Iter:371-----------
Time:0.568107s(1.760232 iter/s)----lr:0.000993-----Loss:1.112890
Accuracy: 71.8321849289 %
-----------Epoch:0-----Iter:372-----------
Time:0.523326s(1.910855 iter/s)----lr:0.000993-----Loss:0.802621
Accuracy: 75.9603637068 %
-----------Epoch:0-----Iter:373-----------
Time:0.529080s(1.890073 iter/s)----lr:0.000993-----Loss:0.640788
Accuracy: 80.4323640741 %
-----------Epoch:0-----Iter:374-----------
Time:0.520594s(1.920883 iter/s)----lr:0.000993-----Loss:1.160721
Accuracy: 69.2505978175 %
-----------Epoch:0-----Iter:375-----------
Time:0.526509s(1.899303 iter/s)----lr:0.000993-----Loss:0.954423
Accuracy: 72.2944576448 %
-----------Epoch:0-----Iter:376-----------
Time:0.509724s(1.961846 iter/s)----lr:0.000993-----Loss:0.681184
Accuracy: 80.5063061511 %
-----------Epoch:0-----Iter:377-----------
Time:0.519999s(1.923081 iter/s)----lr:0.000993-----Loss:0.829242
Accuracy: 77.453300203 %
-----------Epoch:0-----Iter:378-----------
Time:0.524162s(1.907807 iter/s)----lr:0.000993-----Loss:0.924657
Accuracy: 71.0924712162 %
-----------Epoch:0-----Iter:379-----------
Time:0.555897s(1.798894 iter/s)----lr:0.000993-----Loss:1.262261
Accuracy: 68.2547218021 %
-----------Epoch:0-----Iter:380-----------
Time:0.558215s(1.791424 iter/s)----lr:0.000993-----Loss:0.966869
Accuracy: 71.288825874 %
-----------Epoch:0-----Iter:381-----------
Time:0.541495s(1.846739 iter/s)----lr:0.000993-----Loss:0.753693
Accuracy: 77.1555614255 %
-----------Epoch:0-----Iter:382-----------
Time:0.565345s(1.768831 iter/s)----lr:0.000993-----Loss:0.846935
Accuracy: 75.0121582034 %
-----------Epoch:0-----Iter:383-----------
Time:0.540946s(1.848613 iter/s)----lr:0.000993-----Loss:0.877451
Accuracy: 76.4357310785 %
-----------Epoch:0-----Iter:384-----------
Time:0.524778s(1.905568 iter/s)----lr:0.000993-----Loss:0.932540
Accuracy: 74.2323442246 %
-----------Epoch:0-----Iter:385-----------
Time:0.533539s(1.874277 iter/s)----lr:0.000993-----Loss:1.241056
Accuracy: 64.303224043 %
-----------Epoch:0-----Iter:386-----------
Time:0.531290s(1.882211 iter/s)----lr:0.000993-----Loss:0.656552
Accuracy: 80.7006710456 %
-----------Epoch:0-----Iter:387-----------
Time:0.550575s(1.816283 iter/s)----lr:0.000993-----Loss:1.130560
Accuracy: 65.6602082187 %
-----------Epoch:0-----Iter:388-----------
Time:0.547507s(1.826461 iter/s)----lr:0.000993-----Loss:1.007901
Accuracy: 70.2224043555 %
-----------Epoch:0-----Iter:389-----------
Time:0.540250s(1.850995 iter/s)----lr:0.000993-----Loss:0.925668
Accuracy: 72.7877761163 %
-----------Epoch:0-----Iter:390-----------
Time:0.553794s(1.805726 iter/s)----lr:0.000993-----Loss:0.860001
Accuracy: 74.8370418876 %
-----------Epoch:0-----Iter:391-----------
Time:0.554218s(1.804344 iter/s)----lr:0.000993-----Loss:1.138395
Accuracy: 72.2806716577 %
-----------Epoch:0-----Iter:392-----------
Time:0.549704s(1.819161 iter/s)----lr:0.000993-----Loss:1.060007
Accuracy: 67.0251323718 %
-----------Epoch:0-----Iter:393-----------
Time:0.544996s(1.834876 iter/s)----lr:0.000993-----Loss:0.873751
Accuracy: 71.9474507772 %
-----------Epoch:0-----Iter:394-----------
Time:0.519472s(1.925032 iter/s)----lr:0.000993-----Loss:0.615190
Accuracy: 79.6951269254 %
-----------Epoch:0-----Iter:395-----------
Time:0.519148s(1.926233 iter/s)----lr:0.000993-----Loss:0.995985
Accuracy: 72.1615018259 %
-----------Epoch:0-----Iter:396-----------
Time:0.544507s(1.836524 iter/s)----lr:0.000992-----Loss:1.137017
Accuracy: 69.4083035435 %
-----------Epoch:0-----Iter:397-----------
Time:0.518028s(1.930398 iter/s)----lr:0.000992-----Loss:0.717310
Accuracy: 77.7984465281 %
-----------Epoch:0-----Iter:398-----------
Time:0.555708s(1.799506 iter/s)----lr:0.000992-----Loss:0.779131
Accuracy: 75.4802508179 %
